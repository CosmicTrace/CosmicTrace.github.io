<!DOCTYPE html>
<html lang="zh-CN,en,zh-TW,default">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Yiuhang Chan">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://www.yiuhangblog.com/2021/10/28/20211128scrapy/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy">
<meta property="og:url" content="https://www.yiuhangblog.com/2021/10/28/20211128scrapy/index.html">
<meta property="og:site_name" content="角灯随笔">
<meta property="og:description" content="Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/f796f07e-1ef6-45ba-7001-bcfe018f7300/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/6abf6e1a-af6c-433d-6c3b-2d28dd132700/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/5d754191-0f99-42c0-e839-335887c57b00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/70cfa294-f9bd-4895-16f8-e46a7c5b6f00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/5d53dadd-b77e-4703-3fd4-2d1d423b3c00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/ddee2469-c831-46e8-fda1-e27ac9400d00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/7c434622-9bb1-4125-9746-56a18b039a00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/a1deee4e-4d48-4c82-e2e1-8a1d4880d800/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/c82af201-f954-438c-1ee6-7efb40e80a00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/89f31df7-e603-41eb-5464-8c5efe093300/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/6ffff2a9-b43e-4819-2fb5-ae56540da400/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/b8f56e7a-3972-4b36-18e6-e6e57ee27600/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/94269f46-9430-4c2e-c2f9-9f553db3a100/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/c6c79b2f-3b31-4306-c87a-eb8ffdac8900/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/70fb8b82-5e37-45c9-10af-b20b65bb7a00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/a0f874b2-3219-4842-082e-de43448cc100/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/31526e35-d310-4fb0-15ff-a1b91744ba00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/2f87abc1-95ca-4c9a-381d-215cc6863100/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/bc76cc19-8c16-4849-e146-10e8e01f6600/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/4d3dc2ee-996e-4b93-1c43-5d9e56066700/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/88ae35cb-20eb-4b88-88aa-47a6b950b400/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/ae0ddfae-1c39-488e-b671-cf052174e300/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/b8e2bf40-9dbd-46e4-417b-39dda1d11900/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/dc9f98ec-a482-43d8-c3d9-58ce71096b00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/72d94aa2-a334-4558-2a96-f36c2d1cf500/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/f9d7d000-374c-429b-3e8b-d998e8e61000/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/980e1a03-5250-421d-819a-35e0effab800/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/c8f0d32b-0784-4675-0f4d-7607e011f100/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/82901ac0-b156-4034-eb67-4849826d7f00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/4c20a872-96e7-4874-39ef-88cbd76a0500/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/cb576bde-dc2c-4b41-03a2-ac981a3a6f00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/c6f9d98f-1567-488d-d2ca-ff51a7512400/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/fed1f6a3-b5b8-4a8b-a549-a7d7c274f300/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/0bf96745-e6e2-46a8-207d-c7b8fc3b4b00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/5cb85ca5-058b-43b7-e27d-c99d25a7fc00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/cd021416-57dd-4aae-6b83-d4d80fd7cc00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/ddbef679-365c-42f1-c75c-de15d103a500/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/02f70597-affb-4947-3c06-02baba0ee600/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/75e9ccf3-7176-40e0-005b-8da0595f7b00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/6bbae919-0c59-43c7-f3f1-74576158c400/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/46935bf8-4016-4473-fd9d-e987b01fd100/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/de4ff6da-3de9-4d6d-d113-370d7ddee800/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/af0f69ee-c5b3-43b4-8f94-45958240ec00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/4ae3b81e-1b99-4297-ec77-7340c8ada700/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/1a16d792-70d7-4750-e228-ef21a8a0cb00/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/1f0ac0fb-c284-4fc5-f542-7b15d1a03900/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/a87697e5-898e-470d-6d0a-49635a56e600/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/3cde6b5e-2c19-4226-0403-1beecf4f4900/public">
<meta property="og:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/775cf980-849c-4067-1e49-6a38b9890500/public">
<meta property="article:published_time" content="2021-10-28T05:12:54.000Z">
<meta property="article:modified_time" content="2024-02-28T10:50:35.000Z">
<meta property="article:author" content="Yiuhang Chan">
<meta property="article:tag" content="基础">
<meta property="article:tag" content="理论">
<meta property="article:tag" content="爬虫">
<meta property="article:tag" content="Scrapy">
<meta property="article:tag" content="网页抓取">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/f796f07e-1ef6-45ba-7001-bcfe018f7300/public">
    
    
        <!-- Google tag (gtag.js) -->
        <script src="https://www.googletagmanager.com/gtag/js?id=G-MQ43G0NKCT"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-MQ43G0NKCT');
        </script>
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/e75f26bb-7787-428d-ec58-214a6faf9800/public" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/e75f26bb-7787-428d-ec58-214a6faf9800/public">
    <meta name="theme-color" content="#A1AFC9">
    <link rel="shortcut icon" href="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/e75f26bb-7787-428d-ec58-214a6faf9800/public">
    <!--- Page Info-->
    
    <title>
        
            Scrapy -
        
        角灯随笔
    </title>

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">


    <!--- Inject Part-->
    

    
        <style>
    :root {
        --preloader-background-color: #fff;
        --preloader-text-color: #000;
    }

    @media (prefers-color-scheme: dark) {
        :root {
            --preloader-background-color: #202124;
            --preloader-text-color: #fff;
        }
    }

    @media (prefers-color-scheme: light) {
        :root {
            --preloader-background-color: #fff;
            --preloader-text-color: #000;
        }
    }

    @media (max-width: 600px) {
        .ml13 {
            font-size: 2.6rem !important; /* Adjust this value as needed */
        }
    }

    .preloader {
        display: flex;
        flex-direction: column;
        gap: 1rem; /* Tailwind 'gap-4' is 1rem */
        align-items: center;
        justify-content: center;
        position: fixed;
        padding: 12px;
        top: 0;
        right: 0;
        bottom: 0;
        left: 0;
        width: 100vw;
        height: 100vh; /* 'h-screen' is 100% of the viewport height */
        background-color: var(--preloader-background-color);
        z-index: 1100; /* 'z-[1100]' sets the z-index */
        transition: opacity 0.2s ease-in-out;
    }

    .ml13 {
        font-size: 3.2rem;
        /* text-transform: uppercase; */
        color: var(--preloader-text-color);
        letter-spacing: -1px;
        font-weight: 500;
        font-family: 'Chillax-Variable', sans-serif;
        text-align: center;
    }

    .ml13 .word {
        display: inline-flex;
        flex-wrap: wrap;
        white-space: nowrap;
    }

    .ml13 .letter {
        display: inline-block;
        line-height: 1em;
    }
</style>

<div class="preloader">
    
<script src="/js/libs/anime.min.js"></script>

    <h1 class="ml13">
        角灯随笔
    </h1>
    <script>
        var textWrapper = document.querySelector('.ml13');
        // Split text into words
        var words = textWrapper.textContent.trim().split(' ');

        // Clear the existing content
        textWrapper.innerHTML = '';

        // Wrap each word and its letters in spans
        words.forEach(function(word) {
            var wordSpan = document.createElement('span');
            wordSpan.classList.add('word');
            wordSpan.innerHTML = word.replace(/\S/g, "<span class='letter'>$&</span>");
            textWrapper.appendChild(wordSpan);
            textWrapper.appendChild(document.createTextNode(' ')); // Add space between words
        });


        anime.timeline({loop: true})
            .add({
                targets: '.ml13 .letter',
                translateY: [100,0],
                translateZ: 0,
                opacity: [0,1],
                easing: "easeOutExpo",
                duration: 1400,
                delay: (el, i) => 300 + 30 * i
            }).add({
            targets: '.ml13 .letter',
            translateY: [0,-100],
            opacity: [1,0],
            easing: "easeInExpo",
            duration: 1200,
            delay: (el, i) => 100 + 30 * i
        });

        let themeStatus = JSON.parse(localStorage.getItem('REDEFINE-THEME-STATUS'))?.isDark;

        // If the theme status is not found in local storage, check the preferred color scheme
        if (themeStatus === undefined || themeStatus === null) {
            if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                themeStatus = 'dark';
            } else {
                themeStatus = 'light';
            }
        }

        // Now you can use the themeStatus variable in your code
        if (themeStatus) {
            document.documentElement.style.setProperty('--preloader-background-color', '#202124');
            document.documentElement.style.setProperty('--preloader-text-color', '#fff');
        } else {
            document.documentElement.style.setProperty('--preloader-background-color', '#fff');
            document.documentElement.style.setProperty('--preloader-text-color', '#000');
        }

        window.addEventListener('load', function () {
            hidePreloaderAfterTimeout(1000); // Hide after 1000 milliseconds once the window has loaded
        });

        // Backup failsafe: Hide preloader after a maximum of 5000 milliseconds, regardless of the window load event
        hidePreloaderAfterTimeout(5000);

        function hidePreloaderAfterTimeout(delay) {
            setTimeout(function () {
                var preloader = document.querySelector('.preloader');
                preloader.style.opacity = '0';
                setTimeout(function () {
                    preloader.style.display = 'none';
                }, 200);
            }, delay);
        }
    </script>
</div>
    

    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/assets/build/styles.css">

    

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    <!--- Font Part-->
    
    
    
    


    <script id="hexo-configurations">
    window.config = {"hostname":"www.yiuhangblog.com","root":"/","language":["zh-CN","en","zh-TW","default"]};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"title_alignment":"left","headings_top_spacing":{"h1":"5rem","h2":"4rem","h3":"2.8rem","h4":"2.5rem","h5":"2.2rem","h6":"2rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":false,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":6,"number":false,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":5,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A1AFC9","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":true},"scroll_progress":{"bar":true,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":true,"open_graph":true,"google_analytics":{"enable":true,"id":"G-MQ43G0NKCT"}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/e63fc400-dd0e-4395-bd1b-9ee1ac3bfb00/public","dark":"https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/a34140d6-7960-45dd-b8b5-c3ddd14d1f00/public"},"title":"角灯随笔","subtitle":{"text":["全栈也不是不行"],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":false,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"style":"default","links":{"github":"https://github.com/CosmicTrace","instagram":null,"zhihu":null,"twitter":null,"email":"kenchansk0@gmail.com"},"qrs":null}},"plugins":{"feed":{"enable":true},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":true,"version":"9.3.0"}},"version":"2.6.1","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Tags":{"icon":"fa-solid fa-tags","path":"/tags/"},"Categories":{"icon":"fa-solid fa-folder","path":"/categories/"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"娱乐":{"icon":"fa-solid fa-gamepad","submenus":{"王者荣耀":"/hok","守望先锋":"/ow","使命召唤":"/cod"}},"About":{"icon":"fa-regular fa-user","submenus":{"Me":"/about","Github":"https://github.com/CosmicTrace"}}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":"谦逊","show_on_mobile":true,"links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":8}},"footerStart":"2018/07/15 10:15:24"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/atom.xml" title="角灯随笔" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
<!--        <span class="swup-progress-icon">-->
<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
<!--        </span>-->
    
</div>


<main class="page-container" id="swup">

    

    <div class="main-content-container">


        <div class="main-content-header">
            <header class="navbar-container px-6 md:px-12">

    <div class="navbar-content ">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/e75f26bb-7787-428d-ec58-214a6faf9800/public">
                </a>
            
            <a class="logo-title" href="/">
                
                角灯随笔
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    首页
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/tags/"
                                        >
                                    <i class="fa-solid fa-tags fa-fw"></i>
                                    标签
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/categories/"
                                        >
                                    <i class="fa-solid fa-folder fa-fw"></i>
                                    分类
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/archives"
                                        >
                                    <i class="fa-regular fa-archive fa-fw"></i>
                                    归档
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown"
                                   href="#"
                                        onClick=&#34;return false;&#34;>
                                    <i class="fa-solid fa-gamepad fa-fw"></i>
                                    娱乐
                                    <i class="fa-solid fa-chevron-down fa-fw"></i>
                                </a>

                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                        
                                            <li>
                                                <a href="/hok">
                                                    王者荣耀
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a href="/ow">
                                                    守望先锋
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a href="/cod">
                                                    使命召唤
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown"
                                   href="#"
                                        onClick=&#34;return false;&#34;>
                                    <i class="fa-regular fa-user fa-fw"></i>
                                    关于
                                    <i class="fa-solid fa-chevron-down fa-fw"></i>
                                </a>

                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                        
                                            <li>
                                                <a href="/about">
                                                    ME
                                                </a>
                                            </li>
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://github.com/CosmicTrace">
                                                    GITHUB
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i>
                    </div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-screen w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                首页
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/tags/"
                        >
                            <span>
                                标签
                            </span>
                            
                                <i class="fa-solid fa-tags fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/categories/"
                        >
                            <span>
                                分类
                            </span>
                            
                                <i class="fa-solid fa-folder fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/archives"
                        >
                            <span>
                                归档
                            </span>
                            
                                <i class="fa-regular fa-archive fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item-sub text-base my-1.5 flex flex-col w-full">
                        
                        <div class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary cursor-pointer text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                             navbar-data-toggle="submenu-娱乐"
                        >
                            <span>
                                娱乐
                            </span>
                            
                                <i class="fa-solid fa-chevron-right fa-sm fa-fw transition-all"></i>
                            
                        </div>
                        

                        
                            <div class="flex-col items-start px-2 py-2 hidden" data-target="submenu-娱乐">
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           href="/hok">王者荣耀</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           href="/ow">守望先锋</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           href="/cod">使命召唤</a>
                                    </div>
                                
                            </div>
                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item-sub text-base my-1.5 flex flex-col w-full">
                        
                        <div class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary cursor-pointer text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                             navbar-data-toggle="submenu-About"
                        >
                            <span>
                                关于
                            </span>
                            
                                <i class="fa-solid fa-chevron-right fa-sm fa-fw transition-all"></i>
                            
                        </div>
                        

                        
                            <div class="flex-col items-start px-2 py-2 hidden" data-target="submenu-About">
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           href="/about">ME</a>
                                    </div>
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://github.com/CosmicTrace">GITHUB</a>
                                    </div>
                                
                            </div>
                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">96</div>
        <div class="label text-third-text-color text-sm">标签</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">7</div>
        <div class="label text-third-text-color text-sm">分类</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">41</div>
        <div class="label text-third-text-color text-sm">文章</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container flex relative justify-between box-border w-full h-full">
    <div class="article-content-container">

        <div class="article-title relative w-full">
            
                
                
                <img src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/bdc7a33f-ae45-4477-a0fc-b7fd3fe97400/public" alt="Scrapy" class="w-full h-60 sm:h-72 md:h-80 object-cover sm:rounded-t-large dark:brightness-75"/>
                
                <div class="w-full flex items-center absolute bottom-0 justify-start">
                    <h1 class="article-title-cover text-center mx-6 my-6 text-second-text-color bg-background-color-transparent px-4 py-3 text-3xl sm:text-4xl md:text-5xl font-bold backdrop-blur-lg rounded-xl border border-border-color ">Scrapy</h1>
                </div>
            
            </div>

        
            <div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
                <div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
                    <img src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/26329e9f-d1fb-4280-a0a5-754547bf8500/public">
                </div>
                <div class="info flex flex-col justify-between">
                    <div class="author flex items-center">
                        <span class="name text-default-text-color text-lg font-semibold">Yiuhang Chan</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2021-10-28 13:12:54</span>
        <span class="mobile">2021-10-28 13:12:54</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2024-02-28 18:50:35</span>
            <span class="mobile">2024-02-28 18:50:35</span>
            <span class="hover-info">更新</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96/">数据抓取</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E5%9F%BA%E7%A1%80/">基础</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/%E7%90%86%E8%AE%BA/">理论</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/Scrapy/">Scrapy</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96/">网页抓取</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>41.9k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>170 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        


        <div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
            <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Scrapy是一个用Python编写的高效且结构化的网页抓取框架，广泛应用于数据挖掘、网站监测和自动化测试。它的设计初衷是为了方便用户爬取网站数据和提取结构化数据，但它的用途不仅限于网页抓取，还包括处理API返回的数据（如Amazon Associates Web Services）或进行更广泛的网络爬取任务。</p>
<p>Scrapy的一个关键特性是它使用了Twisted异步网络库来处理网络通信。</p>

  <div class="note-large blue">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-info"></i><p>信息</p>

    </div>
    <div class="notel-content">
      <ol>
<li><strong>异步编程</strong>:<ul>
<li>在传统的同步编程模型中，任务按顺序一个接一个地执行。如果一个任务需要等待（例如，等待网络响应），程序将在此期间停止执行后续任务。</li>
<li>异步编程允许程序在等待一个任务完成的同时继续执行其他任务。这种方式非常适合处理I&#x2F;O密集型任务，比如网络请求，因为程序不需要在每个请求完成时都暂停执行。</li>
</ul>
</li>
<li><strong>Twisted异步网络库</strong>:<ul>
<li>Twisted是一个事件驱动的网络编程框架，专为Python设计。它支持多种协议，可以用于创建高性能的网络服务器和客户端。</li>
<li>在Scrapy中，Twisted用于处理网络请求。当Scrapy发出一个请求并等待服务器响应时，它不会阻塞整个爬虫的进程。相反，Scrapy可以利用这段时间执行其他任务，如处理已经收到的数据或发出更多的网络请求。</li>
</ul>
</li>
</ol>

    </div>
  </div>

<p><strong>Scrapy中的应用</strong>:</p>
<ul>
<li>Scrapy利用Twisted的异步特性来提高爬虫的效率。它能够同时处理多个网络请求，而不会因为单个请求的延迟而阻塞整个爬虫。</li>
<li>这使得Scrapy非常适合执行大规模的网络抓取任务，因为它可以有效地利用网络资源和处理能力，同时维持高效率的数据收集。</li>
</ul>
<p><strong>使用原因：</strong></p>
<p>1.为了更利于我们将精力集中在请求与解析上<br>2.企业级的要求</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flowchart TD</span><br><span class="line">    A(找到目标数据) --&gt; B(分析请求流程)</span><br><span class="line">    B --&gt; C(构造http请求)</span><br><span class="line">    C --&gt; D(提取数据)</span><br><span class="line">    D --&gt; E(数据持久化)</span><br></pre></td></tr></table></figure></div>

<h2 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h2><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/f796f07e-1ef6-45ba-7001-bcfe018f7300/public"
                      width = "500"
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/6abf6e1a-af6c-433d-6c3b-2d28dd132700/public"
                      width = "500"
                >

<h3 id="Scrapy框架的体系结构"><a href="#Scrapy框架的体系结构" class="headerlink" title="Scrapy框架的体系结构"></a>Scrapy框架的体系结构</h3><p>Scrapy框架的体系结构由几个重要组件组成，这些组件协同工作，实现了从发送请求到提取数据再到存储数据的整个流程：</p>
<ol>
<li><strong>引擎（Engine）</strong>: Scrapy引擎是框架的核心，负责控制数据流在各组件之间的流动，触发事件。</li>
<li><strong>调度器（Scheduler）</strong>: 负责接收引擎发送的请求，并将它们加入队列中。在引擎请求新请求时，调度器发送下一个要抓取的请求给引擎。</li>
<li><strong>下载器（Downloader）</strong>: 负责从Internet上下载网页内容，并将其封装为响应对象，然后返回给引擎。</li>
<li><strong>蜘蛛（Spiders）</strong>: 是用户编写用来解析响应并从中提取数据（提取的项目）或额外的跟进URL的类。</li>
<li><strong>项目管道（Item Pipeline）</strong>: 负责处理由蜘蛛提取出来的项目，通常包括清洗、验证和存储等。</li>
<li><strong>下载器中间件（Downloader Middlewares）</strong>: 位于引擎和下载器之间，处理引擎和下载器之间的请求和响应。</li>
<li><strong>蜘蛛中间件（Spider Middlewares）</strong>: 位于引擎和蜘蛛之间，处理蜘蛛的输入（响应）和输出（项目和新的请求）。</li>
</ol>

  <div class="note-large blue">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-info"></i><p>信息</p>

    </div>
    <div class="notel-content">
      <p><strong>Downloader Middleware（下载中间件）</strong></p>
<p>这些钩子允许处理、修改和自定义发往下载器的请求以及从下载器返回的响应。下载中间件在Scrapy的请求&#x2F;响应处理流程中提供了几个关键的扩展点。例如，可以使用它来添加HTTP头部到请求中，或者处理从网站返回的HTTP响应。这些中间件的用途包括：</p>
<ul>
<li>在请求发送到下载器之前修改或过滤请求。</li>
<li>在响应发送到爬虫之前处理或修改响应。</li>
<li>直接生成新的请求，而不是处理当前的响应。</li>
<li>选择性地放弃某些请求</li>
</ul>
<p><strong>Spider Middleware（爬虫中间件）</strong></p>
<p>爬虫中间件则是处理传入响应、传出项目（item）和请求的钩子。这些中间件在Scrapy的数据处理流程中处于核心位置，允许用户对爬虫的输入和输出进行细致的控制。使用爬虫中间件，可以：</p>
<ul>
<li>在爬虫处理回调之后处理请求或项目。</li>
<li>修改或过滤开始请求（由 <code>start_requests</code> 方法生成的请求）。</li>
<li>处理由于响应内容引发的异常，根据响应内容调用错误回调（errback）。</li>
</ul>

    </div>
  </div>

<h3 id="数据流过程"><a href="#数据流过程" class="headerlink" title="数据流过程"></a>数据流过程</h3><ol>
<li><strong>启动过程</strong>:<ul>
<li>Scrapy开始运行，初始化了调度器、下载器以及蜘蛛。</li>
</ul>
</li>
<li><strong>爬取过程</strong>:<ul>
<li>引擎向蜘蛛请求第一个要爬取的URL。</li>
<li>蜘蛛返回第一个要爬取的请求给引擎。</li>
<li>引擎将请求发送到调度器，并请求下一个URL。</li>
<li>调度器返回下一个要爬取的请求给引擎。</li>
</ul>
</li>
<li><strong>下载过程</strong>:<ul>
<li>引擎从调度器中取出请求，通过下载器中间件发送到下载器。</li>
<li>下载器处理请求，返回响应给引擎。</li>
<li>引擎收到响应后，通过蜘蛛中间件发送给相应的蜘蛛处理。</li>
</ul>
</li>
<li><strong>解析和提取数据</strong>:<ul>
<li>蜘蛛处理响应，提取数据并生成新的请求，返回给引擎。</li>
<li>引擎将提取的数据发送到项目管道，并将新的请求发送到调度器。</li>
<li>项目管道处理提取出的数据。</li>
</ul>
</li>
<li><strong>循环处理</strong>:<ul>
<li>以上过程在整个爬取过程中不断重复，直到调度器中没有更多的请求。</li>
</ul>
</li>
<li><strong>关闭过程</strong>:<ul>
<li>当所有预定的URL都被爬取，且调度器中没有更多的请求时，Scrapy关闭，爬取过程结束。</li>
</ul>
</li>
</ol>
<h3 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h3><h4 id="项目命令"><a href="#项目命令" class="headerlink" title="项目命令"></a>项目命令</h4><ol>
<li><strong>创建项目</strong>:<ul>
<li>使用命令 <code>scrapy startproject &lt;project_name&gt; [project_dir]</code> 创建新项目。其中 <code>&lt;project_name&gt;</code> 是必填项，表示项目名称；<code>[project_dir]</code> 是可选项，表示项目目录。</li>
<li>例如：<code>scrapy startproject db</code> 会创建一个名为 ‘db’ 的Scrapy项目。</li>
</ul>
</li>
<li><strong>创建第一个爬虫</strong>:<ul>
<li>首先，需要进入到项目目录中，使用 <code>cd &lt;project_name&gt;</code> 命令。</li>
<li>然后，使用 <code>scrapy genspider &lt;name&gt; &lt;domain&gt;</code> 创建一个新的爬虫。这里 <code>&lt;name&gt;</code> 是爬虫的名称，而 <code>&lt;domain&gt;</code> 是爬虫将要爬取的域名。</li>
<li>例如：<code>scrapy genspider example example.com</code> 会在项目的 <code>spiders</code> 目录下创建一个名为 ‘example’ 的爬虫，它针对的是 ‘example.com’ 网站。</li>
</ul>
</li>
<li><strong>运行项目</strong>:<ul>
<li>使用命令 <code>scrapy crawl &lt;spider_name&gt;</code> 来运行爬虫。这里 <code>&lt;spider_name&gt;</code> 是在创建爬虫时指定的名称。</li>
<li>例如：<code>scrapy crawl example</code> 会运行名为 ‘example’ 的爬虫。</li>
</ul>
</li>
<li><strong>设置配置文件</strong>:<ul>
<li>在 <code>settings.py</code> 文件中，可以配置各种参数，比如 <code>ROBOTSTXT_OBEY</code>（是否遵守网站的robots.txt规则）和 <code>DEFAULT_REQUEST_HEADERS</code>（默认的HTTP头部）。</li>
</ul>
</li>
</ol>

  <div class="note-large red">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-bug"></i><p>警告</p>

    </div>
    <div class="notel-content">
      <ol>
<li><p>由于可以设置多个爬虫，且通过<code>name</code>进行管理，因此<code>scrapy genspider &lt;name&gt; &lt;domain&gt;</code>创建新爬虫的时候的时候<code>name</code>是不能重复的，会报错</p>
</li>
<li><p>无数据可以查看是否限制了域名</p>
<p><code>dbtest1.py</code>：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Test1Spider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;test1&quot;</span> <span class="comment">#爬虫名</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;movie.douban.com&quot;</span>] <span class="comment">#限制域名，可以添加多个域名（列表）</span></span><br><span class="line">    start_urls = [<span class="string">&quot;https://movie.douban.com/top250&quot;</span>] <span class="comment">#初始页面，第一个抓取的页面，放在产生请求的地方也就是在爬虫部件</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>): <span class="comment">#response就是上面start_urls的响应</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></div></li>
</ol>

    </div>
  </div>

<h4 id="项目文件介绍"><a href="#项目文件介绍" class="headerlink" title="项目文件介绍"></a>项目文件介绍</h4><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/5d754191-0f99-42c0-e839-335887c57b00/public"
                      width = "500"
                >

<h2 id="爬取豆瓣电影"><a href="#爬取豆瓣电影" class="headerlink" title="爬取豆瓣电影"></a>爬取豆瓣电影</h2><h3 id="项目目的和要求"><a href="#项目目的和要求" class="headerlink" title="项目目的和要求"></a>项目目的和要求</h3><p>目标是创建一个 Scrapy 爬虫来爬取豆瓣电影的前 250 个电影信息。这些信息包括：</p>
<ol>
<li>电影名称</li>
<li>导演和演员信息</li>
<li>电影评分</li>
</ol>
<p>需要将这些信息保存到本地，同时通过 Scrapy 的管道（Pipelines）机制进行持久化。</p>
<h3 id="项目搭建"><a href="#项目搭建" class="headerlink" title="项目搭建"></a>项目搭建</h3><h4 id="创建-Spider-类"><a href="#创建-Spider-类" class="headerlink" title="创建 Spider 类"></a>创建 Spider 类</h4><p>首先需要创建一个名为 <code>Test1Spider</code> 的类，该类继承自 <code>scrapy.Spider</code>。在这个类中，定义以下属性和方法：</p>
<ul>
<li><code>name</code>: 爬虫的唯一标识名</li>
<li><code>start_urls</code>: 包含开始爬取的 URL 列表</li>
<li><code>parse(self, response)</code>: 是爬虫的核心方法，用于处理响应并返回提取的数据或新的请求。</li>
</ul>
<p>创建命令示例：</p>
<p><code>scrapy genspider dbtest1 movie.douban.com</code></p>
<p>这将在 <code>spiders</code> 目录下生成 <code>dbtest1.py</code> 文件。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/70cfa294-f9bd-4895-16f8-e46a7c5b6f00/public"
                      width = "500"
                >

<h4 id="解析响应（Parsing-Response）"><a href="#解析响应（Parsing-Response）" class="headerlink" title="解析响应（Parsing Response）"></a>解析响应（Parsing Response）</h4><p> 在 <code>parse</code> 方法中，使用 <code>response</code> 对象进行页面内容的解析。可以利用 <code>xpath</code> 或 <code>css</code> 选择器提取所需的数据。</p>
<ol>
<li><p>首先分析该页面结构：</p>
<p>假设要获取标题，导演和分数等信息，通过分析可以发现<code>class=&quot;info&quot;</code>即可包含所有得信息</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/5d53dadd-b77e-4703-3fd4-2d1d423b3c00/public"
                      width = "500"
                >

<p>因此修改<code>response</code>直接使用<code>xpath</code>语法</p>
<p><code>dbtest1.py</code>：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dbtest1Spider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;dbtest1&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;movie.douban.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://movie.douban.com/top250&quot;</span>] <span class="comment">#修改为开始爬取的页面</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse 里面我们只需要关注 怎么解析就行， 因为scrapy会自动帮我们去请求指定的网址</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 这个地方可以直接使用response 就可以了，response对象里面就已经绑定了lxml的一些方法</span></span><br><span class="line">        node_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]&#x27;</span>)</span><br></pre></td></tr></table></figure></div></li>
</ol>
<h4 id="调试前置"><a href="#调试前置" class="headerlink" title="调试前置"></a>调试前置</h4><p>可以在同目录下添加调试文件<code>db_debug.py</code>，方便对爬虫文件进行断点调试</p>
<p><code>db_debug.py</code>:</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">execute([<span class="string">&#x27;scrapy&#x27;</span>, <span class="string">&#x27;crawl&#x27;</span>, <span class="string">&#x27;dbtest1&#x27;</span>]) <span class="comment">#执行命令</span></span><br></pre></td></tr></table></figure></div>

<p>通过调试可以发现传输了25行数据</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/ddee2469-c831-46e8-fda1-e27ac9400d00/public"
                      width = "500"
                >

<h4 id="清洗数据"><a href="#清洗数据" class="headerlink" title="清洗数据"></a>清洗数据</h4><p>对拿到的<code>info</code>标签进行解析，利用循环进行即可</p>
<p>首先查看标题标签</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/7c434622-9bb1-4125-9746-56a18b039a00/public"
                      width = "500"
                >

<p>然后通过其结构利用xpath锁定其位置</p>
<p><code>dbtest1.py</code>：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dbtest1Spider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;dbtest1&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;movie.douban.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://movie.douban.com/top250&quot;</span>] <span class="comment">#修改为开始爬取的页面</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse 里面我们只需要关注 怎么解析就行， 因为scrapy会自动帮我们去请求指定的网址</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 这个地方可以直接使用response 就可以了，response对象里面就已经绑定了lxml的一些方法</span></span><br><span class="line">        node_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> node_list: <span class="comment">#万一没有数据那么就不执行避免报错</span></span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> node_list:</span><br><span class="line">                <span class="comment"># 标题</span></span><br><span class="line">                move_title = node.xpath(<span class="string">&#x27;./div/a/span/text()&#x27;</span>).get()</span><br><span class="line">                <span class="comment"># 工作人员</span></span><br><span class="line">                employee = node.xpath(<span class="string">&#x27;./div/p/text()&#x27;</span>).get()</span><br><span class="line">                <span class="comment"># 评分</span></span><br><span class="line">                score = node.xpath(<span class="string">&#x27;./span[@class=&quot;rating_num&quot;]/text()&#x27;</span>).get()</span><br></pre></td></tr></table></figure></div>


  <div class="note-large blue">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-info"></i><p>信息</p>

    </div>
    <div class="notel-content">
      <p>在XPath中，<code>./</code>和<code>//</code>是两种不同的路径选择符，它们用于指定当前节点的相对路径。理解它们之间的差异对于正确地从HTML文档中提取数据是很重要的。</p>
<ol>
<li><strong><code>./</code> - 当前节点下的直接子节点</strong>:<ul>
<li><code>./</code>用于选择当前节点的直接子节点。</li>
<li>当使用<code>./</code>时，它会限制搜索范围到直接下级的元素。</li>
<li>在例子中，<code>./div</code>意味着选择当前节点（在这里是每个<code>div[@class=&quot;info&quot;]</code>）的直接子<code>div</code>元素。</li>
</ul>
</li>
<li><strong><code>//</code> - 文档中任何位置的节点</strong>:<ul>
<li><code>//</code>用于选择文档中任何位置的节点，而不考虑它们在文档中的位置。</li>
<li>使用<code>//</code>时，它会从整个文档中搜索符合条件的节点，而不仅限于当前节点的子节点。</li>
<li>在例子中，如果使用<code>//div</code>，它将会查找整个文档中所有的<code>div</code>元素，而不仅仅是<code>div[@class=&quot;info&quot;]</code>下的<code>div</code>元素。</li>
</ul>
</li>
</ol>
<p>使用了<code>./div</code>，这意味着想要选择每个<code>div[@class=&quot;info&quot;]</code>元素的直接子<code>div</code>元素。这是一种更精确的选择方式，确保只选取特定父元素下的子元素，避免从整个文档中获取不相关的<code>div</code>。</p>

    </div>
  </div>

<h5 id="调试1"><a href="#调试1" class="headerlink" title="调试1"></a>调试1</h5><p>断点调试后发现其获取的是对象，这是因为在Scrapy框架中，使用<code>.xpath()</code>方法返回的是一个<code>SelectorList</code>对象，而不是直接返回数据。<code>SelectorList</code>是Scrapy中的一个特殊类型，用于表示一组通过XPath选择器找到的元素。</p>
<ol>
<li><strong><code>SelectorList</code> 对象</strong>:<ul>
<li>当调用 <code>.xpath()</code> 方法时，返回的是一个 <code>SelectorList</code> 对象，而不是实际的数据。</li>
<li><code>SelectorList</code> 是一个包含多个 <code>Selector</code> 对象的列表。每个 <code>Selector</code> 对象代表一个通过XPath表达式匹配到的节点。</li>
</ul>
</li>
<li><strong>使用 <code>.get()</code> 方法提取数据</strong>:<ul>
<li>为了从 <code>Selector</code> 或 <code>SelectorList</code> 对象中提取数据，需要使用 <code>.get()</code> 方法（在早期版本的Scrapy中，这个方法叫做 <code>.extract_first()</code>）。</li>
<li><code>.get()</code> 方法从 <code>SelectorList</code> 中提取第一个匹配的元素的数据。如果没有找到匹配的元素，它将返回 <code>None</code>。</li>
<li>如果想要提取所有匹配的元素，可以使用 <code>.getall()</code> 方法（旧版本中对应 <code>.extract()</code>）。</li>
</ul>
</li>
<li><strong>为什么不直接返回数据</strong>:<ul>
<li>这种设计允许在决定如何处理匹配到的元素之前先检查它们。例如，可以检查 <code>SelectorList</code> 是否为空，或者提取它包含的所有元素。</li>
<li>它还提供了更大的灵活性，例如先对匹配到的元素进行进一步的选择或应用额外的XPath表达式。</li>
</ul>
</li>
</ol>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/a1deee4e-4d48-4c82-e2e1-8a1d4880d800/public"
                      width = "500"
                >

<p>因此，要从中提取文本数据，需要调用 <code>.get()</code> 方法。</p>
<h5 id="调试2"><a href="#调试2" class="headerlink" title="调试2"></a>调试2</h5><p>随后继续调试可以发现工作人员部分，带有额外空白字符</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/c82af201-f954-438c-1ee6-7efb40e80a00/public"
                      width = "500"
                >

<p>而使用 <code>.strip()</code> 方法在处理从网页提取的文本数据时是非常常见且有用的，特别是当文本包含了不必要的空白字符（如空格、制表符、换行符等）时。<code>.strip()</code> 方法的作用是移除字符串两端的空白字符，使得提取的数据更加整洁和一致。</p>
<p>原因和原理：</p>
<ol>
<li><strong>移除多余空白</strong>:<ul>
<li>网页中的文本经常包含了在HTML代码中用于格式化的额外空白字符。这些字符在网页上看不出来，但在提取文本数据时会被包含进来。</li>
<li><code>.strip()</code> 方法用于移除这些不需要的空白字符，如前后的空格和换行符，这对于清洁数据和后续处理非常有帮助。</li>
</ul>
</li>
<li><strong>格式化和一致性</strong>:<ul>
<li>使用 <code>.strip()</code> 可以确保数据的格式一致，无论原始HTML中的格式如何。</li>
<li>它有助于减少后续处理数据时的麻烦，特别是在比较、存储或展示数据时。</li>
</ul>
</li>
<li><strong>使用方法</strong>:<ul>
<li>在Python中，<code>.strip()</code> 方法无需任何参数即可移除字符串两端的空白字符。</li>
<li>如果想要移除其他特定字符，也可以传递一个字符串作为参数给 <code>.strip()</code>，它将移除字符串两端的任何包含在该参数中的字符。</li>
</ul>
</li>
</ol>
<p>这样，<code>employee</code> 将不再包含字符串开头和结尾的空格或换行符，使得数据更加整洁和便于处理。</p>
<h5 id="调试3"><a href="#调试3" class="headerlink" title="调试3"></a>调试3</h5><p>最后对分数进行调试，发现没有返回数据，而这是因为搜索节点的范围和深度的缘故造成的</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/89f31df7-e603-41eb-5464-8c5efe093300/public"
                      width = "500"
                >

<ul>
<li>当使用 <code>./span[@class=&quot;rating_num&quot;]/text()</code> ：<ul>
<li>这里的 <code>./</code> 将会导致XPath表达式只在当前处理节点的直接子节点中进行搜索。如果当前处理节点是 <code>div</code> 元素的父节点，那么 <code>./div[@class=&quot;star&quot;]</code> 将会选中直接子节点中的 <code>class=&quot;star&quot;</code> 的 <code>div</code> 元素。然后，<code>/span[@class=&quot;rating_num&quot;]/text()</code> 将会从这个选中的 <code>div</code> 元素中选择 <code>class=&quot;rating_num&quot;</code> 的 <code>span</code> 元素，并提取其文本内容。</li>
<li>然而，由于当前处理的节点是 <code>div[@class=&quot;star&quot;]</code> 内部的一个节点，或者更深层次的一个节点，因此 <code>./</code> 就不能正确地选中所需的元素了，因为它不会搜索更深层次的节点。</li>
<li>所以会返回<code>None</code></li>
</ul>
</li>
<li>当使用 <code>.//span[@class=&quot;rating_num&quot;]/text()</code>：<ul>
<li><code>.//</code> 表示从当前节点开始查找，不限于直接子节点，而是包括所有后代节点。</li>
<li><code>div[@class=&quot;star&quot;]</code> 表示选择 <code>class</code> 属性为 “star” 的 <code>div</code> 元素。</li>
<li><code>span[@class=&quot;rating_num&quot;]</code> 表示选择 <code>class</code> 属性为 “rating_num” 的 <code>span</code> 元素。</li>
<li><code>/text()</code> 表示获取选定 <code>span</code> 元素的文本内容。</li>
<li>由于 <code>span</code> 元素嵌套在 <code>class=&quot;star&quot;</code> 的 <code>div</code> 元素内部，所以需要使用 <code>.//</code> 来确保能够选中正确的元素，无论它位于何种深度</li>
</ul>
</li>
</ul>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/6ffff2a9-b43e-4819-2fb5-ae56540da400/public"
                      width = "500"
                >

<p>在XPath中，<code>./</code> 和 <code>.//</code> 的差异主要在于它们搜索节点的范围和深度。这个差别对于定位和提取网页中的数据是至关重要的。</p>
<ol>
<li><strong><code>./</code> - 直接子节点</strong>:<ul>
<li><code>./</code> 表示选择当前节点的直接子节点。</li>
<li>当使用 <code>./</code> 时，它将只在当前节点的下一级（即直接子节点）中查找匹配的元素。</li>
<li>例如，如果当前节点是一个<code>div</code>元素，<code>./span</code>将只会选择该<code>div</code>元素的直接子<code>span</code>元素。</li>
</ul>
</li>
<li><strong><code>.//</code> - 当前节点及其所有后代节点</strong>:<ul>
<li><code>.//</code> 表示在当前节点及其所有后代（子节点、孙节点等）中选择节点。</li>
<li>当使用 <code>.//</code> 时，它会搜索整个子树，找到所有匹配的元素，无论它们位于当前节点的哪个层级。</li>
<li>所以，如果当前节点是一个<code>div</code>元素，<code>.//span</code>将会选择该<code>div</code>及其所有子节点中的所有<code>span</code>元素，无论这些<code>span</code>元素嵌套得有多深。</li>
</ul>
</li>
</ol>
<p>在实际应用中，如果不确定当前处理的节点在HTML结构中的确切位置，使用 <code>.//</code> 会更加安全，因为它不受节点深度的限制。而 <code>./</code> 更适合在已知确切的层次结构和上下文中使用。</p>
<p><code>dbtest1.py</code>：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dbtest1Spider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;dbtest1&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;movie.douban.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://movie.douban.com/top250&quot;</span>] <span class="comment">#修改为开始爬取的页面</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse 里面我们只需要关注 怎么解析就行， 因为scrapy会自动帮我们去请求指定的网址</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 这个地方可以直接使用response 就可以了，response对象里面就已经绑定了lxml的一些方法</span></span><br><span class="line">        node_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> node_list: <span class="comment">#万一没有数据那么就不执行避免报错</span></span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> node_list:</span><br><span class="line">                <span class="comment"># 标题</span></span><br><span class="line">                move_title = node.xpath(<span class="string">&#x27;./div/a/span/text()&#x27;</span>).get()</span><br><span class="line">                <span class="comment"># 工作人员</span></span><br><span class="line">                employee = node.xpath(<span class="string">&#x27;./div/p/text()&#x27;</span>).get().strip()</span><br><span class="line">                <span class="comment"># 评分</span></span><br><span class="line">                score = node.xpath(<span class="string">&#x27;.//span[@class=&quot;rating_num&quot;]/text()&#x27;</span>).get()</span><br></pre></td></tr></table></figure></div>

<h4 id="定义-Item"><a href="#定义-Item" class="headerlink" title="定义 Item"></a>定义 Item</h4><ol>
<li>Scrapy使用Item类来定义数据结构。当从非结构化数据源（如网页）提取数据时，使用Item对象可以确保数据结构化和一致性。</li>
<li>Item对象类似于Python字典，但提供了额外的结构化和错误检测功能。这在大型项目中尤其重要，因为它减少了字段名错误和数据不一致的风险。</li>
<li>在Scrapy项目中，通常需要在<code>items.py</code>文件中定义Item类，然后在爬虫代码中导入和使用这些类。</li>
</ol>
<p>Scrapy 使用 <code>Item</code> 类来定义结构化数据字段，它类似于 Python 的字典但提供额外的保护和便利。在 <code>items.py</code> 文件中定义数据结构。</p>
<p><code>dbtest1.py</code>：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> DbItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dbtest1Spider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;dbtest1&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;movie.douban.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://movie.douban.com/top250&quot;</span>]  <span class="comment"># 修改为开始爬取的页面</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse 里面我们只需要关注 怎么解析就行， 因为scrapy会自动帮我们去请求指定的网址</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 这个地方可以直接使用response 就可以了，response对象里面就已经绑定了lxml的一些方法</span></span><br><span class="line">        node_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> node_list:  <span class="comment"># 万一没有数据那么就不执行避免报错</span></span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> node_list:</span><br><span class="line">                <span class="comment"># 标题</span></span><br><span class="line">                move_title = node.xpath(<span class="string">&#x27;./div/a/span/text()&#x27;</span>).get()</span><br><span class="line">                <span class="comment"># 工作人员</span></span><br><span class="line">                employee = node.xpath(<span class="string">&#x27;./div/p/text()&#x27;</span>).get().strip()</span><br><span class="line">                <span class="comment"># 评分</span></span><br><span class="line">                score = node.xpath(<span class="string">&#x27;.//span[@class=&quot;rating_num&quot;]/text()&#x27;</span>).get()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 实例化</span></span><br><span class="line">                item = DbItem()</span><br><span class="line">                item[<span class="string">&#x27;move_title&#x27;</span>] = move_title</span><br><span class="line">                item[<span class="string">&#x27;employee&#x27;</span>] = employee</span><br><span class="line">                item[<span class="string">&#x27;score&#x27;</span>] = score</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></div>

<p>只会存储<code>item.py</code>里定义的字段</p>
<p><code>items.py</code>：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DbItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    move_title = scrapy.Field()</span><br><span class="line">    employee = scrapy.Field()</span><br><span class="line">    score = scrapy.Field()</span><br></pre></td></tr></table></figure></div>


  <div class="note-large blue">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-info"></i><p>信息</p>

    </div>
    <div class="notel-content">
      <p>在Python中，点（<code>.</code>）和双点（<code>..</code>）是相对导入的一部分，它们表示当前和父包的目录位置。</p>
<ul>
<li>单个点（<code>.</code>）表示当前模块的目录位置。</li>
<li>双点（<code>..</code>）表示父目录的位置。</li>
</ul>
<p>当在一个Python模块中看到 <code>from ..items import DbItem</code> 这样的语句时，这意味着：</p>
<ul>
<li><code>from ..items</code> 表示从当前模块的父包中的 <code>items</code> 模块导入。</li>
<li><code>import DbItem</code> 是指导入 <code>items</code> 模块中定义的 <code>DbItem</code> 类。</li>
</ul>
<p>这种导入方式通常用在一个包结构中，使得模块可以导入同一包内或父包中的其他模块。例如，在Scrapy项目中，可能有以下的目录结构：</p>
<div class="highlight-container" data-rel="Cmd"><figure class="iseeu highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">myproject/</span><br><span class="line">├── myproject/</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── middlewares.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── settings.py</span><br><span class="line">│   └── spiders/</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       ├── spider1.py</span><br><span class="line">│       └── spider2.py</span><br><span class="line">└── scrapy.cfg</span><br></pre></td></tr></table></figure></div>

<p>如果在 <code>spider1.py</code> 或 <code>spider2.py</code> 中需要导入 <code>items.py</code> 中定义的 <code>DbItem</code> 类，需要使用相对导入。由于 <code>spiders</code> 是一个子包，需要使用 <code>..</code> 来表示 <code>items.py</code> 所在的父包 <code>myproject</code>。</p>
<p>使用相对导入的好处是，它不依赖于包的具体位置，这使得整个包更易于重构和移动。然而，相对导入也要小心使用，因为它们只能用在一个包的内部，不能用于顶级模块，否则会引发 <code>ImportError</code>。</p>

    </div>
  </div>


  <div class="note-large blue">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-info"></i><p>信息</p>

    </div>
    <div class="notel-content">
      <p>在Scrapy框架中，<code>yield</code>关键字用于从一个函数返回数据项，但与<code>return</code>不同，<code>yield</code>实现的是一种称为生成器的概念，允许函数在保持其状态的情况下产生一个序列的值，这使得函数能够在每次产生一个值后暂停执行，并在下一次从它停止的地方继续执行。</p>
<p>在Scrapy中，<code>yield</code>通常在爬虫的<code>parse</code>方法中使用，用来产生以下几种可能的对象：</p>
<ol>
<li><strong>Item对象</strong>: 当爬虫解析网页并提取数据时，它会创建一个Item对象，并填充数据。然后通过<code>yield</code>语句返回这个Item对象，Scrapy引擎会将这个Item传递给Item Pipeline进行进一步的处理，如清洗、验证和存储。</li>
<li><strong>Request对象</strong>: 如果页面需要跟进链接或翻页，爬虫会创建一个Request对象，并通过<code>yield</code>返回。这个Request对象包含了请求的URL和一个回调函数，Scrapy会排队发送这个请求，并在收到响应后将其传递给指定的回调函数。</li>
</ol>
<p>使用<code>yield</code>的优势在于它的内存效率和执行效率。生成器不需要在处理大量数据时将它们全部存储在内存中，它们只在需要时产生一个数据项。对于大规模的爬虫任务，这意味着更低的内存消耗和更好的性能。</p>
<p>下面是使用<code>yield</code>在Scrapy爬虫中返回Item对象的示例代码：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="comment"># 解析页面，提取数据</span></span><br><span class="line">    <span class="keyword">for</span> some_data <span class="keyword">in</span> response.css(<span class="string">&#x27;some_selector&#x27;</span>):</span><br><span class="line">        item = MyItem()</span><br><span class="line">        item[<span class="string">&#x27;field1&#x27;</span>] = some_data.css(<span class="string">&#x27;field1_selector::text&#x27;</span>).get()</span><br><span class="line">        item[<span class="string">&#x27;field2&#x27;</span>] = some_data.css(<span class="string">&#x27;field2_selector::text&#x27;</span>).get()</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="keyword">yield</span> item  <span class="comment"># 循环返回Item对象</span></span><br></pre></td></tr></table></figure></div>

<p>在这个例子中，每次循环都会提取数据，创建一个Item对象，并通过<code>yield</code>返回它。Scrapy引擎接收到这些Item后，会将它们发送到Item Pipeline。</p>

    </div>
  </div>

<h4 id="追踪链接（Following-Links）"><a href="#追踪链接（Following-Links）" class="headerlink" title="追踪链接（Following Links）"></a>追踪链接（Following Links）</h4><ol>
<li>为了实现多页爬取，需要在爬虫中实现链接追踪。这通常涉及从当前页面提取下一页的链接或根据一定的规则构建新的URL。</li>
<li>可以在爬虫中使用类变量来追踪当前页码，并在解析函数中构造下一页的URL。如果没有更多的页面可以爬取，爬虫将停止。</li>
</ol>
<p>要爬取所有250部电影的信息，需要从初始页面提取到其他页面的链接，并对其进行跟踪。这可以通过构造新的 <code>scrapy.Request</code> 对象并从 <code>parse</code> 方法返回实现</p>
<h4 id="定义和使用管道（Pipelines）"><a href="#定义和使用管道（Pipelines）" class="headerlink" title="定义和使用管道（Pipelines）"></a>定义和使用管道（Pipelines）</h4><ol>
<li>Item管道是Scrapy中用于处理爬虫返回的Item对象的组件。它们可以执行多种操作，如清理HTML数据、验证数据完整性、检查重复项和进行数据持久化。</li>
<li>创建管道组件通常涉及编写一个Python类，并在该类中实现特定的方法来处理Item对象。</li>
<li>为了激活特定的管道组件，需要在Scrapy的<code>settings.py</code>文件中的<code>ITEM_PIPELINES</code>设置中添加并配置它们。</li>
</ol>
<p>管道用于处理 <code>Spider</code> 从网页中提取的 Item。常见的用途包括清洗数据、去重、存储数据到文件或数据库中。</p>
<ul>
<li>在 <code>pipelines.py</code> 中定义一个管道类，用于处理 <code>Item</code> 对象。</li>
<li>在 <code>settings.py</code> 中启用管道（<code>ITEM_PIPELINES</code> 设置）。</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter <span class="comment">#Pipeline可以更加通用地处理不同的Item类或字典。</span></span><br><span class="line"><span class="keyword">import</span> json <span class="comment">#导入Python标准库中的json模块，用于将Python字典转换为JSON字符串。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DbPipeline</span>: <span class="comment">#定义了一个名为DbPipeline的新类，这是一个Scrapy Pipeline的实现。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>): <span class="comment">#open_spider是一个特殊的方法，它在Scrapy爬虫开始运行时被调用。</span></span><br><span class="line">        <span class="comment">#在open_spider方法中，打开一个名为dbtest1result.txt的文件用于写入（&#x27;w&#x27;模式）</span></span><br><span class="line">        <span class="comment">#并设置编码为utf-8。这个文件句柄被保存在self.f属性中，以便在整个Pipeline中使用。</span></span><br><span class="line">        self.f = <span class="built_in">open</span>(<span class="string">&#x27;dbtest1result.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">		</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>): <span class="comment">#是Pipeline中的核心方法，每个Item在Pipeline中被处理时都会调用这个方法。</span></span><br><span class="line">        <span class="comment">#将Item对象转换为字典（如果它还不是字典），然后转换为一个JSON字符串。</span></span><br><span class="line">        <span class="comment">#ensure_ascii=False是为了确保非ASCII字符（如中文）能被正确处理。</span></span><br><span class="line">        <span class="comment">#每个JSON字符串后面都加上一个换行符，为了确保写入文件时每个Item占据一行。</span></span><br><span class="line">        json_str = json.dumps(<span class="built_in">dict</span>(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">        self.f.write(json_str) <span class="comment">#将JSON字符串写入之前打开的文件。</span></span><br><span class="line">        <span class="built_in">print</span>(item) <span class="comment">#在控制台上打印Item，这对于调试和监控Pipeline的处理是有用的。</span></span><br><span class="line">        <span class="keyword">return</span> item <span class="comment">#返回Item对象，这样它就能被后续的Pipeline组件进一步处理。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>): <span class="comment">#是另一个特殊的方法，它在爬虫关闭时调用。</span></span><br><span class="line">        self.f.close() <span class="comment">#关闭文件句柄。这是一个好习惯，因为它可以确保所有内容都被正确地写入磁盘，并释放了系统资源。</span></span><br></pre></td></tr></table></figure></div>

<h4 id="存储数据"><a href="#存储数据" class="headerlink" title="存储数据"></a>存储数据</h4><ol>
<li>可以编写管道类来实现特定的数据处理需求，例如将爬取的数据保存到文件中。</li>
<li>管道的执行顺序是根据在<code>settings.py</code>中分配给它们的整数值确定的。</li>
</ol>
<p>修改 <code>pipelines.py</code> 文件以添加数据存储逻辑。例如，将数据写入到本地的 <code>film.txt</code> 文件中。</p>
<h4 id="运行爬虫"><a href="#运行爬虫" class="headerlink" title="运行爬虫"></a>运行爬虫</h4><p>进入项目根目录，运行以下命令以启动爬虫：</p>
<p><code>scrapy crawl db</code></p>

  <div class="note-large orange">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-exclamation"></i><p>注意</p>

    </div>
    <div class="notel-content">
      <ol>
<li><p>Robots.txt 协议： 默认情况下，Scrapy 会遵守 Robots.txt 协议。如果需要爬取被此协议禁止的内容，需在 <code>settings.py</code> 中将 <code>ROBOTSTXT_OBEY</code> 设置为 <code>False</code>。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/b8f56e7a-3972-4b36-18e6-e6e57ee27600/public"
                      width = "500"
                >
</li>
<li><p>表头设置（伪装）： 有些网站会根据请求的 User-Agent 进行内容过滤。如果需要，可在 <code>settings.py</code> 中设置自定义的表头。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/94269f46-9430-4c2e-c2f9-9f553db3a100/public"
                      width = "500"
                >
</li>
<li><p>管道激活<strong>： 需要在 <code>settings.py</code> 中激活管道，以便将提取的数据传递到 <code>pipelines.py</code>。</strong><code>300</code>是一个整数值，定义了Pipeline的执行顺序，数字越小越早执行。</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/c6c79b2f-3b31-4306-c87a-eb8ffdac8900/public"
                      width = "500"
                >
</li>
<li><p>Item 字段定义： 在 <code>items.py</code> 中定义字段，这些字段决定了可以从爬取的页面中提取哪些数据。</p>
</li>
</ol>

    </div>
  </div>

<h2 id="Scrapy-shell"><a href="#Scrapy-shell" class="headerlink" title="Scrapy shell"></a>Scrapy shell</h2><p><code>scrapy shell</code> 是一个强大的交互式环境，它允许开发者在不运行整个爬虫的情况下测试和调试Scrapy的数据抓取代码。它可以加载网页，执行选择器，甚至可以运行爬虫的解析函数，是Scrapy开发中不可或缺的调试工具。</p>
<p>在项目目录下输入 <code>scrapy shell https://movie.douban.com/top250</code> 会启动Scrapy shell并预加载豆瓣电影Top 250的页面进行调试。下面是对Scrapy shell中提供的一些快捷方法和对象的详细说明：</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/70fb8b82-5e37-45c9-10af-b20b65bb7a00/public"
                      width = "500"
                >

<h3 id="快捷方法"><a href="#快捷方法" class="headerlink" title="快捷方法"></a>快捷方法</h3><ul>
<li><strong><code>shelp()</code></strong>:<ul>
<li>显示Scrapy shell可用的快捷方法和支持的对象的帮助文档。</li>
</ul>
</li>
<li><strong><code>fetch(url[,redirect=True])</code></strong>:<ul>
<li>用于在Scrapy shell中获取指定网址的响应。如果<code>redirect</code>为<code>True</code>，则会跟随重定向。</li>
</ul>
</li>
<li><strong><code>fetch(request)</code></strong>:<ul>
<li>与上面的方法相似，但这里可以传递一个Scrapy的<code>Request</code>对象，而不是URL字符串。</li>
</ul>
</li>
<li><strong><code>view(response)</code></strong>:<ul>
<li>在本地浏览器中打开当前响应的页面，便于直观地查看Scrapy获取的数据与实际浏览器中呈现的网页之间的差异。</li>
</ul>
</li>
</ul>
<h3 id="Scrapy-对象"><a href="#Scrapy-对象" class="headerlink" title="Scrapy 对象"></a>Scrapy 对象</h3><ul>
<li><strong><code>crawler</code></strong>:<ul>
<li>表示当前运行的爬虫的<code>Crawler</code>对象，包含了很多与爬虫运行相关的信息和接口。</li>
</ul>
</li>
<li><strong><code>spider</code></strong>:<ul>
<li>当前被加载的<code>Spider</code>对象。如果在shell中加载了一个特定的页面，Scrapy会尝试为这个域创建一个爬虫对象。</li>
</ul>
</li>
<li><strong><code>request</code></strong>:<ul>
<li>当前被执行的<code>Request</code>对象。在执行<code>fetch()</code>方法后，可以通过这个对象查看请求的具体信息。</li>
</ul>
</li>
<li><strong><code>response</code></strong>:<ul>
<li>最近一次执行<code>fetch()</code>方法后返回的<code>Response</code>对象。可以通过它来访问页面的内容，并使用选择器提取数据。</li>
</ul>
</li>
<li><strong><code>settings</code></strong>:<ul>
<li>当前Scrapy项目的设置。可以通过它来查看或修改Scrapy的配置信息。</li>
</ul>
</li>
</ul>
<p>在Scrapy shell中，也可以使用所有的Python标准库和Scrapy提供的选择器，例如<code>response.xpath()</code>或<code>response.css()</code>来测试和调试数据提取的代码。</p>
<p>总结起来，<code>scrapy shell</code>提供了一个沙箱环境，让开发者可以实时测试XPath或CSS选择器，查看响应内容，甚至是测试item pipelines的代码，这样可以在不必运行整个Scrapy项目的情况下快速调试和修正代码中的问题。</p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>使用IPython 作为控制台的后端在终端（Terminal）中运行 Scrapy shell，Scrapy 会自动检测到 IPython 并使用它，相较于标准 Python 解释器这样操作更加简便。</p>
<p>在项目文件目录下成功启动Scrapy shell后输入<code>settings[&#39;DEFAULT_REQUEST_HEADERS&#39;]</code>即可查看<code>settings</code>文件中的<code>DEFAULT_REQUEST_HEADERS</code>的默认配置信息</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/a0f874b2-3219-4842-082e-de43448cc100/public"
                      width = "500"
                >

<p>同样的可以查看当前的爬虫和爬虫类的属性值</p>
<p>例如<code>spider.name</code></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/31526e35-d310-4fb0-15ff-a1b91744ba00/public"
                      width = "500"
                >

<p>或者利用<code>fetch()</code>进行URL的切换</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/2f87abc1-95ca-4c9a-381d-215cc6863100/public"
                      width = "500"
                >

<p>但是个人觉得还是用IDE的调试方便，但是一般对于已经部署在服务器的项目还是使用Shell才行，毕竟不用修改代码</p>
<h2 id="Scrapy-Selector-类"><a href="#Scrapy-Selector-类" class="headerlink" title="Scrapy Selector 类"></a>Scrapy Selector 类</h2><p>选择器 Selector 是 Scrapy 的核心部分，用于提取 HTML&#x2F;XML 文档中的数据。</p>
<ul>
<li><p><strong>Scrapy 和 lxml 的关系</strong>:</p>
<ul>
<li>Scrapy 默认使用 <code>lxml</code> 作为其解析器，这意味着即使没有直接导入 <code>lxml</code>，Scrapy 仍然依赖于它来解析 HTML&#x2F;XML。</li>
<li><code>lxml</code> 提供了底层的解析功能，而 Scrapy 的选择器在此基础上提供了更易用的接口。</li>
</ul>
</li>
<li><p><strong>选择器的主要方法</strong>:</p>
<ul>
<li><code>xpath()</code>: 接受 XPath 表达式，用于选择 HTML&#x2F;XML 文档中的元素。</li>
<li><code>css()</code>: 使用 CSS 选择器来选择元素。</li>
<li>这些方法返回的是选择器列表（SelectorList 对象），可以进一步用于提取或操作数据。</li>
</ul>
</li>
<li><p><strong>提取数据的方法</strong>:</p>
<ul>
<li><code>extract()</code>: 返回所有匹配的元素的数据。</li>
<li><code>extract_first()</code>: 返回第一个匹配元素的数据，如果没有匹配元素则返回 <code>None</code>。</li>
<li><code>get()</code> 和 <code>getall()</code>: 它们是 <code>extract_first()</code> 和 <code>extract()</code> 的别名。</li>
</ul>
</li>
<li><p><strong>嵌套选择器的使用</strong>:</p>
<ul>
<li>在某些情况下，可能需要多次调用 <code>.xpath()</code> 或 <code>.css()</code> 来精确定位数据。</li>
<li>示例：<code>response.css(&#39;img&#39;).xpath(&#39;@src&#39;)</code> 会首先选择所有 <code>&lt;img&gt;</code> 标签，然后提取它们的 <code>src</code> 属性。</li>
</ul>
</li>
<li><p><strong>使用正则表达式提取数据</strong>:</p>
<ul>
<li><code>.re()</code>: 使用正则表达式提取数据，返回匹配的字符串列表。</li>
<li><code>.re_first()</code>: 返回第一个匹配的字符串。</li>
</ul>
</li>
<li><p><strong>示例 HTML 字符串</strong>:</p>
<ul>
<li>提供的 HTML 字符串<code>html_str</code>是一个豆瓣电影页面的一部分，包含电影信息如标题、导演、评分等。</li>
<li>使用 Scrapy 选择器，可以提取这些信息，如电影名称、评分等。</li>
</ul>
</li>
</ul>
<div class="highlight-container" data-rel="Html"><figure class="iseeu highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">html_str=&quot;&quot;&quot;</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;info&quot;</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;hd&quot;</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;https://movie.douban.com/subject/1292052/&quot;</span> <span class="attr">class</span>=<span class="string">&quot;&quot;</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;title&quot;</span>&gt;</span>肖申克的救赎<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;title&quot;</span>&gt;</span><span class="symbol">&amp;nbsp;</span>/<span class="symbol">&amp;nbsp;</span>The Shawshank Redemption<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;other&quot;</span>&gt;</span><span class="symbol">&amp;nbsp;</span>/<span class="symbol">&amp;nbsp;</span>月黑高飞(港)  /  刺激1995(台)<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;playable&quot;</span>&gt;</span>[可播放]<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;bd&quot;</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;&quot;</span>&gt;</span></span><br><span class="line">                            导演: 弗兰克·德拉邦特 Frank Darabont<span class="symbol">&amp;nbsp;</span><span class="symbol">&amp;nbsp;</span><span class="symbol">&amp;nbsp;</span>主演: 蒂姆·罗宾斯 Tim Robbins /...<span class="tag">&lt;<span class="name">br</span>&gt;</span></span><br><span class="line">                            1994<span class="symbol">&amp;nbsp;</span>/<span class="symbol">&amp;nbsp;</span>美国<span class="symbol">&amp;nbsp;</span>/<span class="symbol">&amp;nbsp;</span>犯罪 剧情</span><br><span class="line">                        <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;star&quot;</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;rating5-t&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;rating_num&quot;</span> <span class="attr">property</span>=<span class="string">&quot;v:average&quot;</span>&gt;</span>9.7<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">span</span> <span class="attr">property</span>=<span class="string">&quot;v:best&quot;</span> <span class="attr">content</span>=<span class="string">&quot;10.0&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">span</span>&gt;</span>1980500人评价<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;quote&quot;</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;inq&quot;</span>&gt;</span>希望让人自由。<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong>选择器使用的完整示例：</strong></li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.通过文本构造实例对象</span></span><br><span class="line">select_txt = Selector(text=html_str) </span><br><span class="line"><span class="comment"># 这段代码通过 Selector 类创建一个选择器实例，接受一个 HTML 字符串作为输入。允许对该 HTML 文档执行 XPath 或 CSS 查询。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取电影的名字(提取单个内容使用get())</span></span><br><span class="line"><span class="built_in">print</span>(select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).get())</span><br><span class="line"><span class="comment"># 这里使用 XPath 查询提取了电影名称。get() 方法返回第一个匹配元素的文本，适用于提取单个内容。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取多个标签的内容(提取多个内容使用getall())</span></span><br><span class="line"><span class="built_in">print</span>(select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).getall())</span><br><span class="line"><span class="comment"># 与上面类似，但使用 getall() 方法，它返回所有匹配元素的文本列表，适用于提取多个内容。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取电影的名字(提取单个内容使用extract_first())</span></span><br><span class="line"><span class="built_in">print</span>(select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).extract_first())</span><br><span class="line"><span class="comment"># 这行代码等效于使用 get() 方法。extract_first() 也返回第一个匹配元素的文本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取多个标签的内容(提取多个内容使用extract())</span></span><br><span class="line"><span class="built_in">print</span>(select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).extract())</span><br><span class="line"><span class="comment"># 这行代码等效于使用 getall() 方法。extract() 返回所有匹配元素的文本列表。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 细分一下标签</span></span><br><span class="line"><span class="built_in">print</span>(select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span[1]/text()&#x27;</span>).getall()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 这里通过细分 XPath 查询，只选择第一个 &lt;span&gt; 元素的文本，并通过索引 [0] 获取列表中的第一个元素。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 通过response 构造实例对象</span></span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line">response = HtmlResponse(url=<span class="string">&#x27;http://www.example.com/&#x27;</span>, body=html_str.encode())</span><br><span class="line">select_tet = Selector(response=response)</span><br><span class="line"><span class="comment"># 这里通过 HtmlResponse 对象创建了一个选择器实例。HtmlResponse 接受一个 URL 和 HTML 内容作为输入。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(select_tet.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).get())</span><br><span class="line"><span class="built_in">print</span>(response.selector.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).get())</span><br><span class="line"><span class="comment"># 这两行代码都用于提取电影名称。第一行直接使用通过 HtmlResponse 创建的选择器，第二行使用 response 对象的 selector 属性。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(select_tet.css(<span class="string">&#x27;a&#x27;</span>).xpath(<span class="string">&#x27;./span/text()&#x27;</span>).re_first(<span class="string">&#x27;.*的&#x27;</span>))</span><br><span class="line"><span class="comment"># 这里首先使用 CSS 选择器选择所有的 &lt;a&gt; 标签，然后使用 XPath 选择器进一步选择这些 &lt;a&gt; 标签中的 &lt;span&gt; 标签内的文本。最后，使用正则表达式方法 re_first() 提取包含“的”的第一个文本。</span></span><br></pre></td></tr></table></figure></div>

<h3 id="Selector-类的基础用法"><a href="#Selector-类的基础用法" class="headerlink" title="Selector 类的基础用法"></a>Selector 类的基础用法</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br></pre></td></tr></table></figure></div>

<p>导入 <code>Selector</code> 类和 <code>HtmlResponse</code> 类的语句。</p>
<ol>
<li><strong><code>Selector</code> 类</strong>：<ul>
<li><code>Selector</code> 类用于从 HTML 或 XML 文档中提取数据。</li>
<li>它提供了使用 XPath 和 CSS 选择器查询文档的功能。</li>
<li>通过 <code>Selector</code> 类，可以轻松地提取网页中的特定部分，如文本、链接、标签属性等。</li>
</ul>
</li>
<li><strong><code>HtmlResponse</code> 类</strong>：<ul>
<li><code>HtmlResponse</code> 类是 Scrapy 用于表示 HTTP 响应的一个类。</li>
<li>它是 <code>Response</code> 类的一个子类，专门用于处理 HTML 类型的内容。</li>
<li>通过 <code>HtmlResponse</code>，可以访问响应的 URL、状态码、头部（headers）、正文（body）等信息。</li>
<li>它通常与 <code>Request</code> 对象一起使用，表示发送请求后收到的 HTTP 响应。</li>
</ul>
</li>
</ol>
<p>这两个类在编写 Scrapy 爬虫时非常有用。例如，当发送一个 HTTP 请求并收到响应时，可以使用 <code>HtmlResponse</code> 对象来处理这个响应，并使用 <code>Selector</code> 对象来解析和提取所需的数据。</p>
<h4 id="1-通过文本构造-Selector-实例"><a href="#1-通过文本构造-Selector-实例" class="headerlink" title="1. 通过文本构造 Selector 实例"></a>1. 通过文本构造 Selector 实例</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select_txt = Selector(text=html_str)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>Selector</code> 是 Scrapy 框架中的一个核心类，用于执行对 HTML 或 XML 文档的选择和提取操作。</li>
<li>这行代码创建了一个 <code>Selector</code> 的实例，命名为 <code>select_txt</code>。</li>
<li><code>text=html_str</code>：这里 <code>text</code> 参数用于传递 HTML 内容。<ul>
<li><code>html_str</code> 应该是一个包含 HTML 文档内容的字符串。在实际应用中，这个字符串通常是从网页请求中获取的 HTML 响应体。</li>
<li>通过传递这个 HTML 字符串，<code>Selector</code> 类的实例 <code>select_txt</code> 将能够解析这个 HTML 文档。</li>
</ul>
</li>
<li>创建了 <code>select_txt</code> 实例后，可以使用它来进行数据提取。</li>
<li><code>Selector</code> 提供了多种方法来查询和提取 HTML 文档中的数据，最常见的包括 XPath 和 CSS 查询。<ul>
<li>例如，<code>select_txt.xpath(&#39;//div&#39;)</code> 会选择 HTML 文档中所有的 <code>&lt;div&gt;</code> 元素。</li>
<li>另一个例子，<code>select_txt.css(&#39;div.classname&#39;)</code> 会选择所有 class 属性为 <code>classname</code> 的 <code>&lt;div&gt;</code> 元素。</li>
</ul>
</li>
<li>这行代码的主要功能是创建一个 Selector 对象，用于解析和操作给定的 HTML 字符串。这是 Scrapy 爬虫开发中常见的模式，用于从网页中提取信息，如提取链接、文本内容、图像地址等。通过这种方式，Scrapy 能够高效地处理和解析网页内容。</li>
</ul>
<h5 id="提取电影名称"><a href="#提取电影名称" class="headerlink" title="提取电影名称"></a>提取电影名称</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).get()</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p><code>select_txt</code> 是一个 <code>Selector</code> 对象，它包含了 HTML 文档的内容。</p>
</li>
<li><p><code>.xpath(&#39;//div[@class=&quot;info&quot;]/div/a/span/text()&#39;)</code> 是一个 XPath 查询。这个查询的含义如下：</p>
<ul>
<li><code>//div[@class=&quot;info&quot;]</code>：这部分选择所有具有 <code>class=&quot;info&quot;</code> 属性的 <code>&lt;div&gt;</code> 元素。<code>//</code> 表示在整个文档中查找，而 <code>[@class=&quot;info&quot;]</code> 是一个条件，用于筛选具有指定类名的 <code>&lt;div&gt;</code> 元素。</li>
<li><code>/div/a/span</code>：对于每个符合上述条件的 <code>&lt;div&gt;</code> 元素，进一步选择其内部的子 <code>&lt;div&gt;</code>，然后选择这些 <code>&lt;div&gt;</code> 内部的 <code>&lt;a&gt;</code> 元素，再选择这些 <code>&lt;a&gt;</code> 元素内部的 <code>&lt;span&gt;</code> 元素。这是一个层层深入的选择过程。</li>
<li><code>/text()</code>：最后，这部分选择了前面找到的 <code>&lt;span&gt;</code> 元素中的文本内容。<code>text()</code> 函数用于获取一个元素的文本部分。</li>
</ul>
</li>
<li><p><code>.get()</code>：这个方法用于从上面的 XPath 查询中提取第一个匹配元素的文本内容。如果查询没有找到匹配的元素，则返回 <code>None</code>。</p>
</li>
</ul>
<h5 id="提取所有匹配元素的文本列表"><a href="#提取所有匹配元素的文本列表" class="headerlink" title="提取所有匹配元素的文本列表"></a>提取所有匹配元素的文本列表</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).getall()</span><br></pre></td></tr></table></figure></div>

<ul>
<li>类似于前面的查询，但使用 <code>getall()</code> 方法来获取所有匹配元素的文本内容列表。</li>
<li><code>.getall()</code>：这个方法从 XPath 查询中提取所有匹配元素的文本内容，并返回一个列表。如果没有找到匹配的元素，则返回一个空列表。</li>
</ul>
<h5 id="使用-extract-first-提取第一个匹配元素的文本"><a href="#使用-extract-first-提取第一个匹配元素的文本" class="headerlink" title="使用 extract_first() 提取第一个匹配元素的文本"></a>使用 extract_first() 提取第一个匹配元素的文本</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).extract_first()</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>extract_first()</code> 方法的功能与 <code>get()</code> 相似，用于提取第一个匹配元素的文本。</li>
</ul>
<h5 id="使用-extract-提取所有匹配元素的文本列表"><a href="#使用-extract-提取所有匹配元素的文本列表" class="headerlink" title="使用 extract() 提取所有匹配元素的文本列表"></a>使用 extract() 提取所有匹配元素的文本列表</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).extract()</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>extract()</code> 方法的功能与 <code>getall()</code> 相似，用于提取所有匹配元素的文本内容列表。</li>
</ul>
<h5 id="提取特定子元素的文本"><a href="#提取特定子元素的文本" class="headerlink" title="提取特定子元素的文本"></a>提取特定子元素的文本</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select_txt.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span[1]/text()&#x27;</span>).getall()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p><code>select_txt</code> 应该是一个 <code>Selector</code> 对象，包含了 HTML 文档的内容。</p>
</li>
<li><p><code>.xpath(&#39;//div[@class=&quot;info&quot;]/div/a/span[1]/text()&#39;)</code> 是一个 XPath 查询。</p>
<ul>
<li><code>//div[@class=&quot;info&quot;]</code>：选择所有具有 <code>class=&quot;info&quot;</code> 属性的 <code>&lt;div&gt;</code> 元素。<code>//</code> 表示在整个文档中查找。</li>
<li><code>/div/a/span[1]</code>：对于每个符合上述条件的 <code>&lt;div&gt;</code> 元素，进一步选择其内部的子 <code>&lt;div&gt;</code>，然后选择这些 <code>&lt;div&gt;</code> 内部的 <code>&lt;a&gt;</code> 元素，再选择这些 <code>&lt;a&gt;</code> 元素内部的第一个 <code>&lt;span&gt;</code> 元素（由 <code>[1]</code> 指定）。</li>
<li><code>/text()</code>：选择前面找到的 <code>&lt;span&gt;</code> 元素中的文本内容。</li>
</ul>
</li>
<li><p><code>.getall()</code>：这个方法用于从 XPath 查询中提取所有匹配元素的文本内容，并返回一个列表。</p>
</li>
<li><p><code>[0]</code>：这个索引用于从列表中获取第一个元素。在 Python 中，列表的索引从 0 开始。</p>
</li>
</ul>
<h4 id="2-通过-response-构造-Selector-实例"><a href="#2-通过-response-构造-Selector-实例" class="headerlink" title="2. 通过 response 构造 Selector 实例"></a>2. 通过 response 构造 Selector 实例</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 HtmlResponse 对象</span></span><br><span class="line">response = HtmlResponse(url=<span class="string">&#x27;http://www.example.com/&#x27;</span>, body=html_str.encode())</span><br><span class="line"><span class="comment"># 创建 Selector 对象</span></span><br><span class="line">select_tet = Selector(response=response)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p><code>HtmlResponse</code> 是 Scrapy 框架中用来表示 HTTP 响应的一个类。</p>
</li>
<li><p><code>url=&#39;http://www.example.com/&#39;</code>：这里指定了响应所对应的 URL。在实际的 Scrapy 应用中，这通常是发起请求的目标 URL。</p>
</li>
<li><p><code>body=html_str.encode()：body</code></p>
<p> 参数用于提供 HTTP 响应的正文内容。</p>
<ul>
<li><code>html_str</code> 应该是一个包含 HTML 内容的字符串。</li>
<li><code>.encode()</code> 方法将这个字符串转换为字节序列。在 HTTP 通信中，正文内容通常以字节形式存在，所以需要进行这样的转换。</li>
</ul>
</li>
<li><p><code>Selector</code> 类在 Scrapy 中用于选择和提取 HTML 或 XML 文档中的数据。</p>
</li>
<li><p><code>response=response</code>：这里将前面创建的 <code>HtmlResponse</code> 对象传递给 <code>Selector</code>。这样，<code>Selector</code> 就可以使用 <code>HtmlResponse</code> 中的 HTML 内容进行解析和数据提取。</p>
</li>
<li><p><code>select_tet</code> 是创建的 <code>Selector</code> 对象的实例，它现在包含了 <code>response</code> 中的 HTML 数据，并可以使用该对象进行各种选择器查询（如 XPath 或 CSS 查询）。</p>
</li>
<li><p>这两行代码组合使用 <code>HtmlResponse</code> 和 <code>Selector</code> 类来处理和解析 HTML 数据。首先，使用 <code>HtmlResponse</code> 来模拟一个 HTTP 响应，包括它的 URL 和正文内容。然后，创建一个 <code>Selector</code> 对象来解析这个响应的 HTML 内容，并提供数据提取的功能。这是在 Scrapy 爬虫中常见的模式，用于从网页中提取所需的信息。</p>
</li>
</ul>
<h5 id="使用-Selector-实例和-response-的-selector-属性提取数据"><a href="#使用-Selector-实例和-response-的-selector-属性提取数据" class="headerlink" title="使用 Selector 实例和 response 的 selector 属性提取数据"></a>使用 Selector 实例和 response 的 selector 属性提取数据</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select_tet.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).get()</span><br><span class="line">response.selector.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]/div/a/span/text()&#x27;</span>).get()</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p><code>select_tet</code> 应该是一个由 <code>Selector</code> 类实例化的对象。这个对象包含了某个 HTML 文档的内容。</p>
</li>
<li><p><code>.xpath(&#39;//div[@class=&quot;info&quot;]/div/a/span/text()&#39;)</code></p>
<p> 是一个 XPath 选择器表达式。这个表达式的作用是：</p>
<ul>
<li><code>//div[@class=&quot;info&quot;]</code>：选择所有具有 <code>class=&quot;info&quot;</code> 属性的 <code>&lt;div&gt;</code> 标签。</li>
<li><code>/div/a/span</code>：在每个这样的 <code>&lt;div&gt;</code> 标签内，进一步选择嵌套的 <code>&lt;div&gt;</code>，然后选择其内部的 <code>&lt;a&gt;</code> 标签，再选择 <code>&lt;a&gt;</code> 标签内的 <code>&lt;span&gt;</code> 标签。</li>
<li><code>/text()</code>：提取这些 <code>&lt;span&gt;</code> 标签内的文本内容。</li>
</ul>
</li>
<li><p><code>.get()</code>：这个方法用于获取匹配的第一个元素的文本内容。如果没有找到匹配的元素，将返回 <code>None</code>。</p>
</li>
<li><p><code>response</code> 应该是一个 <code>HtmlResponse</code> 对象，通常在 Scrapy 中表示对某个 URL 请求的响应。</p>
</li>
<li><p><code>response.selector</code> 是 <code>HtmlResponse</code> 对象的属性，它提供了一个 <code>Selector</code> 对象，用于在响应的 HTML 内容上进行选择器查询。</p>
</li>
<li><p><code>.xpath(&#39;//div[@class=&quot;info&quot;]/div/a/span/text()&#39;)</code> 和第一行代码中的 XPath 表达式相同，作用也相同，用于选择特定结构的元素并提取其文本内容。</p>
</li>
<li><p><code>.get()</code> 的作用同上，用于获取匹配的第一个元素的文本内容。</p>
</li>
<li><p>这两行代码实现的功能是相同的：从 HTML 文档中提取具有特定结构的元素（在这种情况下是某些 <code>&lt;span&gt;</code> 标签内的文本内容）。区别在于它们的选择器来源不同 — 第一行代码使用的是直接从 HTML 文本创建的选择器，而第二行代码使用的是从 <code>HtmlResponse</code> 对象创建的选择器。这种灵活性使得 Scrapy 可以适应不同的数据提取场景。</p>
</li>
</ul>
<h5 id="结合-CSS-和-XPath-选择器及正则表达式提取数据"><a href="#结合-CSS-和-XPath-选择器及正则表达式提取数据" class="headerlink" title="结合 CSS 和 XPath 选择器及正则表达式提取数据"></a>结合 CSS 和 XPath 选择器及正则表达式提取数据</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select_tet.css(<span class="string">&#x27;a&#x27;</span>).xpath(<span class="string">&#x27;./span/text()&#x27;</span>).re_first(<span class="string">&#x27;.*的&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong><code>select_tet.css(&#39;a&#39;)</code></strong>:<ul>
<li><code>select_tet</code> 应该是一个 <code>Selector</code> 对象，它包含了从网页中提取的 HTML 数据。</li>
<li><code>.css(&#39;a&#39;)</code> 是一个 CSS 选择器，用来选取所有的 <code>&lt;a&gt;</code> 标签。在 Scrapy 中，<code>css</code> 方法用于执行 CSS 选择器查询。</li>
<li>这一步的结果是一个新的 <code>SelectorList</code> 对象，包含了 HTML 文档中所有的 <code>&lt;a&gt;</code> 标签。</li>
</ul>
</li>
<li><strong><code>.xpath(&#39;./span/text()&#39;)</code></strong>:<ul>
<li><code>.xpath()</code> 是一个 XPath 选择器方法，用来进一步从前一步的结果中筛选数据。</li>
<li><code>&#39;./span/text()&#39;</code> 是一个 XPath 查询，它的含义是：在当前节点（这里指的是每个 <code>&lt;a&gt;</code> 标签）的基础上，选择其子节点中的 <code>&lt;span&gt;</code> 标签，并获取这些 <code>&lt;span&gt;</code> 标签的文本内容。</li>
<li>这一步的结果是一个 <code>SelectorList</code> 对象，包含了所有选中 <code>&lt;span&gt;</code> 标签的文本内容。</li>
</ul>
</li>
<li><strong><code>.re_first(&#39;.\*的&#39;)</code></strong>:<ul>
<li><code>.re_first()</code> 是一个正则表达式方法，用于在之前选择的文本中进行模式匹配。</li>
<li><code>&#39;.*的&#39;</code> 是一个正则表达式，<code>.</code> 表示任意字符，<code>*</code> 表示零次或多次重复，<code>的</code> 是要匹配的具体字符。因此，这个表达式的意思是匹配任何以 “的” 结尾的字符串。</li>
<li><code>re_first</code> 表示只提取第一个匹配的结果。如果没有找到匹配项，则返回 <code>None</code>。</li>
</ul>
</li>
</ul>
<p>在 HTML 文档中查找所有 <code>&lt;a&gt;</code> 标签，然后在每个 <code>&lt;a&gt;</code> 标签中查找 <code>&lt;span&gt;</code> 标签的文本内容，并从这些文本中提取第一个以 “的” 结尾的字符串。这种方法在处理 HTML 数据时非常灵活，可以有效地结合不同的选择器和正则表达式来提取复杂的数据结构。</p>
<h2 id="Scrapy-Spider-类"><a href="#Scrapy-Spider-类" class="headerlink" title="Scrapy Spider 类"></a>Scrapy Spider 类</h2><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/bc76cc19-8c16-4849-e146-10e8e01f6600/public"
                      width = "500"
                >

<ul>
<li><p><strong>Spider的名称 (<code>name</code>)</strong>:</p>
<ul>
<li>这是一个字符串，用于定义此蜘蛛的名称。</li>
<li>名称必须唯一，因为它是Scrapy定位和实例化蜘蛛的方式。</li>
<li>这是最重要的蜘蛛属性，必须提供。</li>
</ul>
</li>
<li><p><strong>起始URLs (<code>start_urls</code>)</strong>:</p>
<ul>
<li>这是蜘蛛开始爬取的URL列表。</li>
<li>爬虫的第一页下载将是此列表中的页面。</li>
<li>后续的请求将从这些起始URL中提取的数据中连续生成。</li>
</ul>
</li>
<li><p><strong>自定义设置 (<code>custom_settings</code>)</strong>:</p>
<ul>
<li>这些设置在运行特定蜘蛛时会覆盖项目范围的全局设置。</li>
<li>必须定义为类属性，因为在实例化蜘蛛之前，设置就已经更新。</li>
</ul>
</li>
</ul>
<h3 id="Spider-类的基础结构"><a href="#Spider-类的基础结构" class="headerlink" title="Spider 类的基础结构"></a>Spider 类的基础结构</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Spider</span>(<span class="title class_ inherited__">object_ref</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Base class for scrapy spiders. All spiders must inherit from this</span></span><br><span class="line"><span class="string">    class.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    name = <span class="literal">None</span></span><br><span class="line">    custom_settings = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.name = name</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> <span class="built_in">getattr</span>(self, <span class="string">&#x27;name&#x27;</span>, <span class="literal">None</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;%s must have a name&quot;</span> % <span class="built_in">type</span>(self).__name__)</span><br><span class="line">        self.__dict__.update(kwargs)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;start_urls&#x27;</span>):</span><br><span class="line">            self.start_urls = []</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>class Spider(object_ref)</code>: 这行定义了一个名为 <code>Spider</code> 的类。它从 <code>object_ref</code> 继承，但这里的 <code>object_ref</code> 看起来像是一个错误或者不完整的代码，因为通常 Python 中的类是从 <code>object</code> 类继承的。这可能是一个笔误或者特定项目中的自定义实现。</li>
<li>类的文档字符串说明了这个类是所有 Scrapy Spider 的基类，意味着所有的 Scrapy 爬虫都应该从这个类继承。</li>
<li><code>name = None</code>: 这是一个类级别的属性，用于存储爬虫的名称。在 Scrapy 中，每个爬虫的名称应该是唯一的。</li>
<li><code>custom_settings = None</code>: 这也是一个类级别的属性，用于定义特定于此爬虫的自定义设置，这些设置将覆盖全局设置。</li>
<li><code>def __init__(self, name=None, **kwargs)</code>: 这是 Spider 类的构造函数。它接受一个可选的 <code>name</code> 参数和任意数量的关键字参数（<code>**kwargs</code>）。</li>
<li>这部分代码用于设置爬虫的名称。如果构造函数中提供了 <code>name</code>，它将被用作爬虫的名称。</li>
<li>如果没有提供 <code>name</code> 并且类的 <code>name</code> 属性也没有被设置，会抛出一个 <code>ValueError</code> 异常，因为每个爬虫必须有一个唯一的名称。</li>
<li><code>self.__dict__.update(kwargs)</code>: 这行代码将所有通过 <code>**kwargs</code> 传入的关键字参数添加到类的实例字典中。这允许在创建爬虫实例时传入额外的属性或设置。</li>
<li><code>if not hasattr(self, &#39;start_urls&#39;)</code>: 这里检查实例是否有 <code>start_urls</code> 属性，如果没有，则初始化为空列表。<code>start_urls</code> 是爬虫开始爬取的 URL 列表。</li>
<li>这个类是 Scrapy 爬虫的一个基本框架，提供了命名、自定义设置和初始化的基本机制。任何具体的 Scrapy 爬虫都应该继承这个类，并根据需要提供具体的实现细节。</li>
</ul>
<h3 id="日志系统-Logger"><a href="#日志系统-Logger" class="headerlink" title="日志系统 Logger"></a>日志系统 Logger</h3><p>在 Scrapy 的 Spider 类中，日志系统被用于记录信息、警告、错误等。这对于监控爬虫的行为、调试和记录重要事件非常有用。</p>
<h4 id="Logger-属性"><a href="#Logger-属性" class="headerlink" title="Logger 属性"></a>Logger 属性</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logger</span>(<span class="params">self</span>):</span><br><span class="line">    logger = logging.getLogger(self.name)</span><br><span class="line">    <span class="keyword">return</span> logging.LoggerAdapter(logger, &#123;<span class="string">&#x27;spider&#x27;</span>: self&#125;)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>@property</code>：这是一个 Python 装饰器，用于将一个方法转换为属性。这意味着可以通过 <code>self.logger</code> 访问这个方法返回的值，而不是 <code>self.logger()</code>。</li>
<li><code>logger = logging.getLogger(self.name)</code>：这里创建了一个日志记录器（logger）。<code>getLogger(self.name)</code> 使用爬虫的名称（<code>self.name</code>）获取一个日志记录器实例。如果不存在具有该名称的记录器，将自动创建一个。</li>
<li><code>return logging.LoggerAdapter(logger, &#123;&#39;spider&#39;: self&#125;)</code>：返回一个 <code>LoggerAdapter</code> 实例。这是一个对标准日志记录器的包装，它提供了额外的上下文信息，使得每个日志消息都能够知道是由哪个爬虫实例产生的。这里的上下文信息是 <code>&#123;&#39;spider&#39;: self&#125;</code>，即当前的爬虫实例。</li>
</ul>
<h4 id="Log-方法"><a href="#Log-方法" class="headerlink" title="Log 方法"></a>Log 方法</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">self, message, level=logging.DEBUG, **kw</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Log the given message at the given log level</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This helper wraps a log call to the logger within the spider, but you</span></span><br><span class="line"><span class="string">    can use it directly (e.g. Spider.logger.info(&#x27;msg&#x27;)) or use any other</span></span><br><span class="line"><span class="string">    Python logger too.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    self.logger.log(level, message, **kw)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>def log(self, message, level=logging.DEBUG, **kw)</code>: 这是一个辅助方法，用于在爬虫内部发送日志消息。</li>
<li><code>level=logging.DEBUG</code>：默认的日志级别设置为 DEBUG。日志级别决定了记录的消息类型。常见的级别包括 DEBUG, INFO, WARNING, ERROR, 和 CRITICAL。</li>
<li><code>self.logger.log(level, message, **kw)</code>：这个调用使用了之前定义的 <code>logger</code> 属性。它将消息 <code>message</code> 记录到日志中，级别为 <code>level</code>。<code>**kw</code> 可以传递额外的关键字参数。</li>
</ul>
<h4 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;my_spider&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 使用日志记录信息</span></span><br><span class="line">        self.logger.info(<span class="string">&#x27;Parsing response from %s&#x27;</span>, response.url)</span><br><span class="line">        <span class="comment"># 其他解析逻辑...</span></span><br></pre></td></tr></table></figure></div>

<p>在这个例子中，每当 <code>parse</code> 方法被调用时，都会记录一个包含响应 URL 的信息级别日志。</p>
<h3 id="from-crawler-和-set-crawler-方法"><a href="#from-crawler-和-set-crawler-方法" class="headerlink" title="from_crawler 和 _set_crawler 方法"></a>from_crawler 和 _set_crawler 方法</h3><p>Spider 类的 <code>from_crawler</code> 类方法和 <code>_set_crawler</code> 实例方法。这些方法在 Scrapy 的内部机制中扮演重要角色，尤其是在爬虫的初始化过程中。</p>
<p><strong><code>from_crawler</code> 方法</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler, *args, **kwargs</span>):</span><br><span class="line">    spider = cls(*args, **kwargs)</span><br><span class="line">    spider._set_crawler(crawler)</span><br><span class="line">    <span class="keyword">return</span> spider</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>@classmethod</code>：这是一个 Python 装饰器，用于定义一个类方法。不同于普通的实例方法，类方法接收类本身作为第一个参数（通常命名为 <code>cls</code>）而非类的实例。</li>
<li><code>from_crawler</code> 是 Scrapy 用来创建 Spider 实例的标准方法。它接收一个 <code>crawler</code> 对象作为参数，以及任意数量的额外的位置和关键字参数。</li>
<li><code>spider = cls(*args, **kwargs)</code>：这行代码使用传入的参数创建了一个 Spider 实例。</li>
<li><code>spider._set_crawler(crawler)</code>：这行代码调用 <code>_set_crawler</code> 方法，将 <code>crawler</code> 对象设置到创建的 Spider 实例上。</li>
<li><code>return spider</code>：返回创建好的 Spider 实例。</li>
</ul>
<p><strong><code>_set_crawler</code> 方法</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_set_crawler</span>(<span class="params">self, crawler</span>):</span><br><span class="line">    self.crawler = crawler</span><br><span class="line">    self.settings = crawler.settings</span><br><span class="line">    crawler.signals.connect(self.close, signals.spider_closed)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>这是一个内部方法，用于在 Spider 实例上设置 <code>crawler</code> 对象。</li>
<li><code>self.crawler = crawler</code>：将传入的 <code>crawler</code> 对象赋值给 Spider 实例的 <code>crawler</code> 属性。</li>
<li><code>self.settings = crawler.settings</code>：将 <code>crawler</code> 对象的设置赋值给 Spider 实例的 <code>settings</code> 属性。这样，Spider 可以访问和使用 Scrapy 项目的设置。</li>
<li><code>crawler.signals.connect(self.close, signals.spider_closed)</code>：这行代码连接了 Spider 的 <code>close</code> 方法到 Scrapy 的 <code>spider_closed</code> 信号。当爬虫关闭时，这个信号会被触发，并调用 Spider 的 <code>close</code> 方法。</li>
</ul>
<p>在 Scrapy 框架中，<code>from_crawler</code> 方法通常由框架自身调用，用于创建并初始化 Spider 对象。这个过程包括设置爬虫的配置和信号处理等。一般情况下，开发者无需覆盖这个方法，除非需要进行一些特殊的初始化操作。<code>from_crawler</code> 和 <code>_set_crawler</code> 方法是 Scrapy 框架中 Spider 类的重要组成部分，它们负责爬虫的初始化和设置。这些方法确保了每个 Spider 实例可以访问到它所需的资源和配置，同时也使得爬虫能够正确响应 Scrapy 框架的信号。</p>
<h3 id="start-requests-方法"><a href="#start-requests-方法" class="headerlink" title="start_requests 方法"></a>start_requests 方法</h3><p><code>start_requests</code> 是一个关键方法，用于生成爬虫启动时的初始请求。这个方法只会在爬虫开始时调用一次。下面是对这个方法的代码和概念的详细解释和整理：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">    cls = self.__class__</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.start_urls <span class="keyword">and</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;start_url&#x27;</span>):</span><br><span class="line">        <span class="keyword">raise</span> AttributeError(</span><br><span class="line">            <span class="string">&quot;Crawling could not start: &#x27;start_urls&#x27; not found &quot;</span></span><br><span class="line">            <span class="string">&quot;or empty (but found &#x27;start_url&#x27; attribute instead, &quot;</span></span><br><span class="line">            <span class="string">&quot;did you miss an &#x27;s&#x27;?)&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> method_is_overridden(cls, Spider, <span class="string">&#x27;make_requests_from_url&#x27;</span>):</span><br><span class="line">        warnings.warn(</span><br><span class="line">            <span class="string">&quot;Spider.make_requests_from_url method is deprecated; it &quot;</span></span><br><span class="line">            <span class="string">&quot;won&#x27;t be called in future Scrapy releases. Please &quot;</span></span><br><span class="line">            <span class="string">&quot;override Spider.start_requests method instead (see %s.%s).&quot;</span> % (</span><br><span class="line">                cls.__module__, cls.__name__</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">            <span class="keyword">yield</span> self.make_requests_from_url(url)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">            <span class="keyword">yield</span> Request(url, dont_filter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>def start_requests(self)</code>: 这是 Scrapy Spider 类中的一个实例方法，用于生成初始的网页请求。</li>
<li><code>cls = self.__class__</code>: 获取当前实例的类。</li>
<li><code>if not self.start_urls and hasattr(self, &#39;start_url&#39;)</code>: 这个条件检查是否定义了 <code>start_urls</code> 属性。如果 <code>start_urls</code> 没有定义或为空，并且错误地定义了 <code>start_url</code>（缺少末尾的 ‘s’），则抛出异常。</li>
<li><code>if method_is_overridden(cls, Spider, &#39;make_requests_from_url&#39;)</code>: 检查是否重写了 <code>make_requests_from_url</code> 方法。这个方法已被弃用，Scrapy 鼓励使用 <code>start_requests</code> 方法。</li>
<li><code>warnings.warn(...)</code>: 如果使用了弃用的方法，显示警告信息。</li>
<li><code>for url in self.start_urls</code>: 遍历 <code>start_urls</code> 列表中的每个 URL。</li>
<li><code>yield self.make_requests_from_url(url)</code>: 对于每个 URL，使用 <code>make_requests_from_url</code> 方法生成请求。这是兼容旧代码的方式。</li>
<li><code>yield Request(url, dont_filter=True)</code>: 对于每个 URL，创建一个 Scrapy 的 <code>Request</code> 对象并返回。<code>dont_filter=True</code> 表示对这些请求不应用去重过滤器。</li>
<li>通常，Scrapy 爬虫定义了 <code>start_urls</code> 列表，并依赖 <code>start_requests</code> 方法来为这些 URL 创建初始请求。可以覆盖 <code>start_requests</code> 方法以提供更复杂的启动逻辑，例如从外部数据源读取 URL，或添加特殊的请求头和元数据。</li>
<li><code>start_requests</code> 方法是 Scrapy 爬虫的起点，负责生成爬虫的初始请求。这个方法应返回一个可迭代对象，其中包含了爬虫开始时需要处理的 <code>Request</code> 对象。通过重写这个方法，开发者可以自定义爬虫的启动行为，例如，从外部文件读取起始 URL，或者添加特定的请求参数和头部</li>
</ul>
<h3 id="parse-方法"><a href="#parse-方法" class="headerlink" title="parse 方法"></a>parse 方法</h3><p>这是Scrapy在其请求未指定回调时处理下载的响应时使用的默认回调</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">&#x27;&#123;&#125;.parse callback is not defined&#x27;</span>.<span class="built_in">format</span>(self.__class__.__name__))</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>parse</code> 是 Scrapy 爬虫中最重要的方法之一。它是默认的回调函数，用于处理由 Scrapy 发出的请求所返回的响应。</li>
<li>当一个请求没有指定特定的回调函数时，Scrapy 会自动调用 <code>parse</code> 方法。</li>
<li>在这个示例中，<code>parse</code> 方法被定义为抛出 <code>NotImplementedError</code> 异常。这是一个通常的做法，用来提示开发者应该在自己的爬虫类中覆盖这个方法，提供具体的实现逻辑。</li>
<li>实际使用中，<code>parse</code> 方法通常包含解析响应（<code>response</code> 对象）并提取数据或进一步生成请求（<code>Request</code> 对象）的逻辑。</li>
<li><code>parse</code> 是处理响应的默认方法</li>
</ul>
<h3 id="close-方法"><a href="#close-方法" class="headerlink" title="close 方法"></a>close 方法</h3><ul>
<li><code>close</code> 方法在爬虫关闭时被调用。它是一个钩子（hook），提供了一种机会在爬虫结束时执行某些操作，比如清理资源、保存状态、发出通知等。</li>
<li><code>close</code> 方法的具体实现会根据爬虫的需求而有所不同。在 Scrapy 框架的默认实现中，这个方法可能并未显式地定义，但可以根据需要在自定义爬虫中覆盖它。</li>
<li>在开发 Scrapy 爬虫时，通常需要实现 <code>parse</code> 方法来定义如何处理响应。这可能包括解析 HTML、处理数据和生成新的请求。</li>
<li><code>close</code> 方法则用于定义在爬虫关闭时需要执行的任何清理或结束任务。</li>
<li><code>close</code> 提供了在爬虫结束时执行操作的机会。</li>
</ul>
<h2 id="次级页面抓取及数据传递拼接"><a href="#次级页面抓取及数据传递拼接" class="headerlink" title="次级页面抓取及数据传递拼接"></a>次级页面抓取及数据传递拼接</h2><h3 id="次级页面抓取"><a href="#次级页面抓取" class="headerlink" title="次级页面抓取"></a>次级页面抓取</h3><p>在 Scrapy 爬虫中，处理多级页面通常涉及以下步骤：</p>
<ul>
<li><p>一级页面（列表页）</p>
<ul>
<li>在列表页，会解析出指向详情页的链接，并对每个链接发起请求以进入二级页面。</li>
</ul>
</li>
<li><p>二级页面（详情页）</p>
<ul>
<li>在详情页，会提取所需的具体数据。</li>
</ul>
</li>
</ul>
<p>这个过程通常在 <code>parse</code> 方法（处理列表页）和一个自定义的回调方法（处理详情页）中实现。</p>
<h3 id="详情页抓取"><a href="#详情页抓取" class="headerlink" title="详情页抓取"></a>详情页抓取</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_detail</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>get_detail</code> 是一个自定义的回调方法，用于处理从列表页提取的详情页链接的响应。</li>
<li>这个方法中，编写解析详情页的逻辑，例如提取特定的数据。</li>
</ul>
<h3 id="数据提取示例"><a href="#数据提取示例" class="headerlink" title="数据提取示例"></a>数据提取示例</h3><ul>
<li>第一种方式：<code>&#39;//div[@id=&quot;link-report&quot;]/span[@property=&quot;v:summary&quot;]&#39;</code>。这个 XPath 查询用于提取比较简单的结构的数据。</li>
<li>第二种方式：<code>&#39;//div[@id=&quot;link-report&quot;]/span[@class=&quot;all hidden&quot;]/text()|//div[@id=&quot;link-report&quot;]/span[@property=&quot;v:summary&quot;]/text()&#39;</code>。这个查询适用于当数据结构更复杂或有多种可能的格式时。它使用了 XPath 的联合操作符 <code>|</code> 来匹配多个可能的路径。</li>
</ul>
<h3 id="参数的传递拼接"><a href="#参数的传递拼接" class="headerlink" title="参数的传递拼接"></a>参数的传递拼接</h3><ul>
<li>在 Scrapy 中，<code>meta</code> 参数用于在不同的请求之间传递数据。</li>
<li>当从列表页发起对详情页的请求时，可以使用 <code>meta</code> 参数携带需要在详情页中使用的数据。例如，可能想传递从列表页提取的某些上下文信息到详情页。</li>
</ul>
<p>使用 <code>meta</code> 的示例</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="comment"># 提取详情页链接</span></span><br><span class="line">    detail_url = response.xpath(<span class="string">&#x27;some_xpath_to_detail_url&#x27;</span>).get()</span><br><span class="line">    <span class="comment"># 向详情页发起请求，并传递额外的数据</span></span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(detail_url, callback=self.get_detail, meta=&#123;<span class="string">&#x27;item&#x27;</span>: <span class="string">&#x27;some_data&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure></div>

<p>在这个示例中，<code>parse</code> 方法提取了详情页的链接，并发起了一个新的请求。通过设置 <code>meta=&#123;&#39;item&#39;: &#39;some_data&#39;&#125;</code>，任何想从列表页到详情页传递的数据都可以包含在这个 <code>meta</code> 字典中。</p>
<p>在 Scrapy 爬虫中处理多级页面时，通常会在列表页解析出详情页的链接，并为这些链接发起请求。在处理这些请求的回调方法中，会提取详情页的具体数据。使用 <code>meta</code> 参数可以在不同请求之间传递数据，这对于保持爬虫逻辑的连贯性和传递上下文信息非常重要。</p>
<h3 id="豆瓣为例子"><a href="#豆瓣为例子" class="headerlink" title="豆瓣为例子"></a>豆瓣为例子</h3><h4 id="Spider-文件"><a href="#Spider-文件" class="headerlink" title="Spider 文件"></a>Spider 文件</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> DbItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Db250Spider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;db250&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;movie.douban.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://movie.douban.com/top250&#x27;</span>]</span><br><span class="line">    page_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        node_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> node_list:</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> node_list:</span><br><span class="line">                <span class="comment"># 标题</span></span><br><span class="line">                movie_name = node.xpath(<span class="string">&#x27;./div/a/span/text()&#x27;</span>).get()</span><br><span class="line">                <span class="comment"># 导演</span></span><br><span class="line">                director = node.xpath(<span class="string">&#x27;./div/p/text()&#x27;</span>).get().strip()</span><br><span class="line">                <span class="comment"># 分数</span></span><br><span class="line">                score = node.xpath(<span class="string">&#x27;.//span[@class=&quot;rating_num&quot;]/text()&#x27;</span>).get()</span><br><span class="line"></span><br><span class="line">                item = DbItem()</span><br><span class="line">                item[<span class="string">&quot;movie_name&quot;</span>] = movie_name</span><br><span class="line">                item[<span class="string">&quot;director&quot;</span>] = director</span><br><span class="line">                item[<span class="string">&quot;score&quot;</span>] = score</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 电影详情页</span></span><br><span class="line">                detail_url = node.xpath(<span class="string">&#x27;./div/a/@href&#x27;</span>).get()</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(detail_url, callback=self.get_detail, meta=&#123;<span class="string">&quot;info&quot;</span>:item&#125;)</span><br><span class="line"></span><br><span class="line">            self.page_num +=<span class="number">1</span></span><br><span class="line">            page_url = <span class="string">&#x27;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#x27;</span>.<span class="built_in">format</span>(self.page_num*<span class="number">25</span>)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(page_url, callback=self.parse)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 详情页解析</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_detail</span>(<span class="params">self,response</span>):</span><br><span class="line">        item = DbItem()</span><br><span class="line">        info = response.meta.get(<span class="string">&quot;info&quot;</span>)</span><br><span class="line">        item.update(info)</span><br><span class="line">        description_list = response.xpath(<span class="string">&#x27;//div[@id=&quot;link-report&quot;]/span[@class=&quot;all hidden&quot;]/text()|//div[@id=&quot;link-report&quot;]/span[@property=&quot;v:summary&quot;]/text()&#x27;</span>).getall()</span><br><span class="line">        description = <span class="string">&#x27;&#x27;</span>.join([des.strip() <span class="keyword">for</span> des <span class="keyword">in</span> description_list])</span><br><span class="line"></span><br><span class="line">        item[<span class="string">&quot;description&quot;</span>] = description</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></div>

<p>这段代码是一个 Scrapy 爬虫的示例，用于爬取豆瓣电影 Top 250 的相关信息。下面是对这个代码的详细分析：</p>
<h5 id="爬虫定义"><a href="#爬虫定义" class="headerlink" title="爬虫定义"></a>爬虫定义</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Db250Spider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;db250&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;movie.douban.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://movie.douban.com/top250&#x27;</span>]</span><br><span class="line">    page_num = <span class="number">0</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>这个类继承自<code>scrapy.Spider</code>，是Scrapy框架中用于创建爬虫的基础类。</li>
<li><code>class Db250Spider(scrapy.Spider):</code> 这行定义了一个名为<code>Db250Spider</code>的新类，它继承自<code>scrapy.Spider</code>。这意味着<code>Db250Spider</code>是一个Scrapy爬虫，可以继承并使用Scrapy框架提供的所有功能和属性。</li>
<li><code>name = &#39;db250&#39;</code> 这里设置了爬虫的名字为<code>db250</code>。这个名称是Scrapy项目中唯一的标识符，用于在命令行中指定和运行特定的爬虫。</li>
<li><code>allowed_domains = [&#39;movie.douban.com&#39;]</code> <code>allowed_domains</code>是一个列表，包含了爬虫允许爬取的域名。这有助于确保爬虫不会跨域爬取。在这个例子中，爬虫只会爬取<code>movie.douban.com</code>域下的页面。</li>
<li><code>start_urls = [&#39;https://movie.douban.com/top250&#39;]</code> <code>start_urls</code>是一个包含起始URL的列表。当爬虫启动时，Scrapy会自动开始从这些URL发起请求。在这个例子中，爬虫将从豆瓣电影Top 250的主页开始爬取。</li>
<li><code>page_num = 0</code> <code>page_num</code>是一个类属性，用于跟踪当前爬取的页码。这在处理分页或需要在多个页面之间导航时非常有用。在这个例子中，它被初始化为0。</li>
<li>接下来在这个爬虫类中定义如<code>parse</code>等方法，以指定如何解析和处理每个请求的响应，以及如何从中提取数据或者进一步生成新的请求。例如，解析豆瓣电影Top 250页面的响应，提取电影信息，然后爬取每个电影的详细页面等。</li>
</ul>
<h5 id="主页解析方法-parse"><a href="#主页解析方法-parse" class="headerlink" title="主页解析方法 parse"></a>主页解析方法 parse</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    node_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> node_list:</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> node_list:</span><br><span class="line">            movie_name = node.xpath(<span class="string">&#x27;./div/a/span/text()&#x27;</span>).get()</span><br><span class="line">            director = node.xpath(<span class="string">&#x27;./div/p/text()&#x27;</span>).get().strip()</span><br><span class="line">            score = node.xpath(<span class="string">&#x27;.//span[@class=&quot;rating_num&quot;]/text()&#x27;</span>).get()</span><br><span class="line"></span><br><span class="line">            item = DbItem()</span><br><span class="line">            item[<span class="string">&quot;movie_name&quot;</span>] = movie_name</span><br><span class="line">            item[<span class="string">&quot;director&quot;</span>] = director</span><br><span class="line">            item[<span class="string">&quot;score&quot;</span>] = score</span><br><span class="line"></span><br><span class="line">            detail_url = node.xpath(<span class="string">&#x27;./div/a/@href&#x27;</span>).get()</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(detail_url, callback=self.get_detail, meta=&#123;<span class="string">&quot;info&quot;</span>:item&#125;)</span><br><span class="line"></span><br><span class="line">        self.page_num += <span class="number">1</span></span><br><span class="line">        page_url = <span class="string">&#x27;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#x27;</span>.<span class="built_in">format</span>(self.page_num*<span class="number">25</span>)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(page_url, callback=self.parse)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure></div>

<h6 id="0-方法定义"><a href="#0-方法定义" class="headerlink" title="0. 方法定义"></a>0. 方法定义</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>self</code> 是对当前对象实例（即Spider对象）的引用。</li>
<li><code>response</code> 是一个包含了HTTP响应数据的对象。它包括响应内容（通常是HTML）、URL、HTTP头等信息。</li>
<li><code>parse</code>方法的主要功能是处理<code>response</code>，提取有用信息，或者进一步生成需要跟进的URL请求。</li>
<li>在这个方法中，通常会使用XPath或CSS选择器来解析响应内容（如提取数据）。</li>
<li>数据提取：可以使用<code>response.xpath()</code>或<code>response.css()</code>方法来选择HTML元素，然后提取所需数据。</li>
<li>生成跟进请求：如果页面中有链接到其他页面，且想爬取那些页面的数据，可以通过<code>scrapy.Request</code>生成新的请求。</li>
<li><code>parse</code>方法可以返回以下类型的对象：<ul>
<li>字典（在Python中通常是通过yield语句生成）。</li>
<li><code>scrapy.Item</code>对象，这是一个更结构化的方式来管理数据。</li>
<li><code>Request</code>对象，用于生成新的爬取请求。</li>
<li>或者它们的组合。</li>
</ul>
</li>
<li><code>parse</code>方法是Scrapy爬虫的核心，用于处理响应并从中提取信息，或者根据页面中的链接生成新的请求。</li>
</ul>
<h6 id="1-解析节点列表"><a href="#1-解析节点列表" class="headerlink" title="1. 解析节点列表"></a>1. 解析节点列表</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;info&quot;]&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>这行代码使用 XPath 查询从响应中提取所有类属性为 <code>&quot;info&quot;</code> 的 <code>&lt;div&gt;</code> 元素。</li>
<li>这些 <code>&lt;div&gt;</code> 元素包含了电影的主要信息，并存储在 <code>node_list</code> 中。</li>
<li><code>response</code>:<ul>
<li>这通常是一个由Scrapy框架（或类似的库）提供的响应对象。</li>
<li>它代表了一个从网页请求中获得的HTTP响应。</li>
<li>这个对象包含了完整的网页数据，通常是HTML格式。</li>
</ul>
</li>
<li><code>.xpath()</code>:<ul>
<li>这是一个方法，用于对<code>response</code>对象中的HTML内容执行XPath查询。</li>
<li>XPath是一种在XML和HTML文档中查找信息的语言。</li>
<li>通过XPath，可以导航文档的结构，并选择需要的元素、属性等。</li>
</ul>
</li>
<li><code>&#39;//div[@class=&quot;info&quot;]&#39;</code>:<ul>
<li>这是传递给<code>.xpath()</code>方法的XPath查询表达式。</li>
<li>让我们分解这个表达式：<ul>
<li><code>//</code>: 这个符号表示选择文档中的所有匹配元素，而不仅仅是直接子元素。它在整个文档中进行搜索。</li>
<li><code>div</code>: 这指定了想要选择的元素类型。在这种情况下，它是<code>&lt;div&gt;</code>元素。</li>
<li><code>[@class=&quot;info&quot;]</code>: 这是一个条件（谓语），用来进一步细化选择。它指定只选择那些具有<code>class=&quot;info&quot;</code>属性的<code>&lt;div&gt;</code>元素。<ul>
<li><code>@class</code>: 表示选择元素的<code>class</code>属性。</li>
<li><code>&quot;info&quot;</code>: 表示属性值必须精确匹配<code>info</code>字符串。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>在网页的响应内容中选择所有<code>class</code>属性值为<code>info</code>的<code>&lt;div&gt;</code>元素，并将这些元素存储在<code>node_list</code>变量中。这个列表可以被进一步用于抽取数据、分析或其他处理。在网页抓取和数据提取的过程中，这是一种非常常见的做法。</li>
</ul>
<h6 id="2-判断并遍历节点"><a href="#2-判断并遍历节点" class="headerlink" title="2. 判断并遍历节点"></a>2. 判断并遍历节点</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> node_list:</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> node_list:</span><br></pre></td></tr></table></figure></div>

<ul>
<li>首先检查 <code>node_list</code> 是否非空，确保有要处理的节点。</li>
<li>然后遍历这些节点，每个节点 (<code>node</code>) 代表一个电影条目。</li>
<li><code>if node_list:</code><ul>
<li>这是一个条件语句，检查<code>node_list</code>是否非空。在Python中，空列表（<code>[]</code>）被认为是<code>False</code>，而非空列表被认为是<code>True</code>。</li>
<li>这个条件确保仅当<code>node_list</code>中有元素时，即列表不为空时，才会执行下面的代码块。这是一个良好的编程实践，可以避免在空列表上执行操作时出现错误。</li>
</ul>
</li>
<li><code>for node in node_list:</code><ul>
<li>这是一个<code>for</code>循环，用于遍历<code>node_list</code>中的每个元素。</li>
<li>在每次迭代中，<code>node</code>变量会被赋予<code>node_list</code>中的当前元素。</li>
<li>如果<code>node_list</code>是由之前提到的<code>response.xpath(&#39;//div[@class=&quot;info&quot;]&#39;)</code>表达式返回的，那么每个<code>node</code>很可能是一个表示HTML <code>&lt;div&gt;</code>元素的对象。这种对象通常提供了进一步提取数据（如文本内容、属性等）的方法。</li>
</ul>
</li>
<li>如果<code>node_list</code>不为空，即至少包含一个元素，那么对于列表中的每个元素（每个<code>&lt;div class=&quot;info&quot;&gt;</code>），执行循环体中的代码。循环体的具体内容没有提供，但通常它会包含进一步处理每个<code>node</code>的代码，例如提取信息、打印数据、存储结果等。</li>
</ul>
<h6 id="3-提取电影信息"><a href="#3-提取电影信息" class="headerlink" title="3. 提取电影信息"></a>3. 提取电影信息</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">movie_name = node.xpath(<span class="string">&#x27;./div/a/span/text()&#x27;</span>).get()</span><br><span class="line">director = node.xpath(<span class="string">&#x27;./div/p/text()&#x27;</span>).get().strip()</span><br><span class="line">score = node.xpath(<span class="string">&#x27;.//span[@class=&quot;rating_num&quot;]/text()&#x27;</span>).get()</span><br></pre></td></tr></table></figure></div>

<ul>
<li>提取电影名称：使用 XPath 查询从当前节点 (<code>node</code>) 中提取电影名称。</li>
<li>提取导演信息：获取导演信息，这里使用 <code>.strip()</code> 来移除字符串开头和结尾的空白字符。</li>
<li>提取电影评分：获取电影评分，这里使用 <code>.//</code> 从当前节点的所有子孙节点中查找符合条件的节点。</li>
<li><code>movie_name = node.xpath(&#39;./div/a/span/text()&#39;).get()</code><ul>
<li>这行代码从当前的<code>node</code>中提取电影名称。</li>
<li><code>./div/a/span/text()</code>: XPath查询语句。<ul>
<li><code>./</code> 表示从当前节点开始。</li>
<li><code>div/a/span</code> 定位到一个<code>&lt;span&gt;</code>元素，这个元素是一个<code>&lt;a&gt;</code>元素的子元素，而<code>&lt;a&gt;</code>元素又是一个<code>&lt;div&gt;</code>元素的子元素。</li>
<li><code>/text()</code> 选择这个<code>&lt;span&gt;</code>元素的文本内容。</li>
</ul>
</li>
<li><code>.get()</code> 是一个方法，用来获取XPath查询结果的第一个匹配项。</li>
</ul>
</li>
<li><code>director = node.xpath(&#39;./div/p/text()&#39;).get().strip()</code><ul>
<li>这行代码用来提取导演的名称。</li>
<li><code>./div/p/text()</code>: XPath查询语句。<ul>
<li>类似于前面的查询，但这次是选择一个<code>&lt;p&gt;</code>元素的文本内容。</li>
</ul>
</li>
<li><code>.get()</code> 同样获取第一个匹配项的文本。</li>
<li><code>.strip()</code> 是一个字符串方法，用来移除字符串首尾的空白字符（如空格、换行符等）。</li>
</ul>
</li>
<li><code>score = node.xpath(&#39;.//span[@class=&quot;rating_num&quot;]/text()&#39;).get()</code><ul>
<li>这行代码用于提取电影评分。</li>
<li><code>.//span[@class=&quot;rating_num&quot;]/text()</code>: XPath查询语句。<ul>
<li><code>.//</code> 表示在当前节点及其子节点中查找。</li>
<li><code>span[@class=&quot;rating_num&quot;]</code> 选择所有<code>class</code>属性为<code>rating_num</code>的<code>&lt;span&gt;</code>元素。</li>
<li><code>/text()</code> 同样选择这些<code>&lt;span&gt;</code>元素的文本内容。</li>
</ul>
</li>
<li><code>.get()</code> 获取第一个匹配结果。</li>
</ul>
</li>
<li>这三行代码是在遍历一个包含多个节点（每个节点代表一个电影信息）的列表时，用来从每个节点中提取特定信息的标准操作。这种方式在爬虫和数据提取的过程中非常常见，特别是当处理诸如电影列表页面这样的结构化数据时。</li>
</ul>
<h6 id="4-创建并填充-Scrapy-Item"><a href="#4-创建并填充-Scrapy-Item" class="headerlink" title="4. 创建并填充 Scrapy Item"></a>4. 创建并填充 Scrapy Item</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">item = DbItem()</span><br><span class="line">item[<span class="string">&quot;movie_name&quot;</span>] = movie_name</span><br><span class="line">item[<span class="string">&quot;director&quot;</span>] = director</span><br><span class="line">item[<span class="string">&quot;score&quot;</span>] = score</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>item = DbItem()</code><ul>
<li>这行代码创建了一个<code>DbItem</code>的实例。通常在Scrapy项目中定义，用于表示提取的数据结构。</li>
<li>在Scrapy中，<code>Item</code>对象用于定义存储爬取数据的结构。它们类似于Python中的字典，但提供额外的保护和便利，如字段定义和类型检查。</li>
</ul>
</li>
<li><code>item[&quot;movie_name&quot;] = movie_name</code><ul>
<li>这行代码将之前提取的<code>movie_name</code>（电影名称）赋值给<code>item</code>的<code>&quot;movie_name&quot;</code>键。</li>
<li>这意味着<code>DbItem</code>类中定义了一个字段<code>movie_name</code>，用于存储电影名称。</li>
</ul>
</li>
<li><code>item[&quot;director&quot;] = director</code><ul>
<li>类似地，这行代码将提取的<code>director</code>（导演名称）赋值给<code>item</code>的<code>&quot;director&quot;</code>键。</li>
</ul>
</li>
<li><code>item[&quot;score&quot;] = score</code><ul>
<li>同样地，这行代码将提取的<code>score</code>（电影评分）赋值给<code>item</code>的<code>&quot;score&quot;</code>键。</li>
</ul>
</li>
<li>创建了一个<code>DbItem</code>实例，并填充了从网页中提取的数据：电影名称、导演和评分。在Scrapy中，这样的<code>item</code>对象通常会被进一步传递到管道（pipelines），在那里可以进行如存储到数据库、进行数据清洗或其他处理的操作。使用<code>Item</code>对象的好处在于它提供了结构化的数据存储方式，有助于维护代码的清晰度和可维护性。</li>
</ul>
<h6 id="5-请求电影详情页"><a href="#5-请求电影详情页" class="headerlink" title="5. 请求电影详情页"></a>5. 请求电影详情页</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">detail_url = node.xpath(<span class="string">&#x27;./div/a/@href&#x27;</span>).get()</span><br><span class="line"><span class="keyword">yield</span> scrapy.Request(detail_url, callback=self.get_detail, meta=&#123;<span class="string">&quot;info&quot;</span>:item&#125;)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>detail_url = node.xpath(&#39;./div/a/@href&#39;).get()</code><ul>
<li>这行代码使用XPath选择器从当前处理的节点（<code>node</code>）中提取详情页面的链接。</li>
<li><code>./div/a/@href</code>: XPath查询语句。<ul>
<li><code>./</code> 表示从当前节点开始。</li>
<li><code>div/a</code> 定位到当前节点下的<code>&lt;div&gt;</code>元素中的<code>&lt;a&gt;</code>元素。</li>
<li><code>@href</code> 获取这个<code>&lt;a&gt;</code>元素的<code>href</code>属性，即链接地址。</li>
</ul>
</li>
<li><code>.get()</code> 方法从XPath选择器返回的结果中获取第一个匹配项，也就是详情页面的URL。</li>
</ul>
</li>
<li><code>yield scrapy.Request(detail_url, callback=self.get_detail, meta=&#123;&quot;info&quot;:item&#125;)</code><ul>
<li>这行代码创建一个新的Scrapy请求（<code>Request</code>），用于爬取在<code>detail_url</code>中找到的URL。</li>
<li><code>scrapy.Request(detail_url, ...)</code> 创建一个新的请求对象，其目标是<code>detail_url</code>。</li>
<li><code>callback=self.get_detail</code> 设置当请求得到响应时应调用的回调方法为<code>get_detail</code>。这意味着，当Scrapy成功访问<code>detail_url</code>并获得响应时，它将自动调用<code>get_detail</code>方法来处理响应。</li>
<li><code>meta=&#123;&quot;info&quot;:item&#125;</code> 将一个额外的信息字典<code>&#123;&quot;info&quot;: item&#125;</code>传递给回调方法。这通常用于在不同请求之间传递数据。在这里，<code>item</code>对象（可能包含已经提取的某些数据，如电影名、导演、评分等）被传递到详情页面的处理方法中。</li>
</ul>
</li>
<li>在Scrapy中，<code>yield</code>关键字被用于生成请求而不是直接返回它们。这允许Scrapy处理请求队列，并根据可用的资源和设置来调度这些请求。这种方法使得爬虫能够有效地管理大量的并行请求，同时避免过载目标服务器或被封禁。</li>
</ul>

  <div class="note-large orange">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-info"></i><p>信息</p>

    </div>
    <div class="notel-content">
      <p>在编程中，特别是在异步编程和事件驱动编程中，”callback”（回调函数）是一个非常重要的概念。回调函数是传递给另一个函数或方法的函数，它在那个函数或方法执行完某些操作之后被调用。在不同的编程语言和框架中，回调函数的具体实现和使用方式可能有所不同，但其基本概念是相似的。</p>
<p><strong>回调函数的基本原理</strong></p>
<ol>
<li><strong>定义回调函数</strong>: 回调函数是定义的一个函数，它将在未来的某个时间点被调用。这个函数通常定义了在某些操作完成后应该执行的操作。</li>
<li><strong>将回调函数作为参数传递</strong>: 将这个回调函数作为参数传递给另一个函数或方法。这通常发生在希望在那个函数完成其主要操作后执行一些额外操作的情况下。</li>
<li><strong>异步操作或事件处理</strong>: 在进行异步操作（如网络请求、文件读写等）或处理事件（如用户输入、定时器触发等）时，回调函数特别有用。在这些情况下，不能立即得到结果，回调函数提供了一种在操作完成时得到通知并执行相关代码的方式。</li>
</ol>
<p><strong>回调函数的例子</strong></p>
<p>在Python中，回调函数的一个常见例子是在多线程或网络请求中使用：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_success</span>(<span class="params">response</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Success:&quot;</span>, response.text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_error</span>(<span class="params">error</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Error:&quot;</span>, error)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送异步HTTP请求</span></span><br><span class="line">requests.get(<span class="string">&quot;https://www.example.com&quot;</span>, success=on_success, error=on_error)</span><br></pre></td></tr></table></figure></div>

<p>在这个例子中，<code>on_success</code>和<code>on_error</code>是回调函数。它们分别在HTTP请求成功时和出错时被调用。</p>
<p><strong>在Scrapy中的回调函数</strong></p>
<p>在Scrapy这类网络爬虫框架中，回调函数用于处理从网页请求中返回的响应。例如：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;example_spider&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=<span class="string">&quot;https://www.example.com&quot;</span>, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 处理响应的代码</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></div>

<p>在这里，<code>parse</code>方法是一个回调函数，当Scrapy框架从指定的URL接收到响应时调用。</p>
<p>总的来说，回调函数是一种在某个操作完成后（通常是异步操作）再执行的函数。它们在事件驱动的程序设计中尤为重要，允许程序在等待一个操作完成时继续执行其他任务，并在适当的时候处理操作结果。</p>

    </div>
  </div>

<h6 id="6-分页处理"><a href="#6-分页处理" class="headerlink" title="6. 分页处理"></a>6. 分页处理</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.page_num += <span class="number">1</span></span><br><span class="line">page_url = <span class="string">&#x27;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#x27;</span>.<span class="built_in">format</span>(self.page_num*<span class="number">25</span>)</span><br><span class="line"><span class="keyword">yield</span> scrapy.Request(page_url, callback=self.parse)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>self.page_num += 1</code><ul>
<li>这行代码将类属性<code>page_num</code>的值增加1。这个属性用于追踪当前爬取的页数。在每次处理完一个页面后，递增<code>page_num</code>以移至下一个页面。</li>
</ul>
</li>
<li><code>page_url = &#39;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#39;.format(self.page_num*25)</code><ul>
<li>这里构造了下一个页面的URL。</li>
<li><code>&#39;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#39;</code>是基本的URL格式。豆瓣电影Top 250页面的分页通过URL参数<code>start</code>实现，其中<code>start</code>表示列表的起始位置。</li>
<li><code>.format(self.page_num*25)</code>用于插入计算后的起始位置。由于每页显示25部电影，那么每递增一页，<code>start</code>的值应增加25。例如，第一页是从0开始（<code>0*25</code>），第二页是从25开始（<code>1*25</code>），依此类推。</li>
</ul>
</li>
<li><code>yield scrapy.Request(page_url, callback=self.parse)</code><ul>
<li>这行代码生成了一个新的Scrapy请求，用于爬取下一个页面。</li>
<li><code>scrapy.Request(page_url, callback=self.parse)</code>创建一个新的请求对象，其目标是<code>page_url</code>。</li>
<li><code>callback=self.parse</code>指定当请求得到响应时，应该调用<code>parse</code>方法来处理该响应。<code>parse</code>是Scrapy爬虫中的标准方法，用于处理和解析响应内容。</li>
<li>使用<code>yield</code>关键字来生成请求对象，允许Scrapy根据需要来调度和处理这些请求。</li>
</ul>
</li>
<li>在Scrapy中，适当地管理分页是提取多页数据的关键。代码正确地实现了递增页码并构建新页面URL的逻辑。通过在<code>parse</code>方法中重复这个过程，爬虫可以遍历并爬取整个列表的所有页面。</li>
</ul>
<h6 id="7-处理结束"><a href="#7-处理结束" class="headerlink" title="7. 处理结束"></a>7. 处理结束</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>如果 <code>node_list</code> 为空，说明没有更多的电影信息可以处理，函数返回并结束。</li>
<li>如果条件不满足（<code>else</code>部分），<code>return</code>语句会被执行。在Python中，<code>return</code>语句用于从函数返回。如果<code>return</code>后没有任何值或表达式，它默认返回<code>None</code>。在这个上下文中，它表示不再继续生成新的请求，爬虫的这个部分将停止执行。</li>
</ul>
<p>这个 <code>parse</code> 方法展示了如何在 Scrapy 中处理分页和详情页的链接提取、数据抓取和传递。它有效地结合了 XPath 选择器、Scrapy Item 的使用，以及 Scrapy 的请求和响应机制。</p>
<h5 id="详情页解析方法-get-detail"><a href="#详情页解析方法-get-detail" class="headerlink" title="详情页解析方法 get_detail"></a>详情页解析方法 get_detail</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_detail</span>(<span class="params">self, response</span>):</span><br><span class="line">    item = DbItem()</span><br><span class="line">    info = response.meta.get(<span class="string">&quot;info&quot;</span>)</span><br><span class="line">    item.update(info)</span><br><span class="line">    description_list = response.xpath(<span class="string">&#x27;//div[@id=&quot;link-report&quot;]/span[@class=&quot;all hidden&quot;]/text()|//div[@id=&quot;link-report&quot;]/span[@property=&quot;v:summary&quot;]/text()&#x27;</span>).getall()</span><br><span class="line">    description = <span class="string">&#x27;&#x27;</span>.join([des.strip() <span class="keyword">for</span> des <span class="keyword">in</span> description_list])</span><br><span class="line"></span><br><span class="line">    item[<span class="string">&quot;description&quot;</span>] = description</span><br><span class="line">    <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></div>

<h6 id="0-方法定义-1"><a href="#0-方法定义-1" class="headerlink" title="0. 方法定义"></a>0. 方法定义</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_detail</span>(<span class="params">self, response</span>):</span><br></pre></td></tr></table></figure></div>

<ul>
<li>这是一个在 Scrapy 爬虫中定义的 <code>get_detail</code> 方法，它是一个回调函数，用于处理电影详情页的响应。</li>
<li><code>response</code> 参数是 Scrapy 传递给这个方法的响应对象，包含了详情页的数据。</li>
<li>这个方法定义表明<code>get_detail</code>是一个实例方法，用于处理从一个特定的请求返回的响应。这个方法接收一个参数<code>response</code>，它包含了对应请求的HTTP响应。</li>
</ul>
<h6 id="1-初始化和更新-Item"><a href="#1-初始化和更新-Item" class="headerlink" title="1. 初始化和更新 Item"></a>1. 初始化和更新 Item</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">item = DbItem()</span><br><span class="line">info = response.meta.get(<span class="string">&quot;info&quot;</span>)</span><br><span class="line">item.update(info)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>item = DbItem()</code>：创建一个 <code>DbItem</code> 对象。<code>DbItem</code> 通常是一个在 Scrapy 项目中定义的 Item 类，用于结构化存储提取的数据。继承自<code>scrapy.Item</code>，用于指定希望收集的数据字段。</li>
<li><code>info = response.meta.get(&quot;info&quot;)</code>：从响应的 <code>meta</code> 属性中获取传递过来的电影基本信息。这些信息之前在处理列表页时已经被提取并通过 <code>meta</code> 参数传递到这个方法。这里从<code>response.meta</code>中提取了之前传递的元数据。<code>response.meta</code>是一个字典，用于在Scrapy的不同请求之间传递数据。在这个例子中，它被用来传递之前页面上已经抓取的数据。</li>
<li><code>item.update(info)</code>：将获取到的电影基本信息更新到 <code>item</code> 对象中。这行代码将从上一个页面提取的数据（存储在<code>info</code>字典中）合并到新创建的<code>item</code>对象中。</li>
</ul>
<h6 id="2-提取电影描述信息"><a href="#2-提取电影描述信息" class="headerlink" title="2. 提取电影描述信息"></a>2. 提取电影描述信息</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">description_list = response.xpath(<span class="string">&#x27;//div[@id=&quot;link-report&quot;]/span[@class=&quot;all hidden&quot;]/text()|//div[@id=&quot;link-report&quot;]/span[@property=&quot;v:summary&quot;]/text()&#x27;</span>).getall()</span><br><span class="line">description = <span class="string">&#x27;&#x27;</span>.join([des.strip() <span class="keyword">for</span> des <span class="keyword">in</span> description_list])</span><br></pre></td></tr></table></figure></div>

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/4d3dc2ee-996e-4b93-1c43-5d9e56066700/public"
                      width = "500"
                >

<ul>
<li><code>response.xpath()</code> 方法用于执行XPath查询。它从响应的HTML中选择指定的元素。</li>
<li>XPath查询字符串有两部分，通过（并集运算符）连接：<ul>
<li><code>//div[@id=&quot;link-report&quot;]/span[@class=&quot;all hidden&quot;]/text()</code>: 选取所有<code>id</code>为<code>&quot;link-report&quot;</code>的<code>&lt;div&gt;</code>元素中，类名为<code>&quot;all hidden&quot;</code>的<code>&lt;span&gt;</code>子元素的文本内容。</li>
<li><code>//div[@id=&quot;link-report&quot;]/span[@property=&quot;v:summary&quot;]/text()</code>: 选取所有<code>id</code>为<code>&quot;link-report&quot;</code>的<code>&lt;div&gt;</code>元素中，<code>property</code>属性为<code>&quot;v:summary&quot;</code>的<code>&lt;span&gt;</code>子元素的文本内容。</li>
</ul>
</li>
<li><code>.getall()</code> 方法获取所有匹配的元素的文本内容，返回一个字符串列表</li>
<li>将<code>description_list</code>中的所有文本片段合并成一个单一的字符串。</li>
<li><code>[des.strip() for des in description_list]</code> 是一个列表推导式，用于遍历<code>description_list</code>中的每个元素（<code>des</code>），并对每个元素应用<code>.strip()</code>方法。<code>.strip()</code>方法移除字符串两端的空白字符（包括空格、换行符等）。</li>
<li><code>&#39;&#39;.join([...])</code> 方法将经过清洁的字符串列表连接成一个单一的字符串。空字符串<code>&#39;&#39;</code>作为连接符，意味着直接将文本片段连在一起，没有额外的字符插入。</li>
<li>这段代码的作用是从HTML响应中提取电影描述，可能包括多个文本片段，并将它们清洁和合并为一个完整的描述字符串。这种方法在处理包含多个文本块或可选文本块（例如有些电影可能只有一种描述格式）的HTML页面时非常有用。</li>
</ul>
<h6 id="3-设置描述并返回-Item"><a href="#3-设置描述并返回-Item" class="headerlink" title="3. 设置描述并返回 Item"></a>3. 设置描述并返回 Item</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">item[<span class="string">&quot;description&quot;</span>] = description</span><br><span class="line"><span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>item[&quot;description&quot;] = description</code><ul>
<li>这行代码将之前提取并合并的描述文本（<code>description</code>）赋值给<code>item</code>对象的<code>&quot;description&quot;</code>字段。</li>
<li><code>item</code>是一个类似于字典的对象，用于存储提取的数据。在Scrapy中，这通常是一个继承自<code>scrapy.Item</code>的类的实例，用来定义和存储爬取的数据结构。</li>
<li>这里，<code>&quot;description&quot;</code>字段是之前定义在<code>DbItem</code>（或类似的Item类）中的一个字段，用于保存电影的描述文本。</li>
</ul>
</li>
<li><code>yield item</code><ul>
<li>使用<code>yield</code>关键字来产生<code>item</code>对象，将其传递给Scrapy的管道系统（Pipelines）。</li>
<li>在Scrapy中，<code>yield</code>的使用允许框架接管并异步处理这些item对象。这些item随后会通过定义好的管道进行处理，例如进行数据清洗、验证、存储到数据库等操作。</li>
<li>这种基于生成器的方法使得Scrapy能够有效地处理大量数据，并允许多个item同时在管道中进行处理，从而提高整个爬取和数据处理的效率。</li>
</ul>
</li>
</ul>
<p>这两行代码的作用是将提取的描述文本保存到一个item对象中，并将这个对象传递给Scrapy的后续处理流程，如管道处理。这是Scrapy爬虫中处理和传递数据的常见模式。</p>
<h4 id="Items-文件"><a href="#Items-文件" class="headerlink" title="Items 文件"></a>Items 文件</h4><p>定义了一个 Scrapy Item 类，名为 <code>DbItem</code>，用于在 Scrapy 爬虫中存储和组织爬取的数据。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DbItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    movie_name = scrapy.Field()</span><br><span class="line">    director = scrapy.Field()</span><br><span class="line">    score = scrapy.Field()</span><br><span class="line">    description = scrapy.Field()</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>import scrapy</code>: 导入 Scrapy 模块，这是使用 Scrapy 框架的基础。</li>
<li><code>class DbItem(scrapy.Item)</code>: 定义了一个名为 <code>DbItem</code> 的类，该类继承自 <code>scrapy.Item</code>。在 Scrapy 中，<code>Item</code> 类用于定义一种数据结构，使得爬取的数据可以被结构化和标准化。</li>
<li><code>movie_name = scrapy.Field()</code>: 定义了一个字段 <code>movie_name</code> 来存储电影的名称。使用 <code>scrapy.Field()</code> 表示这是一个用于存储数据的字段。</li>
<li><code>director = scrapy.Field()</code>: 定义了一个字段 <code>director</code> 用于存储电影的导演信息。</li>
<li><code>score = scrapy.Field()</code>: 定义了一个字段 <code>score</code> 用于存储电影的评分。</li>
<li><code>description = scrapy.Field()</code>: 定义了一个字段 <code>description</code> 用于存储电影的详细描述。</li>
<li>在 Scrapy 爬虫中，<code>Item</code> 类被用来收集和整理爬取的数据。例如，在爬取一个电影网站时，可以使用 <code>DbItem</code> 实例来存储每部电影的相关信息。这样的数据结构化处理使得数据的存储、输出和后续处理变得更加方便和标准化。</li>
</ul>
<p><code>DbItem</code> 类是 Scrapy 项目中用于定义和存储特定数据结构的方式。它允许爬虫以一种清晰和一致的方式处理爬取的数据。在这个例子中，<code>DbItem</code> 用于存储电影名称、导演、评分和描述等信息，从而使得数据在 Scrapy 爬虫的整个流程中易于处理和维护。</p>
<h4 id="Pipelines-文件"><a href="#Pipelines-文件" class="headerlink" title="Pipelines 文件"></a>Pipelines 文件</h4><p>定义了一个 Scrapy 的 <code>Pipeline</code> 类，名为 <code>DbPipeline</code>，用于处理由爬虫提取的数据。在 Scrapy 中，<code>Pipeline</code> 用于数据的清洗、验证和存储。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DbPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.f = <span class="built_in">open</span>(<span class="string">&#x27;film.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        json_str = json.dumps(<span class="built_in">dict</span>(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">        self.f.write(json_str)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.f.close()</span><br></pre></td></tr></table></figure></div>

<h5 id="1-初始化及打开文件"><a href="#1-初始化及打开文件" class="headerlink" title="1. 初始化及打开文件"></a>1. 初始化及打开文件</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">    self.f = <span class="built_in">open</span>(<span class="string">&#x27;film.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p><code>class DbPipeline:</code> 定义了一个名为<code>DbPipeline</code>的新类。</p>
</li>
<li><p><code>def open_spider(self, spider)</code>: 这个<code>open_spider</code>方法在爬虫开始时被调用。用于进行一些初始化工作。</p>
</li>
<li><p><code>self.f = open(&#39;film.txt&#39;,&#39;w&#39;, encoding=&#39;utf-8&#39;)</code>: 这个方法打开一个名为<code>film.txt</code>的文件用于写入（’w’模式），并且指定编码为UTF-8。这是为了确保可以正确写入包含非ASCII字符（如中文）的文本。</p>
</li>
<li><p>文件对象被赋值给<code>self.f</code>，这样在这个类的其他方法中也能使用这个文件对象。</p>
</li>
</ul>
<h5 id="2-处理并存储数据"><a href="#2-处理并存储数据" class="headerlink" title="2. 处理并存储数据"></a>2. 处理并存储数据</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">    json_str = json.dumps(<span class="built_in">dict</span>(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">    self.f.write(json_str)</span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>def process_item(self, item, spider)</code>: 这个方法会被 Scrapy 框架自动调用，每次爬虫提取出一个 <code>item</code> 时都会执行。<code>process_item</code>方法是管道处理每个item的核心方法。</li>
<li><code>json_str = json.dumps(dict(item), ensure_ascii=False) + &#39;\n&#39;</code>: 将 <code>item</code> 对象转换成 JSON 字符串。<code>ensure_ascii=False</code> 参数确保非 ASCII 字符（如中文）被正确处理。</li>
<li><code>self.f.write(json_str)</code>: 将 JSON 字符串写入之前打开的文件。然后，这个JSON字符串（<code>json_str</code>）被写入到前面打开的文件中。每个item占一行，因为在字符串末尾添加了换行符<code>\n</code>。</li>
<li><code>return item</code>: 返回 <code>item</code>，以便它能够传递到其他可能存在的 pipeline 或最终输出。</li>
</ul>
<h5 id="3-关闭文件"><a href="#3-关闭文件" class="headerlink" title="3. 关闭文件"></a>3. 关闭文件</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">    self.f.close()</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>def close_spider(self, spider)</code>: 在爬虫结束时被调用。当爬虫关闭时，Scrapy框架会调用<code>close_spider</code>方法。</li>
<li><code>self.f.close()</code>: 关闭打开的文件。这是一个重要的步骤，确保数据被完整写入并且文件正常关闭。这个方法关闭了文件对象<code>self.f</code>。这是一个良好的实践，可以确保所有数据都被写入到文件中，并且文件正确地关闭。</li>
</ul>
<p>在 Scrapy 项目中，<code>Pipeline</code> 类用于处理从爬虫传递过来的数据。在这个例子中，<code>DbPipeline</code> 负责将每个电影的信息以 JSON 格式保存到一个文本文件中。这样的处理方式适合于数据的持久化存储，例如保存到文件、数据库等。</p>
<p><code>DbPipeline</code> 类演示了 Scrapy 爬虫中如何使用 pipeline 对提取的数据进行进一步的处理和存储。通过在爬虫启动和关闭时执行相关操作，并在提取数据时进行处理，pipeline 为数据的后续使用提供了一个有效的方式。</p>
<h4 id="Settings-文件"><a href="#Settings-文件" class="headerlink" title="Settings 文件"></a>Settings 文件</h4><p><code>settings.py</code> 是 Scrapy 项目的配置文件，用于定义爬虫的各种设置。这些设置影响着爬虫的行为方式。</p>
<p>此处删除了大部分注释</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Scrapy settings for db project</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">&#x27;db&#x27;</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">&#x27;db.spiders&#x27;</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;db.spiders&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line"><span class="comment">#USER_AGENT = &#x27;db (+http://www.yourdomain.com)&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">  <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure item pipelines</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;db.pipelines.DbPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h5 id="基本设置"><a href="#基本设置" class="headerlink" title="基本设置"></a>基本设置</h5><ul>
<li><code>BOT_NAME = &#39;db&#39;</code>: 定义了爬虫项目的名称，这里是 <code>&#39;db&#39;</code>。</li>
<li><code>SPIDER_MODULES = [&#39;db.spiders&#39;]</code>: 指定了包含 Scrapy 爬虫的模块。</li>
<li><code>NEWSPIDER_MODULE = &#39;db.spiders&#39;</code>: 指定了新爬虫的模板搜索路径。</li>
</ul>
<h5 id="用户代理设置"><a href="#用户代理设置" class="headerlink" title="用户代理设置"></a>用户代理设置</h5><ul>
<li><code>DEFAULT_REQUEST_HEADERS</code>: 这是默认的请求头部设置。在这里，可以设置 <code>Accept</code> 和 <code>Accept-Language</code> 头部，以及用户代理 (<code>User-Agent</code>)。这有助于模仿常规浏览器请求，避免被目标网站识别为爬虫。</li>
<li><code>ROBOTSTXT_OBEY = False</code>: 这个设置告诉 Scrapy 是否遵守 <code>robots.txt</code> 规则。在这里，它被设置为 <code>False</code>，意味着爬虫将忽略目标网站的 <code>robots.txt</code> 规则。</li>
</ul>
<h5 id="Item-Pipeline-设置"><a href="#Item-Pipeline-设置" class="headerlink" title="Item Pipeline 设置"></a>Item Pipeline 设置</h5><ul>
<li><code>ITEM_PIPELINES</code>: 这个设置定义了项目中启用的 Item Pipeline。在这里，<code>DbPipeline</code> 被设置为处理 item 的 pipeline，<code>300</code> 表示其优先级。</li>
</ul>
<h4 id="爬虫项目的目标和请求流程总结"><a href="#爬虫项目的目标和请求流程总结" class="headerlink" title="爬虫项目的目标和请求流程总结"></a>爬虫项目的目标和请求流程总结</h4><h5 id="目标数据"><a href="#目标数据" class="headerlink" title="目标数据"></a>目标数据</h5><ul>
<li>目标是爬取电影信息以及从次级页面（详情页）获取电影简介。</li>
</ul>
<h5 id="请求流程"><a href="#请求流程" class="headerlink" title="请求流程"></a>请求流程</h5><ul>
<li><strong>访问一级页面</strong>：爬取电影列表页，提取电影的基本信息和指向详情页的 URL。</li>
<li><strong>访问次级页面</strong>：访问每部电影的详情页，进一步提取电影的详细简介。</li>
</ul>
<h5 id="数据存储和一致性"><a href="#数据存储和一致性" class="headerlink" title="数据存储和一致性"></a>数据存储和一致性</h5><ul>
<li>由于 Scrapy 是异步的，页面响应的顺序可能与请求发送的顺序不同。因此，使用 <code>meta</code> 参数在请求间传递数据，保证了数据（如电影的基本信息和详细简介）之间的一致性和关联。</li>
</ul>
<h2 id="项目案例"><a href="#项目案例" class="headerlink" title="项目案例"></a>项目案例</h2><h3 id="分析需求"><a href="#分析需求" class="headerlink" title="分析需求"></a>分析需求</h3><ol>
<li><p>获取腾讯社会招聘的岗位信息</p>
</li>
<li><p>要获取信息的URL并不会显示存在页面中因此要通过开发者模式查看，能发现其有众多异步请求且要获取的信息就在其中</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/88ae35cb-20eb-4b88-88aa-47a6b950b400/public"
                      width = "500"
                >
</li>
<li><p>分析排前的请求，在<code>Fetch/XHR</code>下的<code>Preview</code>可以发现图示请求的数据结构与页面内容近似，因此该页面的URL就应该在这个请求的标头中</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/ae0ddfae-1c39-488e-b671-cf052174e300/public"
                      width = "500"
                >
</li>
<li><p>通过标头构造页面即可开始实施抓取数据，这个URL可以发现拼接了很多参数</p>
</li>
</ol>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/b8e2bf40-9dbd-46e4-417b-39dda1d11900/public"
                      width = "500"
                >

<h3 id="分析规律"><a href="#分析规律" class="headerlink" title="分析规律"></a>分析规律</h3><ol>
<li>通过分析每页的URL，发现切换页面的规律</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">URL_1 = <span class="string">&#x27;https://careers.tencent.com/tencentcareer/api/post/Query?timestamp=1702285787290&amp;countryId=&amp;cityId=&amp;bgIds=&amp;productId=&amp;categoryId=&amp;parentCategoryId=&amp;attrId=&amp;keyword=&amp;pageIndex=1&amp;pageSize=10&amp;language=zh-cn&amp;area=au&#x27;</span></span><br><span class="line">URL_2 = <span class="string">&#x27;https://careers.tencent.com/tencentcareer/api/post/Query?timestamp=1702289736331&amp;countryId=&amp;cityId=&amp;bgIds=&amp;productId=&amp;categoryId=&amp;parentCategoryId=&amp;attrId=&amp;keyword=&amp;pageIndex=2&amp;pageSize=10&amp;language=zh-cn&amp;area=au&#x27;</span></span><br></pre></td></tr></table></figure></div>

<p>可以发现<code>pageIndex</code>是不一样的，一个为1一个为2，那么后续只要利用花括号<code>pageSize=&#123;&#125;</code>传值即可实现页面跳转</p>
<p>同时可以发现URL都存在<code>api</code>因此其都是为接口数据</p>
<h3 id="爬取思路"><a href="#爬取思路" class="headerlink" title="爬取思路"></a>爬取思路</h3><ol>
<li>找到列表页的数据的url</li>
<li>使用scrapy.Request 方法进行请求</li>
<li>解析第二步的响应，提取里面的内容</li>
</ol>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><h4 id="1-创建项目"><a href="#1-创建项目" class="headerlink" title="1. 创建项目"></a>1. 创建项目</h4><p>首先在终端输入 <code>scrapy startproject tx</code> 其中<code>tx</code>代表项目名称为腾讯缩写</p>
<h4 id="2-创建爬虫"><a href="#2-创建爬虫" class="headerlink" title="2. 创建爬虫"></a>2. 创建爬虫</h4><p>然后输入<code>scrapy genspider tencent_data careers.tencent.com</code> 创建爬虫<code>tencent_data</code> 其爬取域名为<code>careers.tencent.com</code></p>
<h4 id="3-编辑爬虫"><a href="#3-编辑爬虫" class="headerlink" title="3. 编辑爬虫"></a>3. 编辑爬虫</h4><p>在生成<code>tencent_data.py</code>的爬虫文件下继承自<code>scrapy.Spider</code>命名为<code>TencentDataSpider</code>的类中编辑需要构造的URL，在原本的URL内修改参数<code>pageIndex=1</code>为<code>pageIndex=&#123;&#125;</code>以便分页（页面跳转），将<code>url_data</code>格式化字符串中的<code>pageIndex</code>参数设为1，生成第一页的URL，作为爬虫开始爬取的起始点。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TencentDataSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;tencent_data&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;careers.tencent.com&quot;</span>]</span><br><span class="line"></span><br><span class="line">    url_data = <span class="string">&#x27;https://careers.tencent.com/tencentcareer/api/post/Query?timestamp=1702285787290&amp;countryId=&amp;cityId=&amp;bgIds=&amp;productId=&amp;categoryId=&amp;parentCategoryId=&amp;attrId=&amp;keyword=&amp;pageIndex=&#123;&#125;&amp;pageSize=10&amp;language=zh-cn&amp;area=au&#x27;</span></span><br><span class="line"></span><br><span class="line">    start_urls = [url_data.<span class="built_in">format</span>(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></div>



<h4 id="4-处理列表列"><a href="#4-处理列表列" class="headerlink" title="4. 处理列表列"></a>4. 处理列表列</h4><p>在<code>parse</code>方法中添加一个循环以确定要爬取的页面数量，可以通过循环<code>for</code>来进行。<code>parse</code> 是Scrapy爬虫中默认的回调方法，用于处理响应。该方法通过循环生成接下来四页的URL（从第1页到第4页）。对于每一页，它创建并产生一个新的Scrapy请求 (<code>scrapy.Request</code>)，并指定回调函数为<code>self.parse_data</code>。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TencentDataSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;tencent_data&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;careers.tencent.com&quot;</span>]</span><br><span class="line"></span><br><span class="line">    url_data = <span class="string">&#x27;https://careers.tencent.com/tencentcareer/api/post/Query?timestamp=1702285787290&amp;countryId=&amp;cityId=&amp;bgIds=&amp;productId=&amp;categoryId=&amp;parentCategoryId=&amp;attrId=&amp;keyword=&amp;pageIndex=&#123;&#125;&amp;pageSize=10&amp;language=zh-cn&amp;area=au&#x27;</span></span><br><span class="line"></span><br><span class="line">    start_urls = [url_data.<span class="built_in">format</span>(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">            next_page_url = self.url_data.<span class="built_in">format</span>(i)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_page_url, callback=self.parse_data)</span><br></pre></td></tr></table></figure></div>


  <div class="note-large orange">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-exclamation"></i><p>注意</p>

    </div>
    <div class="notel-content">
      <p>在提供的<code>parse</code>方法中，<code>response</code>参数实际上没有被使用。这是因为该方法的主要目的是生成后续页面的URL，并对每个URL发起新的请求，而不是处理<code>response</code>中的数据。</p>
<p>在常规的Scrapy爬虫中，<code>parse</code>方法通常用于处理响应，从中提取数据或发现新的URL来跟进。但在这个特定的例子中，<code>parse</code>方法仅用于根据初始响应生成一系列后续页面的请求。实际处理这些页面的响应内容的任务被委托给了另一个方法<code>parse_data</code>（在后续定义）。这意味着在这个特定的<code>parse</code>实现中，初始的<code>response</code>对象不包含需要立即处理的相关数据，其主要作用是触发爬虫开始执行后续页面的请求。</p>

    </div>
  </div>

<p>显然请求的文件是JSON文件，因此需要编辑方法处理从网页请求中返回的JSON格式的响应。这个方法特别适用于处理返回JSON数据的API响应。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TencentDataSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;tencent_data&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;careers.tencent.com&quot;</span>]</span><br><span class="line"></span><br><span class="line">    url_data = <span class="string">&#x27;https://careers.tencent.com/tencentcareer/api/post/Query?timestamp=1702285787290&amp;countryId=&amp;cityId=&amp;bgIds=&amp;productId=&amp;categoryId=&amp;parentCategoryId=&amp;attrId=&amp;keyword=&amp;pageIndex=&#123;&#125;&amp;pageSize=10&amp;language=zh-cn&amp;area=au&#x27;</span></span><br><span class="line"></span><br><span class="line">    start_urls = [url_data.<span class="built_in">format</span>(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">            next_page_url = self.url_data.<span class="built_in">format</span>(i)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_page_url, callback=self.parse_data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_data</span>(<span class="params">self, response</span>):</span><br><span class="line">        dict_data = response.json() <span class="comment"># json.loads(response.text)</span></span><br><span class="line">        <span class="built_in">print</span>(dict_data)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>parse_data</code>是一个实例方法，与<code>parse</code>方法类似，它是Scrapy框架用于处理响应的回调函数。不同的是，这个方法专门用于处理JSON响应。</li>
<li><code>response</code>参数是一个包含HTTP响应数据的对象。在Scrapy中，这个对象通常包括响应的内容和其他元数据。</li>
<li><code>dict_data = response.json()</code>这一行是核心功能。<code>response.json()</code>方法将响应的内容（假定是JSON格式）解析成Python字典。这允许以Python原生的方式访问JSON响应中的数据。</li>
<li>在将JSON响应解析为字典后，可以进一步处理这些数据。例如从字典中提取特定的字段，并将它们存储在Scrapy的Item对象中，或者直接进行数据清洗、转换等操作。</li>
</ul>
<p>在修改<code>settings.py</code>配置文件的参数为正确后进行调试，发现其<code>response</code>依旧能返回<code>200</code>，然而实际输入的URL和抓取时间戳<code>timestamp</code>并不一样，这说明其对时间戳并不敏感。</p>
<p>如果敏感且失败则需要构造时间戳<code>int(time.time() * 1000)</code></p>
<p>进行分析调试结果<code>response.json()</code> 返回的数据结构是一个字典且具有数据</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/dc9f98ec-a482-43d8-c3d9-58ce71096b00/public"
                      width = "500"
                >

<p>但是由于这样数据有太多且杂乱无章，因此在获取数据的时候就进行部分的过滤，只获取<code>Data</code>下的<code>Posts</code>里的部分数据即可，例如名称，地址，时间，详情URL等</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/72d94aa2-a334-4558-2a96-f36c2d1cf500/public"
                      width = "500"
                >

<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TencentDataSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;tencent_data&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;careers.tencent.com&quot;</span>]</span><br><span class="line"></span><br><span class="line">    url_data = <span class="string">&#x27;https://careers.tencent.com/tencentcareer/api/post/Query?timestamp=1702285787290&amp;countryId=&amp;cityId=&amp;bgIds=&amp;productId=&amp;categoryId=&amp;parentCategoryId=&amp;attrId=&amp;keyword=&amp;pageIndex=&#123;&#125;&amp;pageSize=10&amp;language=zh-cn&amp;area=au&#x27;</span></span><br><span class="line"></span><br><span class="line">    PostURL = <span class="string">&#x27;https://careers.tencent.com/tencentcareer/api/post/ByPostId?timestamp=1701864924030&amp;postId=&#123;&#125;&amp;language=zh-cn&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    start_urls = [url_data.<span class="built_in">format</span>(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">            next_page_url = self.url_data.<span class="built_in">format</span>(i)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_page_url, callback=self.parse_data)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_data</span>(<span class="params">self, response</span>):</span><br><span class="line">        dict_data = response.json().get(<span class="string">&quot;Data&quot;</span>).get(<span class="string">&quot;Posts&quot;</span>) <span class="comment"># json.loads(response.text)</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> dict_data:</span><br><span class="line">            item = &#123;&#125;</span><br><span class="line">            item[<span class="string">&#x27;RecruitPostName&#x27;</span>] = data.get(<span class="string">&#x27;RecruitPostName&#x27;</span>)</span><br><span class="line">            item[<span class="string">&#x27;LocationName&#x27;</span>] = data.get(<span class="string">&#x27;LocationName&#x27;</span>)</span><br><span class="line">            item[<span class="string">&#x27;LastUpdateTime&#x27;</span>] = data.get(<span class="string">&#x27;LastUpdateTime&#x27;</span>)</span><br><span class="line">            item[<span class="string">&#x27;CategoryName&#x27;</span>] = data.get(<span class="string">&#x27;CategoryName&#x27;</span>)</span><br><span class="line">            PostId = data.get(<span class="string">&#x27;PostId&#x27;</span>)</span><br><span class="line">			detail_url = self.PostURL.<span class="built_in">format</span>(PostId)</span><br><span class="line">			<span class="keyword">yield</span> scrapy.Request(detail_url, callback=self.parse_post_detail, meta=&#123;<span class="string">&#x27;info&#x27;</span>: item&#125;)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>response.json()</code>将返回的JSON数据转换为Python字典。</li>
<li><code>.get(&quot;Data&quot;).get(&quot;Posts&quot;)</code>从这个字典中获取<code>Data</code>键对应的值，然后从<code>Data</code>中获取<code>Posts</code>键对应的值。JSON结构是<code>&#123;&quot;Data&quot;: &#123;&quot;Posts&quot;: [...]&#125;&#125;</code>，这将得到包含职位信息的列表。</li>
<li>遍历<code>dict_data</code>列表中的每个元素（每个元素代表一个职位）。</li>
<li>对于每个职位，创建一个新的字典<code>item</code>，并从<code>data</code>中提取相关字段。</li>
<li>使用<code>PostId</code>来格式化<code>self.PostURL</code>字符串，以创建指向每个职位详情页的URL。</li>
<li><code>item = &#123;&#125;</code>: 这行代码创建了一个空字典<code>item</code>，用于存储从每个职位信息中提取的数据。这是一种常见的在爬虫中收集数据的方法。</li>
<li><code>self.PostURL</code>是在访问当前类实例（在这种情况下是<code>TencentDataSpider</code>的实例）的<code>PostURL</code>属性。这意味着<code>PostURL</code>是这个类的实例属性，而非一个局部变量或一个类变量（类变量将会使用类名来访问，比如<code>TencentDataSpider.PostURL</code>）。</li>
<li>从<code>data</code>字典中获取<code>PostId</code>值，然后使用这个<code>PostId</code>来格式化<code>self.PostURL</code>字符串。</li>
<li><code>PostURL</code>  为 <code>self.PostURL.format(PostId)</code>使用了<code>format</code>方法来插入<code>PostId</code>进行构造</li>
<li>格式化后的URL被存储在<code>detail_url</code>变量中。然后，这个URL被用来生成一个新的请求，该请求被发送到<code>parse_post_detail</code>方法（这是一个假定的方法，用于处理每个职位详情页面的响应）<ul>
<li><strong><code>scrapy.Request</code></strong>:<ul>
<li>这是Scrapy用来创建新HTTP请求的类。</li>
<li><code>detail_url</code>是这个新请求的目标URL。这个URL通常是从当前页面提取的，指向一个需要进一步爬取的页面。</li>
</ul>
</li>
<li><strong><code>callback=self.parse_post_detail</code></strong>:<ul>
<li><code>callback</code>参数指定了Scrapy在收到此请求的响应后应该调用的方法。</li>
<li><code>self.parse_post_detail</code>是定义的爬虫类中的一个方法。当Scrapy处理<code>detail_url</code>的响应时，它会将响应数据传递给<code>parse_post_detail</code>方法。</li>
</ul>
</li>
<li><strong><code>meta=&#123;&#39;info&#39;: item&#125;</code></strong>:<ul>
<li><code>meta</code>是一个字典，用于在Scrapy的不同请求之间传递额外的数据。</li>
<li>在这个例子中，<code>meta</code>字典包含一个键<code>&#39;info&#39;</code>，其值为<code>item</code>。这意味着可以在<code>parse_post_detail</code>方法中通过<code>response.meta[&#39;info&#39;]</code>来访问这个<code>item</code>对象。</li>
<li>这种方式在请求链中保持状态或传递数据时非常有用。</li>
</ul>
</li>
<li><strong><code>yield</code></strong>:<ul>
<li>使用<code>yield</code>关键字来生成请求对象。在Scrapy中，<code>yield</code>的使用使得框架可以接管并异步处理这些请求。</li>
<li>这允许Scrapy根据需要进行请求的调度，而不是立即发出请求，从而更有效地管理网络资源和避免对目标网站造成过大压力。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="5-处理详情页和Items-文件"><a href="#5-处理详情页和Items-文件" class="headerlink" title="5. 处理详情页和Items 文件"></a>5. 处理详情页和Items 文件</h4><p>随后定义<code>parse_post_detail</code>方法和构造<code>items.py</code>文件下的类属性</p>
<p><code>tencent_data.py</code>:</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse_post_detail</span>(<span class="params">self, response</span>):</span><br><span class="line">    item = TxItem()</span><br><span class="line">    info = response.meta.get(<span class="string">&#x27;info&#x27;</span>)</span><br><span class="line">    item.update(info)</span><br><span class="line">    dict_data = response.json()</span><br><span class="line">    item[<span class="string">&#x27;Requirement&#x27;</span>] = dict_data.get(<span class="string">&quot;Data&quot;</span>).get(<span class="string">&#x27;Requirement&#x27;</span>).replace(<span class="string">&#x27;\r\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    item[<span class="string">&#x27;Responsibility&#x27;</span>] = dict_data.get(<span class="string">&quot;Data&quot;</span>).get(<span class="string">&#x27;Responsibility&#x27;</span>).replace(<span class="string">&#x27;\r\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></div>

<p><code>items.py</code>:</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TxItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    RecruitPostName = scrapy.Field()</span><br><span class="line">    LocationName = scrapy.Field()</span><br><span class="line">    LastUpdateTime = scrapy.Field()</span><br><span class="line">    CategoryName = scrapy.Field()</span><br><span class="line">    Requirement = scrapy.Field()</span><br><span class="line">    Responsibility = scrapy.Field()</span><br></pre></td></tr></table></figure></div>

<p>然后调试检查查看传值是否成功</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/f9d7d000-374c-429b-3e8b-d998e8e61000/public"
                      width = "500"
                >


  <div class="note p-4 mb-4 rounded-small orange fa-circle-exclamation 注意">
    <p>调试处理详情页相关方法前需要同时配置Items 文件，不然无法传值</p>

  </div>



<h4 id="6-编辑Pipelines"><a href="#6-编辑Pipelines" class="headerlink" title="6. 编辑Pipelines"></a>6. 编辑Pipelines</h4><p>在<code>Settings.py</code>文件激活Pipelines后，配置Pipelines文件</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TxPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tencent.txt&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="built_in">str</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></div>

<p>在Scrapy的管道（Pipeline）中，可以选择用不同的方式来处理文件的打开和关闭。</p>
<ol>
<li><strong>类定义</strong>:<ul>
<li><code>class TxPipeline:</code> 定义了一个名为<code>TxPipeline</code>的新类。</li>
<li>这个类是Scrapy的一个管道，它必须实现<code>process_item</code>方法。</li>
</ul>
</li>
<li><strong><code>process_item</code>方法</strong>:<ul>
<li><code>def process_item(self, item, spider):</code> 是管道处理每个爬取的项目（item）的方法。</li>
<li>每当爬虫提取出数据并生成<code>item</code>对象时，Scrapy框架会自动调用这个方法，并将<code>item</code>对象和<code>spider</code>对象作为参数传递给它。</li>
</ul>
</li>
<li><strong>文件写入操作</strong>:<ul>
<li><code>with open(&#39;tencent.txt&#39;, &#39;a&#39;, encoding=&#39;utf-8&#39;) as f:</code> 使用<code>with</code>语句打开文件<code>tencent.txt</code>。如果文件不存在，将会创建它。文件以追加模式（<code>&#39;a&#39;</code>）打开，意味着新写入的内容会被添加到文件的末尾，而不是覆盖原有内容。文件以<code>utf-8</code>编码打开，这对于写入非ASCII字符（例如中文）很重要。</li>
<li><code>f.write(str(item))</code> 将<code>item</code>对象转换为字符串，并写入文件。这里需要注意的是，<code>str(item)</code>可能不会以最优雅的格式输出，特别是如果<code>item</code>包含多个字段或嵌套结构时。在实际应用中，可能需要更精细的格式化方法，比如使用<code>json.dumps</code>来生成更可读的JSON格式。</li>
</ul>
</li>
<li><strong>返回项目</strong>:<ul>
<li><code>return item</code>：在处理完项目后，管道返回<code>item</code>对象。这允许同一个<code>item</code>被多个管道依次处理。在Scrapy中，项目（items）可以通过多个管道传递，每个管道都可以执行一些操作（如清洗数据、去重、写入数据库等）。</li>
</ul>
</li>
</ol>
<p><strong>使用<code>with</code>语句:</strong></p>
<p>在<code>TxPipeline</code>类中，<code>process_item</code>方法使用了<code>with</code>语句来打开文件：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tencent.txt&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="built_in">str</span>(item))</span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong><code>with</code>语句</strong>：Python中的<code>with</code>语句用于包裹执行需要资源管理的代码块，比如文件操作。<code>with</code>语句可以确保文件在使用完毕后被正确关闭，即使在处理文件过程中发生错误也是如此。</li>
<li><strong>优点</strong>：使用<code>with</code>语句的主要优点是它会自动处理文件的关闭。这种方式在每次<code>process_item</code>被调用时打开文件，写入数据，然后立即关闭文件，非常适合写入少量数据。</li>
<li><strong>缺点</strong>：如果<code>process_item</code>被频繁调用，这种方式可能会导致性能问题，因为每次调用都会打开和关闭文件。</li>
</ul>
<p><strong>使用<code>open_spider</code>和<code>close_spider</code>方法：</strong></p>
<p>在之前的案例<code>DbPipeline</code>类中，文件的打开和关闭是在<code>open_spider</code>和<code>close_spider</code>方法中处理的：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">    self.f = <span class="built_in">open</span>(<span class="string">&#x27;dbtest1result.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">    self.f.close()</span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong><code>open_spider</code>和<code>close_spider</code>方法</strong>：这两个方法分别在爬虫开始时和结束时被调用。在这里，文件在爬虫启动时被打开，并在爬虫关闭时被关闭。</li>
<li><strong>优点</strong>：这种方式对于处理大量数据更有效，因为它在整个爬虫过程中只打开和关闭文件一次。这减少了文件操作的开销，提高了性能。</li>
<li><strong>缺点</strong>：如果爬虫在执行过程中出现异常并意外终止，可能导致文件没有正确关闭。这可能会导致数据丢失或文件损坏。</li>
<li>如果爬虫产生的数据量不大，或者更关心代码的简洁性，使用<code>with</code>语句是一个很好的选择。</li>
<li>如果爬虫产生大量数据，或者希望减少文件操作的开销，使用<code>open_spider</code>和<code>close_spider</code>方法可能更合适。</li>
</ul>
<h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><p>最后运行，产出文件<code>tencent.txt</code>且具有相应的值则成功</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/980e1a03-5250-421d-819a-35e0effab800/public"
                      width = "500"
                >

<h3 id="数据库存储"><a href="#数据库存储" class="headerlink" title="数据库存储"></a>数据库存储</h3><p>首先在Ubuntu的<code>MySQL</code>创建一个数据库叫<code>tx</code></p>
<p><code>mysql&gt; create database tx;</code></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/c8f0d32b-0784-4675-0f4d-7607e011f100/public"
                      width = "500"
                >

<p>随后检查是否创建成功<code>mysql&gt; show databases;</code>:</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/82901ac0-b156-4034-eb67-4849826d7f00/public"
                      width = "500"
                >

<p>然后切换数据库并检查内容：</p>
<p><code>use tx;</code></p>
<p><code>show tables;</code></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/4c20a872-96e7-4874-39ef-88cbd76a0500/public"
                      width = "500"
                >

<p>检查无误后按照<code>items.py</code>文件下的<code>TxItem</code>类创建表格即可：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 爬虫存数据  必须要先有表格才能进行数据的增加</span><br><span class="line">create table txzp(</span><br><span class="line">    # 主键</span><br><span class="line">    id int primary key auto_increment,</span><br><span class="line">	RecruitPostName varchar(50),</span><br><span class="line">    LocationName varchar(50),</span><br><span class="line">    LastUpdateTime varchar(50),</span><br><span class="line">    CategoryName  varchar(50),</span><br><span class="line">    Requirement text,</span><br><span class="line">    Responsibility text</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/cb576bde-dc2c-4b41-03a2-ac981a3a6f00/public"
                      width = "500"
                >

<p>创建表格后通过<code>show tables;</code>查看当前数据库的表格内容</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/c6f9d98f-1567-488d-d2ca-ff51a7512400/public"
                      width = "500"
                >

<p>使用<code>desc txzp;</code> 查看 <code>txzp</code> 表格内的内容属性</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/fed1f6a3-b5b8-4a8b-a549-a7d7c274f300/public"
                      width = "500"
                >

<h4 id="建立测试文件"><a href="#建立测试文件" class="headerlink" title="建立测试文件"></a>建立测试文件</h4><p><code>tx_debug.py</code>:</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立数据库连接</span></span><br><span class="line"><span class="comment"># host: 数据库服务器地址，这里设置为&#x27;localhost&#x27;表示数据库服务在本地</span></span><br><span class="line"><span class="comment"># port: 数据库服务端口，默认MySQL端口为3306</span></span><br><span class="line"><span class="comment"># user: 用户名，这里使用&#x27;yiuhang_test&#x27;</span></span><br><span class="line"><span class="comment"># password: 用户密码，这里为&#x27;Test123..&#x27;</span></span><br><span class="line"><span class="comment"># db: 要连接的数据库名，这里为&#x27;tx&#x27;</span></span><br><span class="line"><span class="comment"># charset: 设置字符集，这里使用&#x27;utf8&#x27;</span></span><br><span class="line">conn = pymysql.connect(host=<span class="string">&#x27;localhost&#x27;</span>,</span><br><span class="line">                       port=<span class="number">3306</span>,</span><br><span class="line">                       user=<span class="string">&#x27;yiuhang&#x27;</span>,</span><br><span class="line">                       password=<span class="string">&#x27;Test123..&#x27;</span>,</span><br><span class="line">                       db=<span class="string">&#x27;tx&#x27;</span>,</span><br><span class="line">                       charset=<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(conn)  <span class="comment"># 打印连接对象，用于验证是否连接成功</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个cursor对象，用于执行查询和获取结果</span></span><br><span class="line">curs = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在此处添加数据库操作的CRUD（创建、读取、更新、删除）逻辑</span></span><br><span class="line"><span class="comment"># 例如：curs.execute(&quot;SELECT * FROM your_table&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交事务，对于更改数据库的操作（INSERT, UPDATE, DELETE），需要调用commit()来提交更改</span></span><br><span class="line">conn.commit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭cursor和连接</span></span><br><span class="line">curs.close()</span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure></div>

<ul>
<li>这段脚本主要用于建立与MySQL数据库的连接，执行一些数据库操作（CRUD逻辑），然后关闭连接。</li>
<li><code>pymysql.connect()</code> 函数用于创建与MySQL数据库的连接。</li>
<li><code>conn.cursor()</code> 方法创建一个游标对象，该对象可以用来执行SQL语句并获取结果。</li>
<li>数据库的实际操作（如查询、插入数据等）需要在获取游标后进行。</li>
<li>执行修改数据库内容的操作（如INSERT, UPDATE, DELETE）后，需要调用 <code>conn.commit()</code> 来提交这些更改。</li>
<li>最后，使用 <code>curs.close()</code> 和 <code>conn.close()</code> 关闭游标和连接，以释放资源。</li>
</ul>

  <div class="note-large orange">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-exclamation"></i><p>注意</p>

    </div>
    <div class="notel-content">
      <p>在运行测试文件前，如果使用虚拟机需要检查3306端口是否已经让虚拟机和主机映射，否则会产生 <code>Mysql Python Connector No connection could be made because the target machine actively refused it</code> 的报错。</p>
<p>当在虚拟机（例如使用VMware）中运行应用程序（如数据库服务器）并希望从主机（物理机器）访问时，需要进行端口映射。这是因为虚拟机通常运行在与主机隔离的网络环境中。端口映射确保从主机到虚拟机的网络通信能够正确进行。</p>
<p><strong>解释端口映射</strong></p>
<ol>
<li><strong>为什么需要端口映射</strong>:<ul>
<li>虚拟机通常在私有网络中运行，这意味着它们对主机网络而言是不可见的。</li>
<li>端口映射使得主机可以通过指定的端口访问虚拟机中的服务。</li>
</ul>
</li>
<li><strong>示例解释</strong>:<ul>
<li><strong>MySQL（端口3306）</strong>: 假设在虚拟机中运行MySQL服务器，它默认监听3306端口。要从主机上的应用程序连接到这个MySQL服务器，需要将虚拟机的3306端口映射到主机的某个端口（也可以是3306）。</li>
<li><strong>SSH（端口22）</strong>: 同理，如果想要通过SSH连接到虚拟机，需要将虚拟机的22端口映射到主机的某个端口（也可以是22）。</li>
</ul>
</li>
</ol>
<p><strong>端口映射的步骤</strong></p>
<ol>
<li>打开VMware的虚拟网络编辑器。</li>
<li>选择用于虚拟机的网络适配器（通常是NAT模式）。</li>
<li>进入NAT设置，然后设置端口转发规则。</li>
<li>添加规则：指定主机端口，目标IP（虚拟机IP）和虚拟机端口。</li>
</ol>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/0bf96745-e6e2-46a8-207d-c7b8fc3b4b00/public"
                      width = "500"
                >

<p><strong>如果是服务器而非虚拟机</strong></p>
<ul>
<li><strong>物理服务器</strong>:<ul>
<li>如果数据库运行在物理服务器上，通常不需要端口映射，因为服务器与本地机器或其他服务器都在同一网络或可通过互联网直接访问。</li>
<li>需要注意的是安全设置，如防火墙规则，确保只有授权的客户端能够访问特定端口。</li>
</ul>
</li>
<li><strong>云服务器</strong>:<ul>
<li>在云平台（如AWS、Azure）上，可能需要配置安全组或网络安全规则来允许特定端口（如3306）的流量。</li>
<li>这类似于在物理服务器上设置防火墙规则。</li>
</ul>
</li>
</ul>
<p>在所有情况下，重要的是确保只有信任的客户端能够访问敏感服务（如数据库服务器），并且采用强密码和加密连接（如SSL）来保护数据安全。</p>

    </div>
  </div>

<h4 id="配置步骤"><a href="#配置步骤" class="headerlink" title="配置步骤"></a>配置步骤</h4><h5 id="1-在-settings-py-中配置MySQL信息"><a href="#1-在-settings-py-中配置MySQL信息" class="headerlink" title="1. 在 settings.py 中配置MySQL信息"></a>1. 在 settings.py 中配置MySQL信息</h5><p>在Scrapy项目的<code>settings.py</code>文件中，配置数据库连接信息。这是一个字典，包含连接数据库所需的所有信息。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置mysql链接信息</span></span><br><span class="line">DATABASE_CONFIG = &#123;</span><br><span class="line">   <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;mysql&#x27;</span>, <span class="comment"># 使用的是哪一个数据库类型</span></span><br><span class="line">   <span class="string">&#x27;config&#x27;</span>: &#123;</span><br><span class="line">      <span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;localhost&#x27;</span>,  <span class="comment"># 数据库服务器地址</span></span><br><span class="line">      <span class="string">&#x27;port&#x27;</span>: <span class="number">3306</span>,         <span class="comment"># 数据库服务器端口，MySQL默认为3306</span></span><br><span class="line">      <span class="string">&#x27;user&#x27;</span>: <span class="string">&#x27;yiuhang&#x27;</span>,  <span class="comment"># 数据库用户名</span></span><br><span class="line">      <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;Test123..&#x27;</span>, <span class="comment"># 数据库密码</span></span><br><span class="line">      <span class="string">&#x27;db&#x27;</span>: <span class="string">&#x27;tx&#x27;</span>,              <span class="comment"># 数据库名</span></span><br><span class="line">      <span class="string">&#x27;charset&#x27;</span>: <span class="string">&#x27;utf8&#x27;</span>,       <span class="comment"># 数据库编码</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h5 id="2-在-pipelines-py-中创建MySQL存储的类"><a href="#2-在-pipelines-py-中创建MySQL存储的类" class="headerlink" title="2. 在 pipelines.py 中创建MySQL存储的类"></a>2. 在 pipelines.py 中创建MySQL存储的类</h5><p>在<code>pipelines.py</code>文件中，创建一个Pipeline类，该类负责在爬虫开始时连接数据库，在处理每个item时存储数据，以及在爬虫结束时关闭数据库连接。注释掉之前的配置。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TxzpPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="comment"># 此方法在爬虫开启时调用</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 对每个爬取的item调用此方法</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="comment"># 此方法在爬虫关闭时调用</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></div>

<h5 id="3-注册Pipeline"><a href="#3-注册Pipeline" class="headerlink" title="3. 注册Pipeline"></a>3. 注册Pipeline</h5><p>在<code>settings.py</code>文件中，将Pipeline添加到<code>ITEM_PIPELINES</code>设置中。这里的数字表示优先级，范围是0到1000。注释掉之前的配置。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&quot;tx.pipelines.TxzpPipeline&quot;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h5 id="4-编写连接MySQL的逻辑"><a href="#4-编写连接MySQL的逻辑" class="headerlink" title="4. 编写连接MySQL的逻辑"></a>4. 编写连接MySQL的逻辑</h5><p>在<code>TxzpPipeline</code>类中，实现数据库连接和关闭逻辑。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TxzpPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在爬虫开启时调用此方法，用于建立数据库连接。</span></span><br><span class="line"><span class="string">        此方法从spider的设置中获取数据库配置信息，并建立连接。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从爬虫的设置中获取数据库配置</span></span><br><span class="line">        data_config = spider.settings.get(<span class="string">&quot;DATABASE_CONFIG&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查配置类型是否为MySQL，确保我们使用正确的配置</span></span><br><span class="line">        <span class="keyword">if</span> data_config.get(<span class="string">&#x27;type&#x27;</span>) == <span class="string">&#x27;mysql&#x27;</span>:</span><br><span class="line">            <span class="comment"># 使用提供的配置参数建立MySQL连接</span></span><br><span class="line">            self.conn = pymysql.connect(**data_config.get(<span class="string">&#x27;config&#x27;</span>))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 创建一个数据库操作游标</span></span><br><span class="line">            self.cursor = self.conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        对每个从爬虫爬取的item调用此方法。</span></span><br><span class="line"><span class="string">        这里可以添加将item保存到数据库的逻辑。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 示例：保存逻辑（需实现）</span></span><br><span class="line">        <span class="comment"># sql = &quot;INSERT INTO table_name (column1, column2) VALUES (%s, %s)&quot;</span></span><br><span class="line">        <span class="comment"># self.cursor.execute(sql, (item[&#x27;field1&#x27;], item[&#x27;field2&#x27;]))</span></span><br><span class="line">        <span class="comment"># self.conn.commit()</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在爬虫结束时调用此方法，用于关闭数据库连接。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 关闭数据库操作游标</span></span><br><span class="line">        self.cursor.close()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 关闭数据库连接</span></span><br><span class="line">        self.conn.close()</span><br></pre></td></tr></table></figure></div>

<ol>
<li><strong>open_spider 方法</strong>：<ul>
<li>在爬虫启动时被Scrapy框架自动调用。</li>
<li>从<code>spider</code>的设置 (<code>settings.py</code>) 中获取名为<code>DATABASE_CONFIG</code>的数据库配置信息。</li>
<li>如果配置类型是MySQL（<code>&#39;type&#39;: &#39;mysql&#39;</code>），则使用这些配置信息（如主机、端口、用户名、密码等）建立数据库连接。</li>
<li>创建一个用于执行数据库操作的游标。</li>
<li>方法定义：<code>open_spider(self, spider)</code><ul>
<li><strong>作用</strong>：<code>open_spider</code>是一个特殊的Scrapy方法，它在爬虫启动时自动调用。这是设置爬虫运行前所需资源的理想位置，例如建立数据库连接。</li>
<li><strong>参数</strong>：<code>self</code>指向类的实例，而<code>spider</code>是当前运行的爬虫实例。通过<code>spider</code>，可以访问Scrapy项目的设置和其他与爬虫相关的属性。</li>
</ul>
</li>
<li>获取数据库配置：<code>data_config = spider.settings.get(&quot;DATABASE_CONFIG&quot;)</code><ul>
<li><strong>作用</strong>：这一行从Scrapy的<code>settings.py</code>文件中获取数据库配置信息。将配置放在<code>settings.py</code>中可以集中管理配置，并在需要时轻松更改，而无需修改代码本身。</li>
<li><strong>为什么重要</strong>：这种做法增加了代码的灵活性和可维护性，因为可以在不同环境（如开发、测试、生产）中使用不同的数据库配置，而无需更改代码。</li>
</ul>
</li>
<li>检查配置类型：<code>if data_config.get(&#39;type&#39;) == &#39;mysql&#39;:</code></li>
<li><strong>作用</strong>：这个条件检查确保从<code>settings.py</code>中获取的配置是为MySQL数据库设计的。这是一个安全措施，以防配置被错误地设置。</li>
<li><strong>为什么重要</strong>：在可能有多种数据库配置的情况下（例如MySQL、PostgreSQL等），这个检查确保Pipeline使用正确的配置。这有助于避免运行时错误和配置混淆。</li>
<li>建立MySQL连接：<code>self.conn = pymysql.connect(**data_config.get(&#39;config&#39;))</code><ul>
<li><strong>作用</strong>：这行代码使用<code>pymysql</code>模块建立到MySQL数据库的连接。<code>**data_config.get(&#39;config&#39;)</code>是一个字典解包操作，它将配置字典中的键值对作为参数传递给<code>pymysql.connect</code>函数。</li>
<li><strong>为什么重要</strong>：建立数据库连接是进行数据库操作的首要步骤。使用<code>pymysql</code>模块可以方便地与MySQL数据库交互。这种方法的优点是可以直接从配置文件中读取连接信息，提高了代码的可读性和可维护性。</li>
</ul>
</li>
<li>创建数据库游标：<code>self.cursor = self.conn.cursor()</code><ul>
<li><strong>作用</strong>：创建一个数据库游标对象，该对象允许在数据库中执行SQL命令并处理结果。</li>
<li><strong>为什么重要</strong>：游标是执行和管理数据库操作的关键。它不仅允许执行SQL语句，还能帮助有效地检索查询结果。在Scrapy中，通常会在<code>process_item</code>方法中使用此游标执行数据库操作。</li>
</ul>
</li>
</ul>
</li>
<li><strong>process_item 方法</strong>：<ul>
<li>对爬虫爬取的每个item调用此方法。</li>
<li>这是数据处理和保存的主要地方。在这个方法中，可以编写将爬取的数据（<code>item</code>）保存到MySQL数据库的逻辑。</li>
<li>在这个示例中，该方法仅返回<code>item</code>，需要根据具体需求来实现数据插入的逻辑。</li>
</ul>
</li>
<li><strong>close_spider 方法</strong>：<ul>
<li>在爬虫结束时被Scrapy框架自动调用。</li>
<li>关闭数据库游标和连接，确保释放资源。</li>
</ul>
</li>
</ol>
<h5 id="5-编写存储逻辑"><a href="#5-编写存储逻辑" class="headerlink" title="5.编写存储逻辑"></a>5.编写存储逻辑</h5><p>实现<code>process_item</code>方法以将每个item存储到MySQL数据库。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">    <span class="comment"># 定义插入数据的SQL语句</span></span><br><span class="line">    sql = <span class="string">&quot;INSERT INTO txzp(RecruitPostName, LocationName, LastUpdateTime, CategoryName, Requirement, Responsibility) VALUES(%s,%s,%s,%s,%s,%s)&quot;</span></span><br><span class="line">    params = [</span><br><span class="line">        item.get(<span class="string">&#x27;RecruitPostName&#x27;</span>),</span><br><span class="line">        item.get(<span class="string">&#x27;LocationName&#x27;</span>),</span><br><span class="line">        item.get(<span class="string">&#x27;LastUpdateTime&#x27;</span>),</span><br><span class="line">        item.get(<span class="string">&#x27;CategoryName&#x27;</span>),</span><br><span class="line">        item.get(<span class="string">&#x27;Requirement&#x27;</span>),</span><br><span class="line">        item.get(<span class="string">&#x27;Responsibility&#x27;</span>),</span><br><span class="line">    ]</span><br><span class="line">    self.cursor.execute(sql, params)</span><br><span class="line">    self.conn.commit()</span><br></pre></td></tr></table></figure></div>

<h6 id="方法定义"><a href="#方法定义" class="headerlink" title="方法定义"></a><strong>方法定义</strong></h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>process_item</code>：这是Scrapy框架中Pipeline对象的一个核心方法，用于处理从爬虫传递过来的每个item。</li>
<li><code>self</code>：指向当前类（<code>TxzpPipeline</code>）的实例。</li>
<li><code>item</code>：这是一个从爬虫传递到Pipeline的数据项。它通常是一个类似字典的对象，包含了爬虫提取的数据。</li>
<li><code>spider</code>：引用当前的爬虫实例，允许访问爬虫特定的功能或数据。</li>
</ul>
<h6 id="SQL-语句的定义"><a href="#SQL-语句的定义" class="headerlink" title="SQL 语句的定义"></a>SQL <strong>语句的定义</strong></h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql = <span class="string">&quot;INSERT INTO txzp(RecruitPostName, LocationName, LastUpdateTime, CategoryName, Requirement, Responsibility) VALUES(%s,%s,%s,%s,%s,%s)&quot;</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>这行代码定义了一个SQL插入语句，用于将数据插入到名为<code>txzp</code>的MySQL表中。</li>
<li><code>INSERT INTO txzp</code>：指定要插入数据的表名。</li>
<li><code>(RecruitPostName, LocationName, LastUpdateTime, CategoryName, Requirement, Responsibility)</code>：这些是表中的列名，将为这些列插入数据。</li>
<li><code>VALUES(%s,%s,%s,%s,%s,%s)</code>：这是参数化的查询部分，<code>%s</code>是占位符，用于在执行时插入实际的数据值。</li>
</ul>
<h6 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a><strong>准备数据</strong></h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">params = [</span><br><span class="line">    item.get(<span class="string">&#x27;RecruitPostName&#x27;</span>),</span><br><span class="line">    item.get(<span class="string">&#x27;LocationName&#x27;</span>),</span><br><span class="line">    item.get(<span class="string">&#x27;LastUpdateTime&#x27;</span>),</span><br><span class="line">    item.get(<span class="string">&#x27;CategoryName&#x27;</span>),</span><br><span class="line">    item.get(<span class="string">&#x27;Requirement&#x27;</span>),</span><br><span class="line">    item.get(<span class="string">&#x27;Responsibility&#x27;</span>),</span><br><span class="line">]</span><br></pre></td></tr></table></figure></div>

<ul>
<li>这部分代码创建了一个列表<code>params</code>，包含了要插入到数据库中的数据。</li>
<li><code>item.get(&#39;RecruitPostName&#39;)</code>等语句从<code>item</code>对象中提取相应的数据。<code>get</code>方法用于安全地访问字典键值，即使键不存在也不会引发错误。</li>
</ul>
<h6 id="执行SQL语句"><a href="#执行SQL语句" class="headerlink" title="执行SQL语句"></a>执行SQL<strong>语句</strong></h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.cursor.execute(sql, params)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>使用之前创建的数据库游标<code>self.cursor</code>来执行SQL语句。</li>
<li><code>execute</code>方法执行前面定义的SQL语句，并用<code>params</code>列表中的值替换占位符<code>%s</code>。</li>
<li>这种参数化查询方法可以有效防止SQL注入攻击，比直接将字符串拼接到SQL语句更安全。</li>
</ul>
<h6 id="提交事务"><a href="#提交事务" class="headerlink" title="提交事务"></a>提交事务</h6><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.conn.commit()</span><br></pre></td></tr></table></figure></div>

<ul>
<li>调用<code>self.conn.commit()</code>来提交事务，确保更改被保存到数据库。</li>
<li>在数据库操作中，<code>commit</code>是一个重要步骤，它使得执行的操作（如插入、更新、删除）成为永久性的。</li>
<li>如果不调用<code>commit</code>，那么即使执行了SQL语句，数据也不会被实际保存到数据库中。</li>
</ul>
<h3 id="执行确认"><a href="#执行确认" class="headerlink" title="执行确认"></a>执行确认</h3><p>执行测试文件<code>tx_debug.py</code></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/5cb85ca5-058b-43b7-e27d-c99d25a7fc00/public"
                      width = "500"
                >

<p>连接成功后执行测试文件<code>db_debug.py</code></p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/cd021416-57dd-4aae-6b83-d4d80fd7cc00/public"
                      width = "500"
                >

<p>随后连接Linux检查数据库是否存储数据即可</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/ddbef679-365c-42f1-c75c-de15d103a500/public"
                      width = "500"
                >



<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/02f70597-affb-4947-3c06-02baba0ee600/public"
                      width = "500"
                >

<h2 id="Scray-Request-类"><a href="#Scray-Request-类" class="headerlink" title="Scray Request 类"></a>Scray Request 类</h2><p>Scrapy的<code>Request</code>类是用于表示一个HTTP请求的基本类。以下是其构造函数的参数和一些特殊的<code>meta</code>键值的详细说明：</p>
<h3 id="构造函数参数"><a href="#构造函数参数" class="headerlink" title="构造函数参数"></a>构造函数参数</h3><ul>
<li><strong>url</strong> (<code>str</code>): 请求的URL。</li>
<li><strong>callback</strong> (<code>callable</code>): 当请求成功时，Scrapy将调用此回调函数处理响应。</li>
<li><strong>method</strong> (<code>str</code>): HTTP方法（如<code>&#39;GET&#39;</code>, <code>&#39;POST&#39;</code>等）。默认为<code>&#39;GET&#39;</code>。</li>
<li><strong>headers</strong> (<code>dict</code>): 自定义的请求头。</li>
<li><strong>body</strong> (<code>str</code> 或 <code>unicode</code>): 请求体内容。如果未提供，默认为空字符串。</li>
<li><strong>cookies</strong> (<code>dict</code> 或 <code>[dict]</code>): 请求时附带的cookies。</li>
<li><strong>meta</strong> (<code>dict</code>): 包含此请求的额外信息，可以在不同的回调函数间共享。</li>
<li><strong>encoding</strong> (<code>str</code>): 编码类型，默认为<code>&#39;utf-8&#39;</code>。用于URL编码和将body转换为bytes。</li>
<li><strong>priority</strong> (<code>int</code>): 请求的优先级。数字越大优先级越高，默认为0。</li>
<li><strong>dont_filter</strong> (<code>bool</code>): 如果设置为<code>True</code>，则不会对请求进行去重过滤。</li>
<li><strong>errback</strong> (<code>callable</code>): 如果请求过程中发生错误，将调用此函数。</li>
<li><strong>flags</strong> (<code>list</code>): 一组字符串标志，通常用于日志记录或调试。</li>
<li><strong>cb_kwargs</strong> (<code>dict</code>): 传递给回调函数的关键字参数。</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.http  <span class="keyword">import</span>  Request,FormRequest</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">class Request(object_ref):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __init__(self, url, callback=None, method=&#x27;GET&#x27;, headers=None, body=None,</span></span><br><span class="line"><span class="string">                 cookies=None, meta=None, encoding=&#x27;utf-8&#x27;, priority=0,</span></span><br><span class="line"><span class="string">                 dont_filter=False, errback=None, flags=None, cb_kwargs=None):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></div>

<h3 id="Request-meta-的特殊键值"><a href="#Request-meta-的特殊键值" class="headerlink" title="Request.meta 的特殊键值"></a>Request.meta 的特殊键值</h3><ul>
<li><strong>dont_redirect</strong> (<code>bool</code>): 不处理HTTP重定向。</li>
<li><strong>dont_retry</strong> (<code>bool</code>): 不对失败的HTTP请求进行重试。</li>
<li><strong>handle_httpstatus_list</strong> (<code>list</code>): 包含HTTP状态码的列表，这些状态码的响应将不被视为错误。</li>
<li><strong>handle_httpstatus_all</strong> (<code>bool</code>): 处理所有HTTP状态码。</li>
<li><strong>dont_merge_cookies</strong> (<code>bool</code>): 不合并此请求的cookies。</li>
<li><strong>cookiejar</strong> (<code>int</code>&#x2F;<code>object</code>): 指定用于此请求的cookie jar。</li>
<li><strong>dont_cache</strong> (<code>bool</code>): 禁止对此请求使用HTTP缓存。</li>
<li><strong>redirect_reasons</strong> (<code>list</code>): 跟踪重定向的原因。</li>
<li><strong>redirect_urls</strong> (<code>list</code>): 存储请求过程中的所有重定向URL。</li>
<li><strong>bindaddress</strong> (<code>tuple</code>): 指定用于出站连接的本地IP地址和端口。</li>
<li><strong>dont_obey_robotstxt</strong> (<code>bool</code>): 不遵守robots.txt规则。</li>
<li><strong>download_timeout</strong> (<code>float</code>): 设置下载超时时间。</li>
<li><strong>download_maxsize</strong> (<code>int</code>): 设置允许下载的最大字节数。</li>
<li><strong>download_latency</strong> (<code>float</code>): 设置下载延迟。</li>
<li><strong>download_fail_on_dataloss</strong> (<code>bool</code>): 如果出现数据丢失，下载失败。</li>
<li><strong>proxy</strong> (<code>str</code>): 设置此请求的代理服务器。</li>
<li><strong>ftp_user</strong> (<code>str</code>): FTP请求的用户名。</li>
<li><strong>ftp_password</strong> (<code>str</code>): FTP请求的密码。</li>
<li><strong>referrer_policy</strong> (<code>str</code>): 设置引用策略。</li>
<li><strong>max_retry_times</strong> (<code>int</code>): 设置请求最大重试次数。</li>
</ul>
<h2 id="FormRequest-类"><a href="#FormRequest-类" class="headerlink" title="FormRequest 类"></a>FormRequest 类</h2><p>Scrapy框架中的<code>FormRequest</code>类，它是<code>Request</code>类的一个子类，专门用于处理表单请求。</p>
<p><code>FormRequest</code>是Scrapy用来发送数据到表单的特殊请求类型。它继承自基本的<code>Request</code>类，并添加了处理表单数据的特定功能。</p>
<h3 id="构造函数参数-1"><a href="#构造函数参数-1" class="headerlink" title="构造函数参数"></a>构造函数参数</h3><ul>
<li><strong><code>\*args</code> 和 <code>\**kwargs</code></strong>:<ul>
<li>这些是Python中的标准参数，允许接收任意数量的位置参数（<code>*args</code>）和关键字参数（<code>**kwargs</code>）。这在继承时使得<code>FormRequest</code>可以接收<code>Request</code>类接受的所有参数。</li>
</ul>
</li>
<li><strong><code>formdata</code></strong> (<code>dict</code> 或 类似于列表的元组对):<ul>
<li>这是用于传递表单数据的关键字参数。如果提供了<code>formdata</code>，<code>FormRequest</code>将以<code>application/x-www-form-urlencoded</code>格式编码这些数据并包含在请求体中。</li>
<li>如果<code>method</code>未指定且<code>formdata</code>存在，则默认将请求方法设置为<code>POST</code>。</li>
</ul>
</li>
</ul>
<h3 id="方法逻辑"><a href="#方法逻辑" class="headerlink" title="方法逻辑"></a>方法逻辑</h3><ol>
<li><strong>检查<code>formdata</code>和<code>method</code></strong>:<ul>
<li>如果提供了<code>formdata</code>且未指定<code>method</code>，则默认将<code>method</code>设置为<code>POST</code>。</li>
</ul>
</li>
<li><strong>调用基类构造函数</strong>:<ul>
<li>使用<code>super(FormRequest, self).__init__(*args, **kwargs)</code>调用基类（<code>Request</code>）的构造函数，确保所有基本的初始化逻辑被执行。</li>
</ul>
</li>
<li><strong>处理<code>formdata</code></strong>:<ul>
<li>如果提供了<code>formdata</code>，它将被转换为查询字符串。</li>
<li>对于<code>POST</code>请求，这个查询字符串将被设置为请求体，同时设置<code>Content-Type</code>头为<code>application/x-www-form-urlencoded</code>。</li>
<li>对于非<code>POST</code>请求（例如<code>GET</code>），查询字符串将被附加到URL上。</li>
</ul>
</li>
</ol>
<h3 id="特别说明"><a href="#特别说明" class="headerlink" title="特别说明"></a>特别说明</h3><ul>
<li><code>valid_form_methods</code> 是一个类属性，定义了<code>FormRequest</code>支持的HTTP方法。默认为<code>[&#39;GET&#39;, &#39;POST&#39;]</code>。</li>
<li><code>_urlencode</code> 函数用于将表单数据转换为URL编码的字符串。</li>
<li><code>_set_body</code> 和 <code>_set_url</code> 是内部方法，用于设置请求体和更新请求URL。</li>
</ul>
<p><code>FormRequest</code>通常用于向网站提交表单，例如登录页面、搜索查询等。由于它自动处理表单数据的编码，因此相比于普通的<code>Request</code>，在处理表单请求时更加方便。</p>
<p>在使用scrapy发动POST请求的时候,常使用此方法,能较方便的发送请求.具体的使用,见登录github案例;</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FormRequest</span>(<span class="title class_ inherited__">Request</span>):</span><br><span class="line">    valid_form_methods = [<span class="string">&#x27;GET&#x27;</span>, <span class="string">&#x27;POST&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        formdata = kwargs.pop(<span class="string">&#x27;formdata&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> formdata <span class="keyword">and</span> kwargs.get(<span class="string">&#x27;method&#x27;</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            kwargs[<span class="string">&#x27;method&#x27;</span>] = <span class="string">&#x27;POST&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(FormRequest, self).__init__(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> formdata:</span><br><span class="line">            items = formdata.items() <span class="keyword">if</span> <span class="built_in">isinstance</span>(formdata, <span class="built_in">dict</span>) <span class="keyword">else</span> formdata</span><br><span class="line">            querystr = _urlencode(items, self.encoding)</span><br><span class="line">            <span class="keyword">if</span> self.method == <span class="string">&#x27;POST&#x27;</span>:</span><br><span class="line">                self.headers.setdefault(<span class="string">b&#x27;Content-Type&#x27;</span>, <span class="string">b&#x27;application/x-www-form-urlencoded&#x27;</span>)</span><br><span class="line">                self._set_body(querystr)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self._set_url(self.url + (<span class="string">&#x27;&amp;&#x27;</span> <span class="keyword">if</span> <span class="string">&#x27;?&#x27;</span> <span class="keyword">in</span> self.url <span class="keyword">else</span> <span class="string">&#x27;?&#x27;</span>) + querystr)</span><br></pre></td></tr></table></figure></div>

<h2 id="HTTP响应处理"><a href="#HTTP响应处理" class="headerlink" title="HTTP响应处理"></a>HTTP响应处理</h2><p>当使用Scrapy或类似的网络爬虫框架时，处理HTTP响应是一个常见的任务。以下是响应对象的主要属性和方法：</p>
<ul>
<li><strong>url</strong>（字符串）: 此响应的URL。</li>
<li><strong>status</strong>（整数）: 响应的HTTP状态码。默认为200。</li>
<li><strong>headers</strong>（字典）: 此响应的响应头。可以是单值或多值。</li>
<li><strong>body</strong>（字节）: 响应主体。为字节类型，可使用<code>response.text</code>以字符串形式访问。</li>
<li><strong>flags</strong>（列表）: 包含初始响应标志的列表。</li>
<li><strong>request</strong>（Request对象）: 表示生成此响应的请求。</li>
</ul>
<h3 id="属性和方法"><a href="#属性和方法" class="headerlink" title="属性和方法"></a>属性和方法</h3><ul>
<li><strong>url</strong>: 包含请求URL的字符串。只读。</li>
<li><strong>method</strong>: 表示HTTP方法的字符串。</li>
<li><strong>headers</strong>: 请求头的类似字典对象。</li>
<li><strong>body</strong>: 包含请求正文的字符串。只读。</li>
<li><strong>meta</strong>: 包含请求的任意元数据的字典。</li>
<li><strong>copy()</strong>: 返回此请求的副本。</li>
<li><strong>replace()</strong>: 返回一个具有更新字段的新请求。</li>
</ul>
<h2 id="Scrapy日志配置和使用"><a href="#Scrapy日志配置和使用" class="headerlink" title="Scrapy日志配置和使用"></a>Scrapy日志配置和使用</h2><p>在Scrapy项目中，正确配置和使用日志是监控爬虫行为、调试和记录关键信息的重要方面。</p>
<h3 id="日志文件配置"><a href="#日志文件配置" class="headerlink" title="日志文件配置"></a>日志文件配置</h3><p>在Scrapy的<code>settings.py</code>文件中，可以配置以下日志相关的设置：</p>
<ul>
<li><strong>LOG_FILE</strong>: 设置日志输出文件的路径。如果设置为<code>None</code>，日志将输出到控制台。</li>
<li><strong>LOG_ENABLED</strong>: 控制是否启用日志记录，布尔值，默认为<code>True</code>。</li>
<li><strong>LOG_ENCODING</strong>: 设置日志文件的编码，默认为<code>&#39;utf-8&#39;</code>。</li>
<li><strong>LOG_LEVEL</strong>: 设置日志级别，默认为<code>&#39;DEBUG&#39;</code>。其他级别包括<code>&#39;INFO&#39;</code>、<code>&#39;WARNING&#39;</code>、<code>&#39;ERROR&#39;</code>、<code>&#39;CRITICAL&#39;</code>。</li>
<li><strong>LOG_FORMAT</strong>: 自定义日志的格式。格式选项包括：<ul>
<li><code>%(levelno)s</code>: 日志级别的数值。</li>
<li><code>%(levelname)s</code>: 日志级别名称。</li>
<li><code>%(pathname)s</code>: 执行程序的路径。</li>
<li><code>%(filename)s</code>: 执行程序名。</li>
<li><code>%(funcName)s</code>: 日志的当前函数。</li>
<li><code>%(lineno)d</code>: 日志的当前行号。</li>
<li><code>%(asctime)s</code>: 日志记录的时间。</li>
<li><code>%(thread)d</code>: 线程ID。</li>
<li><code>%(threadName)s</code>: 线程名称。</li>
<li><code>%(process)d</code>: 进程ID。</li>
<li><code>%(message)s</code>: 日志信息。</li>
<li><code>%(name)s</code>: 日志记录器的名称。</li>
</ul>
</li>
<li><strong>LOG_DATEFORMAT</strong>: 设置日志的日期格式。</li>
<li><strong>LOG_STDOUT</strong>: 布尔值，控制是否将标准输出（stdout）重定向到日志，通常设为<code>False</code>。</li>
<li><strong>LOG_SHORT_NAMES</strong>: 布尔值，设置为<code>True</code>时，日志记录器将使用短名称。</li>
</ul>
<h3 id="Python日志模块"><a href="#Python日志模块" class="headerlink" title="Python日志模块"></a>Python日志模块</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建logger对象</span></span><br><span class="line">logger = logging.getLogger(<span class="string">&#x27;hello&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建日志处理器：控制台和文件</span></span><br><span class="line">streamH = logging.StreamHandler()</span><br><span class="line">fileH = logging.FileHandler(<span class="string">&#x27;test_log.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置日志格式</span></span><br><span class="line">formatter = logging.Formatter(<span class="string">&#x27;时间:%(asctime)s -- 日志级别:%(levelname)s -- 报错信息:%(message)s&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将格式应用到日志处理器</span></span><br><span class="line">streamH.setFormatter(formatter)</span><br><span class="line">fileH.setFormatter(formatter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将处理器添加到logger</span></span><br><span class="line">logger.addHandler(streamH)</span><br><span class="line">logger.addHandler(fileH)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    logger.error(<span class="string">&quot;我真的知道错误&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>在这个示例中，创建了一个名为<code>hello</code>的日志记录器。</li>
<li>两个处理器（<code>StreamHandler</code>和<code>FileHandler</code>）分别用于输出日志到控制台和文件。</li>
<li>使用<code>Formatter</code>设置了日志的格式。</li>
<li>日志记录器通过添加这些处理器来启用日志记录。</li>
</ul>
<h3 id="项目中的常见设置"><a href="#项目中的常见设置" class="headerlink" title="项目中的常见设置"></a>项目中的常见设置</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LOG_FILE = <span class="string">&#x27;logfile_name.log&#x27;</span></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;INFO&#x27;</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong>logger</strong>: Scrapy在每个Spider实例中提供了一个可用的logger实例，用于记录日志。</li>
</ul>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul>
<li>在生产环境中，通常将日志级别设置为<code>INFO</code>或<code>WARNING</code>，以减少日志文件的大小。</li>
<li>保证日志记录的详细程度与应用程序的需求相符，同时避免记录过于敏感的信息，如用户凭据等。</li>
<li>日志文件的管理（如归档和清理）也是重要的，以避免日志文件占用过多磁盘空间。</li>
</ul>
<h2 id="GitHub登录过程分析与实现"><a href="#GitHub登录过程分析与实现" class="headerlink" title="GitHub登录过程分析与实现"></a>GitHub登录过程分析与实现</h2><h3 id="登录分析"><a href="#登录分析" class="headerlink" title="登录分析"></a>登录分析</h3><p>首先前往GitHub <a class="link"   target="_blank" rel="noopener" href="https://github.com/login" >https://github.com/login <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 进行登录操作，检查登录的过程提交的数据</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/75e9ccf3-7176-40e0-005b-8da0595f7b00/public"
                      width = "500"
                >

<p>可以发现其在登录的过程中进行了<code>POST</code>请求，并提交表单到 <code>https://github.com/session</code>。关键在于准确地捕获并发送所有必要的表单参数。</p>
<p>检查其<code>Payload</code>可以查看到其参数和对应的值</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/6bbae919-0c59-43c7-f3f1-74576158c400/public"
                      width = "500"
                >

<p>由于密码和账户组合有三种情况，因此通过分别保持密码和账户相同分别再进行两次请求，并查看它们的值</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/46935bf8-4016-4473-fd9d-e987b01fd100/public"
                      width = "500"
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/de4ff6da-3de9-4d6d-d113-370d7ddee800/public"
                      width = "500"
                >

<p>比较三者的不同可以发现其是<code>authenticity_token</code> ，<code>login</code>，<code>password</code>，<code>required_field_####</code>，<code>timestamp</code>和<code>timestamp_secret</code>，因此需要在后面进行构造</p>
<h4 id="登录参数"><a href="#登录参数" class="headerlink" title="登录参数"></a>登录参数</h4><p>因此登录GitHub时，提交的表单数据不仅包含用户名和密码，还包括一些隐藏字段，如<code>authenticity_token</code>、<code>timestamp</code>和<code>timestamp_secret</code>。这些字段可能是为了安全性（如防止CSRF攻击）而设置的。</p>
<p>一个典型的登录请求包含以下参数：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">form_data = &#123;</span><br><span class="line">    <span class="string">&quot;commit&quot;</span>: <span class="string">&quot;Sign in&quot;</span>,</span><br><span class="line">    <span class="string">&quot;authenticity_token&quot;</span>: authenticity_token,</span><br><span class="line">    <span class="string">&quot;login&quot;</span>: ACCOUNT,  <span class="comment"># 用户名</span></span><br><span class="line">    <span class="string">&quot;password&quot;</span>: PASSWORD,  <span class="comment"># 密码</span></span><br><span class="line">    <span class="string">&quot;webauthn-support&quot;</span>: <span class="string">&quot;supported&quot;</span>,</span><br><span class="line">    <span class="string">&quot;webauthn-iuvpaa-support&quot;</span>: <span class="string">&quot;unsupported&quot;</span>,</span><br><span class="line">    <span class="string">&quot;return_to&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;required_field_####&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;timestamp&quot;</span>: timestamp,</span><br><span class="line">    <span class="string">&quot;timestamp_secret&quot;</span>: timestamp_secret,</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h4 id="form-data-数据来源"><a href="#form-data-数据来源" class="headerlink" title="form_data 数据来源"></a>form_data 数据来源</h4><p>这些数据大部分可以在访问 <code>https://github.com/login</code> 页面时从页面HTML中捕获。特别是<code>authenticity_token</code>、<code>required_field_</code>、<code>timestamp</code>和<code>timestamp_secret</code>是动态生成的，因此每次登录前都需要先访问登录页面来获取这些数据。</p>
<h3 id="请求流程-1"><a href="#请求流程-1" class="headerlink" title="请求流程"></a>请求流程</h3><h4 id="请求流程-2"><a href="#请求流程-2" class="headerlink" title="请求流程"></a>请求流程</h4><ol>
<li>访问 <code>https://github.com/login</code> 以获取登录所需的参数。</li>
<li>向 <code>https://github.com/session</code> 提交POST请求，携带用户名、密码及其他必需数据。</li>
</ol>
<h3 id="Scrapy实现"><a href="#Scrapy实现" class="headerlink" title="Scrapy实现"></a>Scrapy实现</h3><h4 id="编辑爬虫文件"><a href="#编辑爬虫文件" class="headerlink" title="编辑爬虫文件"></a>编辑爬虫文件</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> git.spiders <span class="keyword">import</span> USER</span><br><span class="line"></span><br><span class="line"><span class="comment"># from git.spiders import USER</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GithubLoginSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;github_login&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;github.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://github.com/login&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        authenticity_token = response.xpath(<span class="string">&#x27;//input[@name=&quot;authenticity_token&quot;]/@value&#x27;</span>).get()</span><br><span class="line">        required_field = response.xpath(<span class="string">&#x27;//input[@type=&quot;text&quot; and @hidden=&quot;hidden&quot; ]/@name&#x27;</span>).get()</span><br><span class="line">        timestamp = response.xpath(<span class="string">&#x27;//input[@name=&quot;timestamp&quot;]/@value&#x27;</span>).get()</span><br><span class="line">        timestamp_secret = response.xpath(<span class="string">&#x27;//input[@name=&quot;timestamp_secret&quot;]/@value&#x27;</span>).get()</span><br><span class="line"></span><br><span class="line">        form_data = &#123;</span><br><span class="line">            <span class="string">&quot;commit&quot;</span>: <span class="string">&quot;Sign in&quot;</span>,</span><br><span class="line">            <span class="string">&#x27;authenticity_token&#x27;</span>: authenticity_token,</span><br><span class="line">            <span class="string">&quot;login&quot;</span>: USER.LOGIN,</span><br><span class="line">            <span class="string">&quot;password&quot;</span>: USER.PASSWORD,</span><br><span class="line">            <span class="string">&quot;webauthn-conditional&quot;</span>: <span class="string">&quot;undefined&quot;</span>,</span><br><span class="line">            <span class="string">&quot;javascript-support&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">            <span class="string">&quot;webauthn-support&quot;</span>: <span class="string">&quot;supported&quot;</span>,</span><br><span class="line">            <span class="string">&quot;webauthn-iuvpaa-support&quot;</span>: <span class="string">&quot;unsupported&quot;</span>,</span><br><span class="line">            <span class="string">&quot;return_to&quot;</span>: <span class="string">&quot;https://github.com/login&quot;</span>,</span><br><span class="line">            <span class="string">&#x27;allow_signup&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;client_id&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;integration&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">            required_field: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">            <span class="string">&quot;timestamp&quot;</span>: timestamp,</span><br><span class="line">            <span class="string">&quot;timestamp_secret&quot;</span>: timestamp_secret,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> scrapy.FormRequest(url=<span class="string">&quot;https://github.com/session&quot;</span>, formdata=form_data, callback=self.login)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">login</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="built_in">print</span>(response)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;CosmicTrace&#x27;</span> <span class="keyword">in</span> response.text:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;已成功拿到值&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;失敗了&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure></div>

<h5 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GithubLoginSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;github_login&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;github.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://github.com/login&quot;</span>]</span><br></pre></td></tr></table></figure></div>

<ul>
<li>定义了一个名为<code>GithubLoginSpider</code>的Scrapy爬虫类。</li>
<li><code>name</code>属性设置为<code>&quot;github_login&quot;</code>，是爬虫的唯一标识。</li>
<li><code>allowed_domains</code>列表限制了爬虫只能爬取<code>&quot;github.com&quot;</code>域名下的页面。</li>
<li><code>start_urls</code>包含了爬虫开始爬取的URL，这里是GitHub的登录页面。</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">	authenticity_token = response.xpath(<span class="string">&#x27;//input[@name=&quot;authenticity_token&quot;]/@value&#x27;</span>).get()</span><br><span class="line">	required_field = response.xpath(<span class="string">&#x27;//input[@type=&quot;text&quot; and @hidden=&quot;hidden&quot; ]/@name&#x27;</span>).get()</span><br><span class="line">	timestamp = response.xpath(<span class="string">&#x27;//input[@name=&quot;timestamp&quot;]/@value&#x27;</span>).get()</span><br><span class="line">	timestamp_secret = response.xpath(<span class="string">&#x27;//input[@name=&quot;timestamp_secret&quot;]/@value&#x27;</span>).get()</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>parse</code>是Scrapy爬虫的默认回调方法，爬虫向一个URL发出请求时，获得的响应会自动传递给这个方法。在这个例子中，<code>response</code> 对象包含了对 <code>start_urls</code> 中URL的HTTP响应。</li>
<li>这几行代码使用XPath从登录页面的HTML中提取了<code>authenticity_token</code>、<code>required_field</code>、<code>timestamp</code>和<code>timestamp_secret</code>。这些字段通常用于防止跨站请求伪造（CSRF）攻击。<ul>
<li><code>response.xpath(&#39;//input[@name=&quot;authenticity_token&quot;]/@value&#39;).get()</code><ul>
<li><code>//input[@name=&quot;authenticity_token&quot;]</code>：这部分的XPath查找文档中所有<code>&lt;input&gt;</code>元素，其中<code>name</code>属性等于<code>&quot;authenticity_token&quot;</code>。<code>//</code>表示在整个文档中查找，而不仅限于某个特定部分。</li>
<li><code>/@value</code>：这部分表示从找到的<code>&lt;input&gt;</code>元素中提取<code>value</code>属性。在HTML中，<code>&lt;input&gt;</code>标签的<code>value</code>属性通常用来存储输入字段的值。</li>
<li><code>.get()</code>：这是Scrapy的Selector对象的方法，用于提取XPath选择器的第一个匹配结果。如果没有匹配的元素，它将返回<code>None</code>。</li>
</ul>
</li>
<li><code>response.xpath(&#39;//input[@type=&quot;text&quot; and @hidden=&quot;hidden&quot;]/@name&#39;).get()</code><ul>
<li><code>//input[@type=&quot;text&quot; and @hidden=&quot;hidden&quot;]</code>：这个XPath查找所有<code>&lt;input&gt;</code>元素，它们的<code>type</code>属性为<code>&quot;text&quot;</code>且同时拥有<code>hidden=&quot;hidden&quot;</code>属性。这通常是隐藏的表单字段，对用户不可见，但对于表单提交可能是必需的。</li>
<li><code>/@name</code>：这表示提取这些<code>&lt;input&gt;</code>元素的<code>name</code>属性。</li>
<li><code>.get()</code>：同样，这是用来获取第一个匹配结果的Scrapy方法。</li>
</ul>
</li>
<li><code>response.xpath(&#39;//input[@name=&quot;timestamp&quot;]/@value&#39;).get()</code><ul>
<li><code>//input[@name=&quot;timestamp&quot;]</code>：这个XPath寻找所有<code>&lt;input&gt;</code>元素，其<code>name</code>属性为<code>&quot;timestamp&quot;</code>。这通常用于跟踪表单的创建或提交时间。</li>
<li><code>/@value</code>：提取这些<code>&lt;input&gt;</code>元素的<code>value</code>属性。</li>
<li><code>.get()</code>：获取第一个匹配结果。</li>
</ul>
</li>
<li><code>response.xpath(&#39;//input[@name=&quot;timestamp_secret&quot;]/@value&#39;).get()</code><ul>
<li><code>//input[@name=&quot;timestamp_secret&quot;]</code>：查找所有<code>&lt;input&gt;</code>元素，其<code>name</code>属性为<code>&quot;timestamp_secret&quot;</code>。这个值可能是与<code>timestamp</code>相关的加密或哈希值。</li>
<li><code>/@value</code>：提取<code>value</code>属性。</li>
<li><code>.get()</code>：获取第一个匹配结果。</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">form_data = &#123;</span><br><span class="line">    <span class="string">&quot;commit&quot;</span>: <span class="string">&quot;Sign in&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;authenticity_token&#x27;</span>: authenticity_token,</span><br><span class="line">    <span class="string">&quot;login&quot;</span>: USER.LOGIN,</span><br><span class="line">    <span class="string">&quot;password&quot;</span>: USER.PASSWORD,</span><br><span class="line">    <span class="string">&quot;webauthn-conditional&quot;</span>: <span class="string">&quot;undefined&quot;</span>,</span><br><span class="line">    <span class="string">&quot;javascript-support&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">    <span class="string">&quot;webauthn-support&quot;</span>: <span class="string">&quot;supported&quot;</span>,</span><br><span class="line">    <span class="string">&quot;webauthn-iuvpaa-support&quot;</span>: <span class="string">&quot;unsupported&quot;</span>,</span><br><span class="line">    <span class="string">&quot;return_to&quot;</span>: <span class="string">&quot;https://github.com/login&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;allow_signup&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;client_id&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;integration&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    required_field: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;timestamp&quot;</span>: timestamp,</span><br><span class="line">    <span class="string">&quot;timestamp_secret&quot;</span>: timestamp_secret,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<ul>
<li><strong>commit</strong>: 表单提交按钮的值，通常在登录表单中可以找到。</li>
<li><strong>authenticity_token</strong>: 一个安全令牌，用于防止CSRF攻击，从登录页面的HTML中提取。</li>
<li><strong>login</strong>: GitHub的用户名，<code>USER.LOGIN</code>应该替换为实际的用户名。</li>
<li><strong>password</strong>: GitHub的密码，<code>USER.PASSWORD</code>应该替换为实际的密码。</li>
<li><strong>webauthn-conditional</strong>, <strong>javascript-support</strong>, <strong>webauthn-support</strong>, <strong>webauthn-iuvpaa-support</strong>: 这些字段可能与GitHub的特定前端逻辑相关，例如Web认证和JavaScript支持。</li>
<li><strong>return_to</strong>: 登录后应重定向到的URL。</li>
<li><strong>allow_signup</strong>, <strong>client_id</strong>, <strong>integration</strong>: 这些可能是额外的表单字段，用于GitHub的内部跟踪或逻辑。</li>
<li><strong>required_field</strong>: 之前从页面提取的隐藏字段，其确切意图可能是内部验证。</li>
<li><strong>timestamp</strong> 和 <strong>timestamp_secret</strong>: 与表单提交时效性和安全性相关的字段。</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.FormRequest(url=<span class="string">&quot;https://github.com/session&quot;</span>, formdata=form_data, callback=self.login)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>这行代码创建了一个Scrapy的<code>FormRequest</code>对象，用于向GitHub的<code>https://github.com/session</code> URL发送一个POST请求。</li>
<li><code>formdata=form_data</code> 指定了要发送的表单数据。</li>
<li><code>callback=self.login</code> 指定了Scrapy在收到响应后应调用的方法。<code>self.login</code>方法将处理登录请求的响应。</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">login</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;CosmicTrace&#x27;</span> <span class="keyword">in</span> response.text:</span><br><span class="line">        <span class="built_in">print</span>(response)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;已成功拿到值&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;失敗了&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>def login(self, response):</code> - 这行定义了一个名为 <code>login</code> 的方法。由于存在 <code>self</code> 参数，可以判断这个方法属于某个类，并且这个方法接受一个参数 <code>response</code>。</li>
<li><code>if &#39;CosmicTrace&#39; in response.text:</code> - 这行代码检查 <code>response.text</code> 中是否包含字符串 ‘CosmicTrace’。如果包含，意味着满足了某种成功的条件。</li>
<li><code>print(&quot;已成功拿到值&quot;)</code> - 如果在 <code>response.text</code> 中找到了字符串，就打印出 “已成功拿到值”，表示操作成功。</li>
<li><code>else:</code> - 如果在 <code>response.text</code> 中没有找到字符串 ‘CosmicTrace’，则执行这部分代码。</li>
<li><code>print(&#39;失敗了&#39;)</code> - 如果没有找到字符串，就打印出 ‘失敗了’，即“失败了”。</li>
<li><code>print(response)</code> - 这行代码会在执行完if-else条件后打印整个 <code>response</code> 对象。这对于调试或记录日志很有用，可以查看请求的完整响应。</li>
</ul>
<h5 id="测试文件"><a href="#测试文件" class="headerlink" title="测试文件"></a>测试文件</h5><p>编辑调试文件</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">execute([<span class="string">&#x27;scrapy&#x27;</span>, <span class="string">&#x27;crawl&#x27;</span>, <span class="string">&#x27;github_login&#x27;</span>])</span><br></pre></td></tr></table></figure></div>

<h5 id="Settings-文件-1"><a href="#Settings-文件-1" class="headerlink" title="Settings 文件"></a>Settings 文件</h5><p>编辑配置文件</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scrapy settings for git project</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For simplicity, this file contains only settings considered important or</span></span><br><span class="line"><span class="comment"># commonly used. You can find more settings consulting the documentation:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span></span><br><span class="line"><span class="comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">&quot;git&quot;</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">&quot;git.spiders&quot;</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&quot;git.spiders&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line"><span class="comment">#USER_AGENT = &quot;git (+http://www.yourdomain.com)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS = 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure a delay for requests for the same website (default: 0)</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span></span><br><span class="line"><span class="comment"># See also autothrottle settings and docs</span></span><br><span class="line"><span class="comment">#DOWNLOAD_DELAY = 3</span></span><br><span class="line"><span class="comment"># The download delay setting will honor only one of:</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_IP = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Disable cookies (enabled by default)</span></span><br><span class="line"><span class="comment">#COOKIES_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Disable Telnet Console (enabled by default)</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">   <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;</span>,</span><br><span class="line">   <span class="string">&quot;Accept-Language&quot;</span>: <span class="string">&quot;en&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable or disable spider middlewares</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"><span class="comment">#SPIDER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#    &quot;git.middlewares.GitSpiderMiddleware&quot;: 543,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="comment">#DOWNLOADER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#    &quot;git.middlewares.GitDownloaderMiddleware&quot;: 543,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable or disable extensions</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span></span><br><span class="line"><span class="comment">#EXTENSIONS = &#123;</span></span><br><span class="line"><span class="comment">#    &quot;scrapy.extensions.telnet.TelnetConsole&quot;: None,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure item pipelines</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"><span class="comment">#ITEM_PIPELINES = &#123;</span></span><br><span class="line"><span class="comment">#    &quot;git.pipelines.GitPipeline&quot;: 300,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable and configure the AutoThrottle extension (disabled by default)</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_ENABLED = True</span></span><br><span class="line"><span class="comment"># The initial download delay</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_START_DELAY = 5</span></span><br><span class="line"><span class="comment"># The maximum download delay to be set in case of high latencies</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_MAX_DELAY = 60</span></span><br><span class="line"><span class="comment"># The average number of requests Scrapy should be sending in parallel to</span></span><br><span class="line"><span class="comment"># each remote server</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span></span><br><span class="line"><span class="comment"># Enable showing throttling stats for every response received:</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_DEBUG = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable and configure HTTP caching (disabled by default)</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span></span><br><span class="line"><span class="comment">#HTTPCACHE_ENABLED = True</span></span><br><span class="line"><span class="comment">#HTTPCACHE_EXPIRATION_SECS = 0</span></span><br><span class="line"><span class="comment">#HTTPCACHE_DIR = &quot;httpcache&quot;</span></span><br><span class="line"><span class="comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span></span><br><span class="line"><span class="comment">#HTTPCACHE_STORAGE = &quot;scrapy.extensions.httpcache.FilesystemCacheStorage&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set settings whose default value is deprecated to a future-proof value</span></span><br><span class="line">REQUEST_FINGERPRINTER_IMPLEMENTATION = <span class="string">&quot;2.7&quot;</span></span><br><span class="line">TWISTED_REACTOR = <span class="string">&quot;twisted.internet.asyncioreactor.AsyncioSelectorReactor&quot;</span></span><br><span class="line">FEED_EXPORT_ENCODING = <span class="string">&quot;utf-8&quot;</span></span><br></pre></td></tr></table></figure></div>

<h5 id="测试执行"><a href="#测试执行" class="headerlink" title="测试执行"></a>测试执行</h5><p>调试测试文件，断点在<code>login</code>方法，发现其返回200，输出响应的<code>TXT</code>格式，复制其内容保存为<code>HTML</code>文件并打开</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/af0f69ee-c5b3-43b4-8f94-45958240ec00/public"
                      width = "500"
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/4ae3b81e-1b99-4297-ec77-7340c8ada700/public"
                      width = "500"
                >

<p>可以发现其成功显示出登录后的界面</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/1a16d792-70d7-4750-e228-ef21a8a0cb00/public"
                      width = "500"
                >

<p>也可以通过判断可以发现<code>HTML</code>是否含有账号名来判断调试是否成功</p>
<h2 id="Scrapy下载中间件（Downloader-Middleware）"><a href="#Scrapy下载中间件（Downloader-Middleware）" class="headerlink" title="Scrapy下载中间件（Downloader Middleware）"></a>Scrapy下载中间件（Downloader Middleware）</h2><p>下载中间件是Scrapy的核心组件之一，它提供了一个灵活的方式来自定义请求和响应的处理过程。</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li><strong>作用</strong>：下载中间件用于拦截并处理Scrapy发出的所有HTTP请求和响应。它们在Scrapy的请求&#x2F;响应处理过程中提供了多个钩子（hooks）点。</li>
<li><strong>功能</strong>：可以用于修改请求和响应、处理重定向、重试失败的请求、设置代理、处理cookies等。</li>
<li><strong>实现</strong>：它是通过实现特定的方法的Python类来定义的。</li>
</ul>
<h3 id="内置中间件"><a href="#内置中间件" class="headerlink" title="内置中间件"></a>内置中间件</h3><ul>
<li><p>Scrapy自带了多种下载中间件，这些中间件提供了对不同方面的处理支持。</p>
</li>
<li><p>可以通过运行命令 <code>scrapy settings --get=DOWNLOADER_MIDDLEWARES_BASE</code> 查看Scrapy自带的所有下载中间件及其优先级。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&quot;</span>: <span class="number">100</span>, <span class="string">&quot;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&quot;</span>: <span class="number">300</span>, <span class="string">&quot;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&quot;</span>: <span class="number">350</span>, <span class="string">&quot;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&quot;</span>: <span class="number">400</span>, <span class="string">&quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;</span>: <span class="number">500</span>, <span class="string">&quot;scrapy.downloadermiddlewares.retry.RetryMiddleware&quot;</span>: <span class="number">550</span>, <span class="string">&quot;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&quot;</span>: <span class="number">560</span>, <span class="string">&quot;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&quot;</span>: <span class="number">580</span>, <span class="string">&quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;</span>: <span class="number">590</span>, <span class="string">&quot;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&quot;</span>: <span class="number">600</span>, <span class="string">&quot;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&quot;</span>: <span class="number">700</span>, <span class="string">&quot;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&quot;</span>: <span class="number">750</span>, <span class="string">&quot;scrapy.downloadermiddlewares.stats.DownloaderStats&quot;</span>: <span class="number">850</span>, <span class="string">&quot;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&quot;</span>: <span class="number">900</span>&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>这些内置中间件包括对robots.txt的处理、HTTP代理支持、Cookies处理等。</p>
</li>
<li><p>用户可以通过在<code>DOWNLOADER_MIDDLEWARES</code>设置中添加自定义的中间件来扩展Scrapy的功能。</p>
</li>
<li><p>设置是一个字典，键是中间件类的路径，值是中间件的顺序（0-1000之间的整数）。数字越小，优先级越高，即越接近Scrapy引擎</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;myproject.middlewares.CustomMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
</ul>
<p>详见官方文档</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html" >https://docs.scrapy.org/en/latest/topics/downloader-middleware.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="下载中间件实例"><a href="#下载中间件实例" class="headerlink" title="下载中间件实例"></a>下载中间件实例</h3><ol>
<li><strong>RobotsTxtMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 此中间件用于处理网站的robots.txt规则。它会自动解析并遵守目标网站的robots.txt文件，从而限制爬虫的行为以符合网站的爬取政策。</li>
<li><strong>配置方法</strong>: 默认情况下，Scrapy会遵守robots.txt规则。可以通过在项目的<code>settings.py</code>中设置<code>ROBOTSTXT_OBEY = False</code>来禁用此功能。</li>
</ul>
<ol start="2">
<li><strong>HttpAuthMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 管理HTTP认证的中间件，用于处理那些需要HTTP认证（如基本认证、摘要认证）的网站。</li>
<li><strong>配置方法</strong>: 可以通过在Scrapy的<code>Request</code>对象中指定<code>http_user</code>和<code>http_pass</code>属性来启用HTTP认证。</li>
</ul>
<ol start="3">
<li><strong>DownloadTimeoutMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 设置下载请求的超时时间。如果请求在指定时间内未得到响应，它将被视为失败，并且可以被重试中间件重新处理。</li>
<li><strong>配置方法</strong>: 可以通过设置<code>DOWNLOAD_TIMEOUT</code>来调整全局超时时间。</li>
</ul>
<ol start="4">
<li><strong>DefaultHeadersMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 为所有的Scrapy请求设置默认HTTP头部。这对于为每个请求添加或覆盖特定的HTTP头（如Accept-Language）非常有用。</li>
<li><strong>配置方法</strong>: 在<code>settings.py</code>文件中，使用<code>DEFAULT_REQUEST_HEADERS</code>设置来定义默认的HTTP头部。</li>
</ul>
<ol start="5">
<li><strong>UserAgentMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 此中间件允许为每个请求随机或固定地设置User-Agent。User-Agent通常被网站用来识别访问者使用的浏览器和操作系统。</li>
<li><strong>配置方法</strong>: 可以通过在<code>settings.py</code>中设置<code>USER_AGENT</code>或者使用自定义的User-Agent提供器来改变User-Agent。</li>
</ul>
<ol start="6">
<li><strong>RetryMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 处理失败的HTTP请求并尝试重新发送。这对于处理暂时的网络问题或服务器错误非常有用。</li>
<li><strong>配置方法</strong>: 默认情况下，Scrapy会重试失败的请求。可以在<code>settings.py</code>中修改<code>RETRY_TIMES</code>来调整重试次数。</li>
</ul>
<ol start="7">
<li><strong>AjaxCrawlMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 用于处理JavaScript生成的页面。这使得Scrapy能够爬取那些需要执行JavaScript代码才能显示完整内容的页面。</li>
<li><strong>配置方法</strong>: 默认不启用，需要在<code>settings.py</code>中显式启用。</li>
</ul>
<ol start="8">
<li><strong>MetaRefreshMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 自动处理页面的meta刷新标签。有些网页可能会使用meta标签来自动刷新或重定向到另一个页面。</li>
<li><strong>配置方法</strong>: 默认启用。可以通过在<code>settings.py</code>中设置<code>METAREFRESH_ENABLED = False</code>来禁用它。</li>
</ul>
<ol start="9">
<li><strong>HttpCompressionMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 此中间件自动处理压缩的HTTP响应，例如gzip或deflate格式的内容。</li>
<li><strong>配置方法</strong>: 默认启用，无需特别配置。</li>
</ul>
<ol start="10">
<li><strong>RedirectMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 自动处理HTTP重定向。对于301和302类型的重定向，此中间件会自动跟随重定向链接。</li>
<li><strong>配置方法</strong>: 默认启用，可以通过<code>REDIRECT_ENABLED</code>设置来禁用。</li>
</ul>
<ol start="11">
<li><strong>CookiesMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 管理Cookies。对于需要管理多个会话或跟踪用户会话的爬虫来说，这个中间件非常有用。</li>
<li><strong>配置方法</strong>: 默认启用，可以通过<code>COOKIES_ENABLED = False</code>来禁用。</li>
</ul>
<ol start="12">
<li><strong>HttpProxyMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 处理HTTP代理。通过使用代理，爬虫可以从不同的IP地址发送请求，这有助于绕过IP封锁或进行匿名抓取。</li>
<li><strong>配置方法</strong>: 可以通过在请求的<code>meta</code>中设置<code>proxy</code>键来启用代理。</li>
</ul>
<ol start="13">
<li><strong>DownloaderStats</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 收集下载统计信息。这个中间件为每个响应或异常收集统计数据，帮助分析和优化爬虫性能。</li>
<li><strong>配置方法</strong>: 默认启用，一般无需更改配置。</li>
</ul>
<ol start="14">
<li><strong>HttpCacheMiddleware</strong></li>
</ol>
<ul>
<li><strong>功能描述</strong>: 提供对HTTP缓存的支持。此中间件能够缓存请求的响应，以便下次请求相同资源时快速获取。</li>
<li><strong>配置方法</strong>: 默认不启用，可以通过在<code>settings.py</code>中设置<code>HTTPCACHE_ENABLED = True</code>来启用。</li>
</ul>
<h3 id="下载中间件API"><a href="#下载中间件API" class="headerlink" title="下载中间件API"></a>下载中间件API</h3><ul>
<li>每个下载中间件可以实现以下一个或多个方法：<ul>
<li><strong><code>process_request(request, spider)</code></strong>: 在发送请求之前调用。可以返回None、一个Response对象、一个Request对象或抛出一个异常。</li>
<li><strong><code>process_response(request, response, spider)</code></strong>: 在接收到响应后调用。可以返回Response对象或抛出异常。</li>
<li><strong><code>process_exception(request, exception, spider)</code></strong>: 当下载处理过程中发生异常时调用。</li>
<li><strong><code>from_crawler(cls, crawler)</code></strong>: 类方法，用于访问Scrapy的核心组件和API，以及创建中间件实例。</li>
</ul>
</li>
</ul>
<h4 id="返回值的重要性"><a href="#返回值的重要性" class="headerlink" title="返回值的重要性"></a>返回值的重要性</h4><ul>
<li>每个方法的返回值都非常重要，它决定了请求或响应接下来的处理流程。<ul>
<li>如果<code>process_request</code>返回非None值，Scrapy将不会继续处理该请求，而是立即调用相应的<code>process_response</code>方法。</li>
<li>如果<code>process_response</code>返回一个新的请求（Request对象），Scrapy将停止调用其他中间件的<code>process_response</code>方法，转而处理这个新的请求。</li>
<li>在<code>process_exception</code>中返回一个新的请求对象将同样导致Scrapy停止调用其他中间件的<code>process_exception</code>方法，并处理这个新的请求。</li>
</ul>
</li>
</ul>
<h3 id="自定义中间件"><a href="#自定义中间件" class="headerlink" title="自定义中间件"></a>自定义中间件</h3><h3 id="UA代理池中间件"><a href="#UA代理池中间件" class="headerlink" title="UA代理池中间件"></a>UA代理池中间件</h3><p>在Scrapy项目中，使用自定义中间件来实现用户代理池（User-Agent Pool）是一种常见的做法，用于避免被目标网站识别并可能被阻止。</p>
<p>自定义的用户代理池中间件允许每个请求随机使用不同的用户代理（User-Agent），从而减少被目标网站识别为爬虫的风险。</p>
<h4 id="1-在-settings-py-中定义用户代理列表"><a href="#1-在-settings-py-中定义用户代理列表" class="headerlink" title="1. 在 settings.py 中定义用户代理列表"></a>1. 在 settings.py 中定义用户代理列表</h4><p>首先，在Scrapy项目的<code>settings.py</code>文件中定义一个用户代理列表：</p>
<p><code>settings</code>文件 <code>user_agent_list</code> </p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#user_agent</span></span><br><span class="line">USER_AGENT_LIST = [</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;</span></span><br><span class="line">    ]</span><br></pre></td></tr></table></figure></div>

<p>这个列表包含了多个不同的用户代理字符串，用于在发起请求时模拟不同的浏览器。</p>
<h4 id="2-在-middlewares-py-中实现用户代理中间件"><a href="#2-在-middlewares-py-中实现用户代理中间件" class="headerlink" title="2. 在 middlewares.py 中实现用户代理中间件"></a>2. 在 middlewares.py 中实现用户代理中间件</h4><p>在<code>middlewares.py</code>文件中实现自定义的中间件，用于在每个请求中随机选择一个用户代理：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> scrapy.conf <span class="keyword">import</span> settings</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomUserAgentMiddleware</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="comment"># 随机选择一个用户代理</span></span><br><span class="line">        user_agent = random.choice(settings.get(<span class="string">&#x27;USER_AGENT_LIST&#x27;</span>))</span><br><span class="line">        <span class="keyword">if</span> user_agent:</span><br><span class="line">            <span class="comment"># 设置请求的User-Agent</span></span><br><span class="line">            request.headers.setdefault(<span class="string">&#x27;User-Agent&#x27;</span>, user_agent)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>RandomUserAgentMiddleware</code> 类定义了一个<code>process_request</code>方法，该方法在每个请求发送之前被调用。</li>
<li>方法中，随机从<code>USER_AGENT_LIST</code>中选取一个用户代理，并将其设置为该请求的<code>User-Agent</code>。</li>
</ul>

  <div class="note-large blue">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-info"></i><p>信息</p>

    </div>
    <div class="notel-content">
      <p>也有另一种方法是直接从 <code>settings</code> 模块导入</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .settings <span class="keyword">import</span> USER_AGENT_LIST</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UAMiddleware</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        request.headers[<span class="string">&#x27;User-Agent&#x27;</span>] = random.choice(USER_AGENT_LIST)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure></div>

<ul>
<li>这种方法直接从<code>settings.py</code>文件中导入<code>user_agent_list</code>。</li>
<li>优点是直接和简洁，尤其是在用户代理列表只在这个中间件中使用时。</li>
<li>缺点是它降低了配置的灵活性。如果想在不同的环境（例如开发环境和生产环境）中使用不同的用户代理列表，或者希望能够通过命令行参数动态覆盖这些设置，这种方法可能不太适用。</li>
<li>之前的方法使用Scrapy的内置<code>settings</code>模块来访问项目设置（在<code>settings.py</code>文件中定义的设置）。</li>
<li>优点是它利用了Scrapy框架的设置管理机制，可以更容易地在整个项目中管理和维护这些设置。</li>
<li>缺点是需要导入<code>scrapy.conf.settings</code>，这在某些情况下可能稍显冗余。</li>
<li>如果倾向于使用Scrapy框架的标准特性，并希望在项目的不同组件间共享配置，那么第一种方法（使用<code>scrapy.conf.settings</code>）可能更适合。</li>
<li>如果项目结构比较简单，或者只在一个地方使用这个用户代理列表，那么第二种方法（直接从<code>settings.py</code>导入）可能更直接有效。</li>
</ul>

    </div>
  </div>

<h4 id="3-在-settings-py-中启用中间件"><a href="#3-在-settings-py-中启用中间件" class="headerlink" title="3. 在 settings.py 中启用中间件"></a>3. 在 settings.py 中启用中间件</h4><p>在<code>settings.py</code>文件中，将自定义的中间件添加到<code>DOWNLOADER_MIDDLEWARES</code>设置中，以启用该中间件：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;myproject.middlewares.RandomUserAgentMiddleware&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<ul>
<li>替换<code>&#39;myproject.middlewares.RandomUserAgentMiddleware&#39;</code>为实际的中间件路径。</li>
<li>数值<code>400</code>是中间件的优先级。可以根据需要调整这个值以控制中间件的执行顺序。</li>
</ul>
<h4 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a>注意事项</h4><ul>
<li>使用用户代理池可以帮助模拟常规用户的浏览行为，但应注意合理使用以避免对目标网站造成不必要的负担。</li>
<li>请确保遵守目标网站的爬虫政策和使用条款。</li>
<li>用户代理字符串应尽量选择常见且更新的版本，以提高爬虫的隐蔽性。</li>
</ul>
<h3 id="IP代理池"><a href="#IP代理池" class="headerlink" title="IP代理池"></a>IP代理池</h3><p>在Scrapy项目中使用IP代理池是为了隐藏爬虫的真实IP地址，这有助于绕过目标网站的IP封锁或请求频率限制。</p>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>IP代理池可以让Scrapy爬虫在每次请求时使用不同的IP地址，从而提高爬虫的匿名性和效率。</p>
<h5 id="1-在-settings-py-中定义IP代理池"><a href="#1-在-settings-py-中定义IP代理池" class="headerlink" title="1. 在 settings.py 中定义IP代理池"></a>1. 在 settings.py 中定义IP代理池</h5><p>在Scrapy项目的<code>settings.py</code>文件中，定义一个包含多个代理IP的列表：</p>
<p>(此处是示例,以下的代理基本无法使用的,同时不建议去找免费的代理,不安全)</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># IP代理池</span></span><br><span class="line">IPPOOL = [</span><br><span class="line">    &#123;<span class="string">&quot;ipaddr&quot;</span>: <span class="string">&quot;61.129.70.131:8080&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;ipaddr&quot;</span>: <span class="string">&quot;61.152.81.193:9100&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;ipaddr&quot;</span>: <span class="string">&quot;120.204.85.29:3128&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;ipaddr&quot;</span>: <span class="string">&quot;219.228.126.86:8123&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;ipaddr&quot;</span>: <span class="string">&quot;61.152.81.193:9100&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;ipaddr&quot;</span>: <span class="string">&quot;218.82.33.225:53853&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;ipaddr&quot;</span>: <span class="string">&quot;223.167.190.17:42789&quot;</span>&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure></div>

<p>这个列表包含了多个字典，每个字典代表一个代理服务器，其中<code>ipaddr</code>键的值是代理服务器的IP地址和端口。</p>
<h5 id="2-在-middlewares-py-中实现IP代理中间件"><a href="#2-在-middlewares-py-中实现IP代理中间件" class="headerlink" title="2. 在 middlewares.py 中实现IP代理中间件"></a>2. 在 middlewares.py 中实现IP代理中间件</h5><p>在<code>middlewares.py</code>文件中，实现一个中间件来随机使用IP代理池中的一个代理：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> scrapy.conf <span class="keyword">import</span> settings</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ProxyMiddleware</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="comment"># 从IP池中随机选择一个IP</span></span><br><span class="line">        thisip = random.choice(settings.get(<span class="string">&#x27;IPPOOL&#x27;</span>))</span><br><span class="line">        <span class="comment"># 将选中的IP设置为请求的代理</span></span><br><span class="line">        request.meta[<span class="string">&quot;proxy&quot;</span>] = <span class="string">&quot;http://&quot;</span> + thisip[<span class="string">&#x27;ipaddr&#x27;</span>]</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>ProxyMiddleware</code> 类定义了一个<code>process_request</code>方法，该方法在每个请求发送之前被调用。</li>
<li>方法中，随机从<code>IPPOOL</code>中选取一个IP代理，并将其设置为该请求的代理。</li>
</ul>

  <div class="note-large blue">
    <div class="notel-title rounded-t-lg p-3 font-bold text-lg flex flex-row gap-2 items-center">
      <i class="notel-icon fa-solid fa-circle-info"></i><p>信息</p>

    </div>
    <div class="notel-content">
      <p>第一种方法是从预定义的IP代理池中随机选择一个代理，而另一种种方法是实时从一个API获取代理IP</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">IPPMiddleware</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        url = <span class="string">&#x27;https://api.hailiangip.com:8522/api/getIpEt?dataType=1&amp;encryptParam=...（省略）&#x27;</span></span><br><span class="line">        res = requests.get(url)</span><br><span class="line">        data = <span class="string">&quot;https://&quot;</span> + res.text</span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = data</span><br></pre></td></tr></table></figure></div>

<p>选择哪种方法取决于具体需求和环境。如果需要更高的灵活性和更大的代理IP池，且不介意依赖外部服务和可能的额外成本，那么使用实时API获取代理可能是更好的选择。如果更注重稳定性和控制，且不希望增加额外的依赖和成本，那么使用预定义的代理池可能更适合。</p>

    </div>
  </div>

<h5 id="3-在-settings-py-中启用自定义中间件"><a href="#3-在-settings-py-中启用自定义中间件" class="headerlink" title="3. 在 settings.py 中启用自定义中间件"></a>3. 在 settings.py 中启用自定义中间件</h5><p>在<code>settings.py</code>文件中，将自定义的中间件添加到<code>DOWNLOADER_MIDDLEWARES</code>设置中，以启用该中间件：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;myproject.middlewares.ProxyMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<ul>
<li>替换<code>&#39;myproject.middlewares.ProxyMiddleware&#39;</code>为实际的中间件路径。</li>
<li>数值<code>543</code>是中间件的优先级。可以根据需要调整这个值以控制中间件的执行顺序。</li>
</ul>
<h5 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a>注意事项</h5><ul>
<li>请注意，示例中的代理IP可能无法使用。在实际应用中，建议使用可靠且安全的付费代理服务。</li>
<li>免费代理可能不稳定，且有安全隐患，例如可能会被用于拦截或篡改数据。</li>
<li>使用代理时，请确保遵守目标网站的爬虫政策和使用条款，合理使用以避免对目标网站造成不必要的负担。</li>
</ul>
<h3 id="Scrapy-settings-py-配置详解"><a href="#Scrapy-settings-py-配置详解" class="headerlink" title="Scrapy settings.py 配置详解"></a>Scrapy settings.py 配置详解</h3><h4 id="优先级"><a href="#优先级" class="headerlink" title="优先级"></a>优先级</h4><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/1f0ac0fb-c284-4fc5-f542-7b15d1a03900/public"
                      width = "500"
                >

<p><code>settings.py</code> 文件是 Scrapy 项目的核心配置文件，用于定义爬虫的行为和项目的全局设置。</p>
<h4 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h4><p><strong>项目名称</strong>:</p>
<ul>
<li><p><code>BOT_NAME</code>: 定义项目名称，通常用作日志记录的标识。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BOT_NAME = <span class="string">&#x27;baidu&#x27;</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>爬虫模块路径</strong>:</p>
<ul>
<li><p><code>SPIDER_MODULES</code>: 指定包含 Scrapy 爬虫的模块。</p>
</li>
<li><p><code>NEWSPIDER_MODULE</code>: 定义使用 <code>genspider</code> 命令创建新爬虫时的默认模块。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPIDER_MODULES = [<span class="string">&#x27;baidu.spiders&#x27;</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;baidu.spiders&#x27;</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>用户代理 (User-Agent)</strong>:</p>
<ul>
<li><p><code>USER_AGENT</code>: 定义爬虫默认使用的用户代理。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#x27;</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>Robots.txt 协议</strong>:</p>
<ul>
<li><p><code>ROBOTSTXT_OBEY</code>: 设置是否遵守网站的 robots.txt 协议。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>Cookies 支持</strong>:</p>
<ul>
<li><p><code>COOKIES_ENABLED</code>: 设置是否启用 cookies。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">COOKIES_ENABLED = <span class="literal">True</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>Telnet 控制台</strong>:</p>
<ul>
<li><p><code>TELNETCONSOLE_ENABLED</code>: 设置是否启用 Telnet 控制台用于查看爬虫运行情况。</p>
  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TELNETCONSOLE_ENABLED = <span class="literal">True</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>默认请求头</strong>:</p>
<ul>
<li><p><code>DEFAULT_REQUEST_HEADERS</code>: 设置 Scrapy 发送 HTTP 请求时默认使用的请求头。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>请求重试</strong>:</p>
<ul>
<li><p><code>RETRY_ENABLED</code>: 设置是否启用请求重试。</p>
</li>
<li><p><code>RETRY_TIMES</code>: 设置请求重试的次数。</p>
</li>
<li><p><code>RETRY_HTTP_CODECS</code>: 设置触发重试的 HTTP 状态码。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RETRY_ENABLED = <span class="literal">True</span></span><br><span class="line">RETRY_TIMES = <span class="number">3</span></span><br><span class="line">RETRY_HTTP_CODECS = [<span class="number">500</span>, <span class="number">502</span>, <span class="number">503</span>, <span class="number">504</span>, <span class="number">408</span>]</span><br></pre></td></tr></table></figure></div></li>
</ul>
<h4 id="并发与延迟"><a href="#并发与延迟" class="headerlink" title="并发与延迟"></a>并发与延迟</h4><p><strong>最大并发请求数</strong>:</p>
<ul>
<li><p><code>CONCURRENT_REQUESTS</code>: 设置下载器最大处理的并发请求数量。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONCURRENT_REQUESTS = <span class="number">32</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>每个域的最大并发请求数</strong>:</p>
<ul>
<li><p><code>CONCURRENT_REQUESTS_PER_DOMAIN</code>: 设置每个域名的最大并发请求数。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONCURRENT_REQUESTS_PER_DOMAIN = <span class="number">16</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>每个IP的最大并发请求数</strong>:</p>
<ul>
<li><p><code>CONCURRENT_REQUESTS_PER_IP</code>: 设置每个 IP 的最大并发请求数。如果设置，<code>CONCURRENT_REQUESTS_PER_DOMAIN</code> 将被忽略。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONCURRENT_REQUESTS_PER_IP = <span class="number">16</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<p><strong>下载延迟</strong>:</p>
<ul>
<li><p><code>DOWNLOAD_DELAY</code>: 设置对同一网站的请求间隔秒数。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOAD_DELAY = <span class="number">3</span></span><br></pre></td></tr></table></figure></div></li>
</ul>
<h4 id="智能限速-AutoThrottle"><a href="#智能限速-AutoThrottle" class="headerlink" title="智能限速 (AutoThrottle)"></a>智能限速 (AutoThrottle)</h4><p><strong>AutoThrottle 扩展</strong>:</p>
<ul>
<li>自动调整 Scrapy 到最佳爬取速度，减轻对目标站点的压力。</li>
<li><code>AUTOTHROTTLE_ENABLED</code>: 开启 AutoThrottle。</li>
<li><code>AUTOTHROTTLE_START_DELAY</code>: 初始下载延迟。</li>
<li><code>AUTOTHROTTLE_MAX_DELAY</code>: 最大下载延迟。</li>
<li><code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>: 每秒并发请求数的目标值。</li>
<li><code>AUTOTHROTTLE_DEBUG</code>: 开启调试模式，以观察 AutoThrottle 的行为。</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">AUTOTHROTTLE_ENABLED = <span class="literal">True</span></span><br><span class="line">AUTOTHROTTLE_START_DELAY = <span class="number">5</span></span><br><span class="line">AUTOTHROTTLE_MAX_DELAY = <span class="number">60</span></span><br><span class="line">AUTOTHROTTLE_TARGET_CONCURRENCY = <span class="number">1.0</span></span><br><span class="line">AUTOTHROTTLE_DEBUG = <span class="literal">False</span></span><br></pre></td></tr></table></figure></div>

<h4 id="中间件、Pipelines、扩展"><a href="#中间件、Pipelines、扩展" class="headerlink" title="中间件、Pipelines、扩展"></a>中间件、Pipelines、扩展</h4><ol>
<li><p><strong>启用或禁用 Spider 中间件</strong>:</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;baidu.middlewares.BaiduSpiderMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p><strong>启用或禁用下载器中间件</strong>:</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;baidu.middlewares.MyCustomDownloaderMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p><strong>启用或禁用扩展</strong>:</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">EXTENSIONS = &#123;</span><br><span class="line">    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p><strong>配置 Item Pipelines</strong>:</p>
<ul>
<li>ITEM_PIPELINES: 设置启用的 Item Pipeline。</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">  <span class="string">&#x27;baidu.pipelines.CustomPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></li>
</ol>
<h2 id="Spider类下载图片"><a href="#Spider类下载图片" class="headerlink" title="Spider类下载图片"></a>Spider类下载图片</h2><p>在Scrapy框架中，下载图片与下载文本数据确实有一些相似之处，但也有其特有的处理方式。Scrapy通过内置的支持使得下载图片变得相对简单。与处理文本数据不同，处理图片通常涉及处理二进制数据，并可能需要额外的中间件支持。</p>
<h3 id="测试文件-1"><a href="#测试文件-1" class="headerlink" title="测试文件"></a>测试文件</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">execute([<span class="string">&#x27;scrapy&#x27;</span>, <span class="string">&#x27;crawl&#x27;</span>, <span class="string">&#x27;baidu_img&#x27;</span>])</span><br></pre></td></tr></table></figure></div>

<h3 id="手动保存图片"><a href="#手动保存图片" class="headerlink" title="手动保存图片"></a>手动保存图片</h3><h4 id="Spider文件"><a href="#Spider文件" class="headerlink" title="Spider文件"></a>Spider文件</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re,os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduImgSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;baidu_img&quot;</span></span><br><span class="line">    allowed_domains = []</span><br><span class="line">    start_urls = [<span class="string">&quot;https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;fm=index&amp;fr=&amp;hs=0&amp;xthttps=111210&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;word=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;oq=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;rsp=-1&quot;</span>]</span><br><span class="line">	</span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">            re_data = re.findall(<span class="string">&#x27;thumbURL&quot;:&quot;(.*?)&quot;&#x27;</span>, response.text)</span><br><span class="line">            <span class="keyword">for</span> im <span class="keyword">in</span> re_data:</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(im, callback=self.get_img)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_img</span>(<span class="params">self, response</span>):</span><br><span class="line">        img_data = response.body</span><br><span class="line">        <span class="comment"># 直接保存图片</span></span><br><span class="line">        <span class="comment"># 如果没有 imgspider 这个文件夹，mkdir创建</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;imgspider&#x27;</span>):</span><br><span class="line">            os.mkdir(<span class="string">&#x27;imgspider&#x27;</span>)</span><br><span class="line">        filename = <span class="string">&#x27;imgspider/&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(self.num)</span><br><span class="line">        self.num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(img_data)</span><br></pre></td></tr></table></figure></div>

<h5 id="代码解释"><a href="#代码解释" class="headerlink" title="代码解释"></a>代码解释</h5><ul>
<li>定义一个继承自<code>scrapy.Spider</code>的爬虫类<code>BaiduImgSpider</code>。</li>
<li><code>name</code>：为爬虫指定一个唯一的名称。</li>
<li><code>allowed_domains</code>：定义爬虫允许爬取的域名列表。这里为空，表示不对域名进行限制。</li>
<li><code>start_urls</code>：包含一个起始URL的列表，该URL是百度图片搜索的结果页面。</li>
<li><code>parse</code>：是爬虫的一个方法，处理响应并提取数据。</li>
<li>使用正则表达式从页面源代码中提取所有图片的URL。正则表达式<code>&#39;thumbURL&quot;:&quot;(.*?)&quot;&#39;</code>用于匹配图片的URL。</li>
<li>遍历所有提取到的图片URL。</li>
<li>对每个图片URL，生成一个Scrapy请求，并将响应发送到<code>get_img</code>方法。</li>
<li><code>get_img</code>：处理图片下载的方法。</li>
<li><code>response.body</code>：获取响应的二进制数据，即图片内容。</li>
<li>检查是否存在名为<code>imgspider</code>的目录，如果不存在，则创建该目录。用于保存下载的图片。</li>
<li>构造图片文件的保存路径和文件名。这里使用<code>self.num</code>来为图片生成唯一的文件名，但需要注意<code>self.num</code>在爬虫类中初始化。</li>
<li>以二进制写入模式打开文件，将图片数据写入文件。</li>
</ul>
<h5 id="注意事项-3"><a href="#注意事项-3" class="headerlink" title="注意事项"></a>注意事项</h5><ul>
<li>该爬虫直接从百度图片的搜索结果页面提取图片URL，具体的URL模式可能会随着百度网站的更新而变化。</li>
<li>使用正则表达式提取数据可能不如使用XPath或CSS选择器那样稳定，因为如果百度网页的结构发生变化，正则表达式可能需要更新。</li>
<li>确保遵守百度图片的版权和使用条款，不要用于任何侵犯版权或违法的用途。</li>
</ul>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/a87697e5-898e-470d-6d0a-49635a56e600/public"
                      width = "500"
                >


  <div class="note p-4 mb-4 rounded-small orange fa-circle-exclamation 注意">
    <p>调试处理前配置Settings文件进行伪装，否则连接失败</p>

  </div>

<h3 id="Pipeline-保存图片"><a href="#Pipeline-保存图片" class="headerlink" title="Pipeline 保存图片"></a>Pipeline 保存图片</h3><h4 id="Items-文件-1"><a href="#Items-文件-1" class="headerlink" title="Items 文件"></a>Items 文件</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduItem</span>(scrapy.Item):</span><br><span class="line">    img_data = scrapy.Field()</span><br></pre></td></tr></table></figure></div>

<ul>
<li>定义一个名为<code>BaiduItem</code>的Item类，该类用于在爬虫和Pipeline之间传递数据。</li>
<li>它继承自<code>scrapy.Item</code>。在Scrapy中，<code>Item</code>是用来收集从网页提取的数据的简单容器。</li>
<li><code>BaiduItem</code>类将被用来存储从百度图片搜索结果中提取的数据。</li>
<li><code>BaiduItem</code>类中定义了一个字段<code>img_data</code>。</li>
<li><code>scrapy.Field()</code>是Scrapy用来定义Item字段的特殊容器，用于存储从网页中提取的数据。</li>
<li>在这个例子中，<code>img_data</code>字段用于存储图片的二进制数据或图片的URL。</li>
<li>创建了一个<code>BaiduItem</code>的实例，并将提取到的图片数据或URL填充到<code>img_data</code>字段。</li>
<li>然后，可以将这个填充了数据的<code>BaiduItem</code>实例传递给Pipeline进行进一步的处理，例如保存图片到文件系统或数据库</li>
</ul>
<h4 id="Spider-文件-1"><a href="#Spider-文件-1" class="headerlink" title="Spider 文件"></a>Spider 文件</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re,os</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> BaiduItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduImgSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;baidu_img&quot;</span></span><br><span class="line">    allowed_domains = []</span><br><span class="line">    start_urls = [<span class="string">&quot;https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;fm=index&amp;fr=&amp;hs=0&amp;xthttps=111210&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;word=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;oq=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;rsp=-1&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">            re_data = re.findall(<span class="string">&#x27;thumbURL&quot;:&quot;(.*?)&quot;&#x27;</span>, response.text)</span><br><span class="line">            <span class="keyword">for</span> im <span class="keyword">in</span> re_data:</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(im, callback=self.get_img)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_img</span>(<span class="params">self, response</span>):</span><br><span class="line">        item = BaiduItem()</span><br><span class="line">        img_data = response.body</span><br><span class="line">        item[<span class="string">&#x27;img_data&#x27;</span>] = img_data</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></div>

<ul>
<li>定义一个名为<code>BaiduImgSpider</code>的爬虫类，指定爬虫名称、允许的域名（在这里为空）和起始URL。</li>
<li><code>parse</code>是Scrapy爬虫处理响应的默认方法。使用正则表达式从响应中提取图片的URL。</li>
<li>对每个找到的图片URL，发起一个新的Scrapy请求，并指定回调方法<code>get_img</code>来处理这些请求。</li>
<li><code>get_img</code>方法处理图片下载的响应。创建一个<code>BaiduItem</code>实例，并将下载的图片数据（二进制格式）存储在<code>img_data</code>字段中。</li>
<li>将包含图片数据的item提交给Pipeline进行进一步处理。</li>
</ul>
<h4 id="Pipelines-文件-1"><a href="#Pipelines-文件-1" class="headerlink" title="Pipelines 文件"></a>Pipelines 文件</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduPipeline</span>():</span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;imgpipeline&#x27;</span>):</span><br><span class="line">            os.mkdir(<span class="string">&#x27;imgpipeline&#x27;</span>)</span><br><span class="line">        filename = <span class="string">&#x27;imgpipeline/&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(self.num)</span><br><span class="line">        self.num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(item.get(<span class="string">&#x27;img_data&#x27;</span>))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></div>

<ul>
<li>导入了所需的模块和类。<code>ItemAdapter</code>用于适配不同类型的Item对象，<code>os</code>模块用于处理文件和路径操作。</li>
<li><code>num</code>是一个类变量，用于生成图片文件的名称。</li>
<li><code>process_item</code>是Pipeline处理item的方法。</li>
<li>检查名为<code>imgpipeline</code>的文件夹是否存在，如果不存在，则创建它。这个文件夹用于存储下载的图片。</li>
<li>为下载的图片生成文件名，并将<code>num</code>递增以确保文件名的唯一性。</li>
<li>以二进制写模式打开文件，并将图片数据写入文件。</li>
<li>返回处理后的item。</li>
</ul>

  <div class="note p-4 mb-4 rounded-small orange fa-circle-exclamation 注意">
    <p><code>settings.py</code>中激活<code>BaiduPipeline</code>避免没有传值</p>

  </div>

<h4 id="注意事项-4"><a href="#注意事项-4" class="headerlink" title="注意事项"></a>注意事项</h4><ul>
<li>这个爬虫和Pipeline的实现假定图片的URL可以直接从百度图片搜索结果的页面HTML中提取。如果百度更改其HTML结构或JavaScript动态加载机制，这个方法可能需要更新。</li>
<li>当处理大量图片或大型网站时，请确保遵循robots.txt规则并尊重网站的版权和使用条款。</li>
</ul>
<h2 id="ImagesPipeline类下载图片"><a href="#ImagesPipeline类下载图片" class="headerlink" title="ImagesPipeline类下载图片"></a>ImagesPipeline类下载图片</h2><p>Scrapy提供了一个专门的<code>ImagesPipeline</code>类，用于方便地下载和处理图片。要正确使用这个类，需要按照以下步骤操作：</p>
<ol>
<li><p>在Spider文件中提取图片URLs</p>
<ul>
<li>爬虫应该解析目标页面，提取图片的URLs，并将它们存储在item的一个字段中。</li>
</ul>
</li>
<li><p>在Items文件中定义<code>image_urls</code>字段</p>
<ul>
<li>定义一个Scrapy Item，并包含一个名为<code>image_urls</code>的字段，用于存储待下载的图片URLs。</li>
</ul>
</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BdImagePipeItem</span>(scrapy.Item):</span><br><span class="line">    image_urls = scrapy.Field()</span><br></pre></td></tr></table></figure></div>

<ol start="3">
<li>创建继承自ImagesPipeline的管道类<ul>
<li>创建一个新的Pipeline类，继承自<code>ImagesPipeline</code>。这个类可以被用来进一步自定义图片的下载和处理行为（如过滤、转换格式等）。</li>
</ul>
</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BdImagePipeline</span>(<span class="title class_ inherited__">ImagesPipeline</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></div>

<ol start="4">
<li>在Settings文件中配置图片存储路径和Pipeline<ul>
<li>在项目的<code>settings.py</code>文件中，设置图片存储路径（<code>IMAGES_STORE</code>）和启用图片管道。</li>
</ul>
</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_STORE = <span class="string">&#x27;/path/to/your/images/dir&#x27;</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;yourproject.pipelines.BdImagePipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<ol start="5">
<li><p>安装Pillow库</p>
<ul>
<li><code>ImagesPipeline</code>需要Pillow库来处理图片。确保安装了Pillow库（版本4.0或以上）。</li>
</ul>
</li>
</ol>
<p>	</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install Pillow</span><br></pre></td></tr></table></figure></div>

<ol start="6">
<li><p>媒体管道的特性和设置</p>
<ul>
<li><p>Scrapy的媒体管道（包括<code>ImagesPipeline</code>）提供了一些有用的特性，例如避免重新下载最近下载的媒体、生成缩略图、检查图像尺寸等。</p>
</li>
<li><p>除了<code>IMAGES_STORE</code>之外，还有许多其他设置可以用来定制媒体管道的行为，如<code>IMAGES_EXPIRES</code>、<code>IMAGES_THUMBS</code>、<code>IMAGES_MIN_HEIGHT</code>、<code>IMAGES_MIN_WIDTH</code>等。</p>
</li>
</ul>
</li>
</ol>
<h3 id="媒体管道的特性"><a href="#媒体管道的特性" class="headerlink" title="媒体管道的特性"></a>媒体管道的特性</h3><p>Scrapy的媒体管道提供了强大的功能来处理下载的媒体文件，如图片和文件。特别是对于图像，Scrapy提供了额外的处理功能。</p>
<h4 id="基本特性"><a href="#基本特性" class="headerlink" title="基本特性"></a>基本特性</h4><ul>
<li><strong>避免重复下载</strong>：媒体管道会跟踪最近下载的文件，避免重复下载相同的媒体内容。</li>
<li><strong>灵活的存储位置</strong>：支持多种存储方式，包括本地文件系统、Amazon S3、谷歌云存储等。</li>
</ul>
<h4 id="图像特有功能"><a href="#图像特有功能" class="headerlink" title="图像特有功能"></a>图像特有功能</h4><ul>
<li><strong>格式转换</strong>：下载的图片会被转换为通用的JPG格式，并确保图片模式为RGB。</li>
<li><strong>缩略图生成</strong>：可以自动生成指定尺寸的缩略图。</li>
<li><strong>尺寸过滤</strong>：通过设置，可以过滤掉低于指定宽度或高度的图片。</li>
</ul>
<h3 id="媒体管道的设置"><a href="#媒体管道的设置" class="headerlink" title="媒体管道的设置"></a>媒体管道的设置</h3><p>为了使用媒体管道，需要在项目的<code>settings.py</code>文件中进行相应的配置。</p>
<h4 id="启用媒体管道"><a href="#启用媒体管道" class="headerlink" title="启用媒体管道"></a>启用媒体管道</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;<span class="string">&#x27;scrapy.pipelines.images.ImagesPipeline&#x27;</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure></div>

<ul>
<li>这行代码启用了Scrapy的图像管道，并设置其优先级为1。</li>
</ul>
<h4 id="设置存储位置和字段"><a href="#设置存储位置和字段" class="headerlink" title="设置存储位置和字段"></a>设置存储位置和字段</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">FILES_STORE=<span class="string">&#x27;/path/to/valid/dir&#x27;</span>	文件管道存放位置</span><br><span class="line">IMAGES_STORE=<span class="string">&#x27;/path/to/valid/dir&#x27;</span>	图片管道存放位置</span><br><span class="line">FILES_URLS_FIELD=<span class="string">&#x27;field_name_for_your_files_urls&#x27;</span>    自定义文件url字段</span><br><span class="line">FILES_RESULT_FIELD=<span class="string">&#x27;field_name_for_your_processed_files&#x27;</span> 自定义结果字段</span><br><span class="line">IMAGES_URLS_FIELD = <span class="string">&#x27;field_name_for_your_images_urls&#x27;</span> 自定义图片url字段</span><br><span class="line">IMAGES_RESULT_FIELD = <span class="string">&#x27;field_name_for_your_processed_images&#x27;</span> 结果字段</span><br><span class="line">FILES_EXPIRES = <span class="number">90</span>    文件过期时间   默认<span class="number">90</span>天</span><br><span class="line">IMAGES_EXPIRES = <span class="number">90</span>    图片过期时间   默认<span class="number">90</span>天</span><br><span class="line">IMAGES_THUMBS= &#123;<span class="string">&#x27;small&#x27;</span>: (<span class="number">50</span>, <span class="number">50</span>), <span class="string">&#x27;big&#x27;</span>:(<span class="number">270</span>, <span class="number">270</span>)&#125; 缩略图尺寸</span><br><span class="line">IMAGES_MIN_HEIGHT= <span class="number">110</span>     过滤最小高度</span><br><span class="line">IMAGES_MIN_WIDTH= <span class="number">110</span>      过滤最小宽度</span><br><span class="line">MEDIA_ALLOW_REDIRECTS= <span class="literal">True</span>    是否重定向，默认为<span class="literal">False</span></span><br></pre></td></tr></table></figure></div>

<h3 id="Spider文件-1"><a href="#Spider文件-1" class="headerlink" title="Spider文件"></a>Spider文件</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re,os</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> BaiduItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduImgSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;baidu_img&quot;</span></span><br><span class="line">    allowed_domains = []</span><br><span class="line">    start_urls = [<span class="string">&quot;https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;fm=index&amp;fr=&amp;hs=0&amp;xthttps=111210&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;word=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;oq=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;rsp=-1&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">            re_data = re.findall(<span class="string">&#x27;thumbURL&quot;:&quot;(.*?)&quot;&#x27;</span>, response.text)</span><br><span class="line"></span><br><span class="line">            item = BaiduItem()</span><br><span class="line">            item[<span class="string">&#x27;image_urls&#x27;</span>] = re_data</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></div>

<ul>
<li>导入了Scrapy框架、正则表达式库和项目中定义的<code>BaiduItem</code>。</li>
<li>定义了一个名为<code>BaiduImgSpider</code>的爬虫类。</li>
<li><code>start_urls</code>包含了开始爬取的URL（百度图片搜索结果）。</li>
<li><code>parse</code>方法处理响应并提取图片URL。</li>
<li>使用正则表达式从页面中提取图片的URL。</li>
<li>创建一个<code>BaiduItem</code>实例，并将提取到的URL列表赋值给<code>image_urls</code>字段。</li>
<li>使用<code>yield</code>语句返回这个item。</li>
</ul>
<h3 id="Items文件"><a href="#Items文件" class="headerlink" title="Items文件"></a>Items文件</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduItem</span>(scrapy.Item):</span><br><span class="line">    image_urls = scrapy.Field()</span><br></pre></td></tr></table></figure></div>

<ul>
<li>定义了一个名为<code>BaiduItem</code>的Item类，包含一个<code>image_urls</code>字段。</li>
<li><code>image_urls</code>用于存储待下载的图片URL列表。</li>
</ul>
<h3 id="Pipelines文件"><a href="#Pipelines文件" class="headerlink" title="Pipelines文件"></a>Pipelines文件</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduPipeline</span>(<span class="title class_ inherited__">ImagesPipeline</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>导入了Scrapy的<code>ImagesPipeline</code>。</li>
<li>定义了一个名为<code>BaiduPipeline</code>的类，它继承自<code>ImagesPipeline</code>。</li>
<li>目前这个类没有进行任何自定义操作，直接继承了<code>ImagesPipeline</code>的全部功能。</li>
</ul>
<h3 id="Settings文件"><a href="#Settings文件" class="headerlink" title="Settings文件"></a>Settings文件</h3><p>添加图片管道存放位置<code>IMAGES_STORE=&#39;imgStore&#39;</code></p>
<p>添加缩略图尺寸设置：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_THUMBS= &#123;</span><br><span class="line">    <span class="string">&#x27;small&#x27;</span>: (<span class="number">60</span>, <span class="number">60</span>),</span><br><span class="line">    <span class="string">&#x27;large&#x27;</span>:(<span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure></div>

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/3cde6b5e-2c19-4226-0403-1beecf4f4900/public"
                      width = "500"
                >

<h2 id="ImagesPipeline类方法重写-改名与翻页"><a href="#ImagesPipeline类方法重写-改名与翻页" class="headerlink" title="ImagesPipeline类方法重写 (改名与翻页)"></a>ImagesPipeline类方法重写 (改名与翻页)</h2><h3 id="Spider文件-2"><a href="#Spider文件-2" class="headerlink" title="Spider文件"></a>Spider文件</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re,os</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> BaiduItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduImgSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;baidu_img&quot;</span></span><br><span class="line">    allowed_domains = []</span><br><span class="line">    start_urls = [<span class="string">&quot;https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;fm=index&amp;fr=&amp;hs=0&amp;xthttps=111210&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;word=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;oq=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;rsp=-1&quot;</span>]</span><br><span class="line"></span><br><span class="line">    page_url =<span class="string">&#x27;https://image.baidu.com/search/acjson?tn=resultjson_com&amp;logid=7201144097221860430&amp;ipn=rj&amp;ct=201326592&amp;is=&amp;fp=result&amp;fr=&amp;word=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;queryWord=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&amp;cl=2&amp;lm=-1&amp;ie=utf-8&amp;oe=utf-8&amp;adpicid=&amp;st=-1&amp;z=&amp;ic=0&amp;hd=&amp;latest=&amp;copyright=&amp;s=&amp;se=&amp;tab=&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;qc=&amp;nc=1&amp;expermode=&amp;nojc=&amp;isAsync=&amp;pn=&#123;&#125;&amp;rn=30&amp;gsm=1e&amp;1702544763785=&#x27;</span></span><br><span class="line"></span><br><span class="line">    page_num = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        re_data = re.findall(<span class="string">&#x27;thumbURL&quot;:&quot;(.*?)&quot;&#x27;</span>, response.text)</span><br><span class="line"></span><br><span class="line">        item = BaiduItem()</span><br><span class="line">        item[<span class="string">&#x27;image_urls&#x27;</span>] = re_data</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.page_num == <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        url = self.page_url.<span class="built_in">format</span>(self.page_num * <span class="number">30</span>)</span><br><span class="line">        self.page_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>page_url</code>定义了用于翻页的URL模板。</li>
<li><code>page_num</code>用于追踪当前的翻页数。</li>
<li><code>parse</code>方法用正则表达式从响应中提取图片URL，并将它们存入<code>BaiduItem</code>的<code>image_urls</code>字段。</li>
<li>控制翻页，如果当前页面数达到4，则停止爬取。</li>
<li>格式化<code>page_url</code>以获取下一页的URL，并更新<code>page_num</code>。</li>
<li>使用<code>scrapy.Request</code>生成新的请求来爬取下一页。</li>
</ul>
<h3 id="Pipelines文件-1"><a href="#Pipelines文件-1" class="headerlink" title="Pipelines文件"></a>Pipelines文件</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> .settings <span class="keyword">import</span> IMAGES_STORE</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduPipeline</span>(<span class="title class_ inherited__">ImagesPipeline</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">item_completed</span>(<span class="params">self, results, item, info</span>):</span><br><span class="line">        images_paths = [x.get(<span class="string">&#x27;path&#x27;</span>) <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">for</span> path <span class="keyword">in</span> images_paths:</span><br><span class="line">            os.rename(os.path.join(IMAGES_STORE,path),</span><br><span class="line">                      os.path.join(IMAGES_STORE, <span class="built_in">str</span>(self.num) + <span class="string">&#x27;.png&#x27;</span>)</span><br><span class="line">                      )</span><br><span class="line">            self.num += <span class="number">1</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li><p>导入所需的模块和类，包括Scrapy的内置<code>ImagesPipeline</code>。</p>
</li>
<li><p>定义了继承自<code>ImagesPipeline</code>的<code>BaiduPipeline</code>类。</p>
</li>
<li><p><code>num</code>用于生成重命名后的图片文件名。</p>
</li>
<li><p>重写<code>item_completed</code>方法，该方法在item的图片被下载完成后调用。</p>
</li>
<li><p>遍历下载的图片路径，并重命名每张图片，使用<code>num</code>作为新的文件名。</p>
</li>
</ul>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://imagedelivery.net/sodhLvOXTBoeXxRe03g_RA/775cf980-849c-4067-1e49-6a38b9890500/public"
                      width = "500"
                >

        </div>

        
            <div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
                <div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> Scrapy</li>
        <li><strong>作者:</strong> Yiuhang Chan</li>
        <li><strong>创建于
                :</strong> 2021-10-28 13:12:54</li>
        
            <li>
                <strong>更新于
                    :</strong> 2024-02-28 18:50:35
            </li>
        
        <li>
            <strong>链接:</strong> https://www.yiuhangblog.com/2021/10/28/20211128scrapy/
        </li>
        <li>
            <strong>
                版权声明:
            </strong>
            

            
                本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a> 进行许可。
            
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden">
                
                    <li class="tag-item mx-0.5">
                        <a href="/tags/%E5%9F%BA%E7%A1%80/">#基础</a>&nbsp;
                    </li>
                
                    <li class="tag-item mx-0.5">
                        <a href="/tags/%E7%90%86%E8%AE%BA/">#理论</a>&nbsp;
                    </li>
                
                    <li class="tag-item mx-0.5">
                        <a href="/tags/%E7%88%AC%E8%99%AB/">#爬虫</a>&nbsp;
                    </li>
                
                    <li class="tag-item mx-0.5">
                        <a href="/tags/Scrapy/">#Scrapy</a>&nbsp;
                    </li>
                
                    <li class="tag-item mx-0.5">
                        <a href="/tags/%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96/">#网页抓取</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
                
                    <div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
                        <a class="prev"
                        rel="prev"
                        href="/2021/12/18/20211218%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E5%8F%96/"
                        >
                            <span class="left arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-left"></i>
                            </span>
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">分布式爬取</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
                        <a class="next"
                        rel="next"
                        href="/2021/10/26/20211026%E9%AA%8C%E8%AF%81%E8%AF%86%E5%88%AB/"
                        >
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">验证识别</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
            <div class="comment-container px-2 sm:px-6 md:px-8 pb-8">
                <div class="comments-container mt-10 w-full ">
    <div id="comment-anchor" class="w-full h-2.5"></div>
    <div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">
        评论
    </div>
    

        
            
    <div id="waline"></div>
    <script type="module" data-swup-reload-script>
      import { init } from '/js/libs/waline.mjs';

      function loadWaline() {
        init({
          el: '#waline',
          serverURL: 'https://comment.yiuhangblog.com',
          lang: 'zh-CN',
          dark: 'body[class~="dark-mode"]',
          requiredMeta: ['nick', 'mail'],
          emoji: [],
          recaptchaV3Key: "wasd",
          
        });
      }

      if (typeof swup !== 'undefined') {
        loadWaline();
      } else {
        window.addEventListener('DOMContentLoaded', loadWaline);
      }
    </script>



        
    
</div>

            </div>
        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">Scrapy</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="nav-text">运行流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="nav-text">Scrapy框架的体系结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81%E8%BF%87%E7%A8%8B"><span class="nav-text">数据流过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8"><span class="nav-text">简单使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E5%91%BD%E4%BB%A4"><span class="nav-text">项目命令</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E6%96%87%E4%BB%B6%E4%BB%8B%E7%BB%8D"><span class="nav-text">项目文件介绍</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1"><span class="nav-text">爬取豆瓣电影</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E7%9B%AE%E7%9A%84%E5%92%8C%E8%A6%81%E6%B1%82"><span class="nav-text">项目目的和要求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA"><span class="nav-text">项目搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-Spider-%E7%B1%BB"><span class="nav-text">创建 Spider 类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E5%93%8D%E5%BA%94%EF%BC%88Parsing-Response%EF%BC%89"><span class="nav-text">解析响应（Parsing Response）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E8%AF%95%E5%89%8D%E7%BD%AE"><span class="nav-text">调试前置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B8%85%E6%B4%97%E6%95%B0%E6%8D%AE"><span class="nav-text">清洗数据</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B0%83%E8%AF%951"><span class="nav-text">调试1</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B0%83%E8%AF%952"><span class="nav-text">调试2</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B0%83%E8%AF%953"><span class="nav-text">调试3</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-Item"><span class="nav-text">定义 Item</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%BD%E8%B8%AA%E9%93%BE%E6%8E%A5%EF%BC%88Following-Links%EF%BC%89"><span class="nav-text">追踪链接（Following Links）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E5%92%8C%E4%BD%BF%E7%94%A8%E7%AE%A1%E9%81%93%EF%BC%88Pipelines%EF%BC%89"><span class="nav-text">定义和使用管道（Pipelines）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE"><span class="nav-text">存储数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E7%88%AC%E8%99%AB"><span class="nav-text">运行爬虫</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy-shell"><span class="nav-text">Scrapy shell</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BF%AB%E6%8D%B7%E6%96%B9%E6%B3%95"><span class="nav-text">快捷方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy-%E5%AF%B9%E8%B1%A1"><span class="nav-text">Scrapy 对象</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy-Selector-%E7%B1%BB"><span class="nav-text">Scrapy Selector 类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Selector-%E7%B1%BB%E7%9A%84%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95"><span class="nav-text">Selector 类的基础用法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%80%9A%E8%BF%87%E6%96%87%E6%9C%AC%E6%9E%84%E9%80%A0-Selector-%E5%AE%9E%E4%BE%8B"><span class="nav-text">1. 通过文本构造 Selector 实例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8F%90%E5%8F%96%E7%94%B5%E5%BD%B1%E5%90%8D%E7%A7%B0"><span class="nav-text">提取电影名称</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8F%90%E5%8F%96%E6%89%80%E6%9C%89%E5%8C%B9%E9%85%8D%E5%85%83%E7%B4%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%97%E8%A1%A8"><span class="nav-text">提取所有匹配元素的文本列表</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-extract-first-%E6%8F%90%E5%8F%96%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8C%B9%E9%85%8D%E5%85%83%E7%B4%A0%E7%9A%84%E6%96%87%E6%9C%AC"><span class="nav-text">使用 extract_first() 提取第一个匹配元素的文本</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-extract-%E6%8F%90%E5%8F%96%E6%89%80%E6%9C%89%E5%8C%B9%E9%85%8D%E5%85%83%E7%B4%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%97%E8%A1%A8"><span class="nav-text">使用 extract() 提取所有匹配元素的文本列表</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8F%90%E5%8F%96%E7%89%B9%E5%AE%9A%E5%AD%90%E5%85%83%E7%B4%A0%E7%9A%84%E6%96%87%E6%9C%AC"><span class="nav-text">提取特定子元素的文本</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%80%9A%E8%BF%87-response-%E6%9E%84%E9%80%A0-Selector-%E5%AE%9E%E4%BE%8B"><span class="nav-text">2. 通过 response 构造 Selector 实例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Selector-%E5%AE%9E%E4%BE%8B%E5%92%8C-response-%E7%9A%84-selector-%E5%B1%9E%E6%80%A7%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="nav-text">使用 Selector 实例和 response 的 selector 属性提取数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%93%E5%90%88-CSS-%E5%92%8C-XPath-%E9%80%89%E6%8B%A9%E5%99%A8%E5%8F%8A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="nav-text">结合 CSS 和 XPath 选择器及正则表达式提取数据</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy-Spider-%E7%B1%BB"><span class="nav-text">Scrapy Spider 类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spider-%E7%B1%BB%E7%9A%84%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84"><span class="nav-text">Spider 类的基础结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F-Logger"><span class="nav-text">日志系统 Logger</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logger-%E5%B1%9E%E6%80%A7"><span class="nav-text">Logger 属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Log-%E6%96%B9%E6%B3%95"><span class="nav-text">Log 方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="nav-text">使用示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#from-crawler-%E5%92%8C-set-crawler-%E6%96%B9%E6%B3%95"><span class="nav-text">from_crawler 和 _set_crawler 方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#start-requests-%E6%96%B9%E6%B3%95"><span class="nav-text">start_requests 方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parse-%E6%96%B9%E6%B3%95"><span class="nav-text">parse 方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#close-%E6%96%B9%E6%B3%95"><span class="nav-text">close 方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AC%A1%E7%BA%A7%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96%E5%8F%8A%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E6%8B%BC%E6%8E%A5"><span class="nav-text">次级页面抓取及数据传递拼接</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AC%A1%E7%BA%A7%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96"><span class="nav-text">次级页面抓取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%A6%E6%83%85%E9%A1%B5%E6%8A%93%E5%8F%96"><span class="nav-text">详情页抓取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E7%A4%BA%E4%BE%8B"><span class="nav-text">数据提取示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%9A%84%E4%BC%A0%E9%80%92%E6%8B%BC%E6%8E%A5"><span class="nav-text">参数的传递拼接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B1%86%E7%93%A3%E4%B8%BA%E4%BE%8B%E5%AD%90"><span class="nav-text">豆瓣为例子</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Spider-%E6%96%87%E4%BB%B6"><span class="nav-text">Spider 文件</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E5%AE%9A%E4%B9%89"><span class="nav-text">爬虫定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E9%A1%B5%E8%A7%A3%E6%9E%90%E6%96%B9%E6%B3%95-parse"><span class="nav-text">主页解析方法 parse</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#0-%E6%96%B9%E6%B3%95%E5%AE%9A%E4%B9%89"><span class="nav-text">0. 方法定义</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-%E8%A7%A3%E6%9E%90%E8%8A%82%E7%82%B9%E5%88%97%E8%A1%A8"><span class="nav-text">1. 解析节点列表</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-%E5%88%A4%E6%96%AD%E5%B9%B6%E9%81%8D%E5%8E%86%E8%8A%82%E7%82%B9"><span class="nav-text">2. 判断并遍历节点</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-%E6%8F%90%E5%8F%96%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF"><span class="nav-text">3. 提取电影信息</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#4-%E5%88%9B%E5%BB%BA%E5%B9%B6%E5%A1%AB%E5%85%85-Scrapy-Item"><span class="nav-text">4. 创建并填充 Scrapy Item</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#5-%E8%AF%B7%E6%B1%82%E7%94%B5%E5%BD%B1%E8%AF%A6%E6%83%85%E9%A1%B5"><span class="nav-text">5. 请求电影详情页</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#6-%E5%88%86%E9%A1%B5%E5%A4%84%E7%90%86"><span class="nav-text">6. 分页处理</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#7-%E5%A4%84%E7%90%86%E7%BB%93%E6%9D%9F"><span class="nav-text">7. 处理结束</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%A6%E6%83%85%E9%A1%B5%E8%A7%A3%E6%9E%90%E6%96%B9%E6%B3%95-get-detail"><span class="nav-text">详情页解析方法 get_detail</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#0-%E6%96%B9%E6%B3%95%E5%AE%9A%E4%B9%89-1"><span class="nav-text">0. 方法定义</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%9B%B4%E6%96%B0-Item"><span class="nav-text">1. 初始化和更新 Item</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-%E6%8F%90%E5%8F%96%E7%94%B5%E5%BD%B1%E6%8F%8F%E8%BF%B0%E4%BF%A1%E6%81%AF"><span class="nav-text">2. 提取电影描述信息</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-%E8%AE%BE%E7%BD%AE%E6%8F%8F%E8%BF%B0%E5%B9%B6%E8%BF%94%E5%9B%9E-Item"><span class="nav-text">3. 设置描述并返回 Item</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Items-%E6%96%87%E4%BB%B6"><span class="nav-text">Items 文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pipelines-%E6%96%87%E4%BB%B6"><span class="nav-text">Pipelines 文件</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%8A%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6"><span class="nav-text">1. 初始化及打开文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E5%A4%84%E7%90%86%E5%B9%B6%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE"><span class="nav-text">2. 处理并存储数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E5%85%B3%E9%97%AD%E6%96%87%E4%BB%B6"><span class="nav-text">3. 关闭文件</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Settings-%E6%96%87%E4%BB%B6"><span class="nav-text">Settings 文件</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E8%AE%BE%E7%BD%AE"><span class="nav-text">基本设置</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%94%A8%E6%88%B7%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE"><span class="nav-text">用户代理设置</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Item-Pipeline-%E8%AE%BE%E7%BD%AE"><span class="nav-text">Item Pipeline 设置</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E7%9A%84%E7%9B%AE%E6%A0%87%E5%92%8C%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93"><span class="nav-text">爬虫项目的目标和请求流程总结</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E6%95%B0%E6%8D%AE"><span class="nav-text">目标数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B"><span class="nav-text">请求流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%92%8C%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-text">数据存储和一致性</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B"><span class="nav-text">项目案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E6%9E%90%E9%9C%80%E6%B1%82"><span class="nav-text">分析需求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E6%9E%90%E8%A7%84%E5%BE%8B"><span class="nav-text">分析规律</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%88%AC%E5%8F%96%E6%80%9D%E8%B7%AF"><span class="nav-text">爬取思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4"><span class="nav-text">步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="nav-text">1. 创建项目</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%88%9B%E5%BB%BA%E7%88%AC%E8%99%AB"><span class="nav-text">2. 创建爬虫</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E7%BC%96%E8%BE%91%E7%88%AC%E8%99%AB"><span class="nav-text">3. 编辑爬虫</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E5%A4%84%E7%90%86%E5%88%97%E8%A1%A8%E5%88%97"><span class="nav-text">4. 处理列表列</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E5%A4%84%E7%90%86%E8%AF%A6%E6%83%85%E9%A1%B5%E5%92%8CItems-%E6%96%87%E4%BB%B6"><span class="nav-text">5. 处理详情页和Items 文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-%E7%BC%96%E8%BE%91Pipelines"><span class="nav-text">6. 编辑Pipelines</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5"><span class="nav-text">检查</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8"><span class="nav-text">数据库存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BB%BA%E7%AB%8B%E6%B5%8B%E8%AF%95%E6%96%87%E4%BB%B6"><span class="nav-text">建立测试文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E6%AD%A5%E9%AA%A4"><span class="nav-text">配置步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E5%9C%A8-settings-py-%E4%B8%AD%E9%85%8D%E7%BD%AEMySQL%E4%BF%A1%E6%81%AF"><span class="nav-text">1. 在 settings.py 中配置MySQL信息</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E5%9C%A8-pipelines-py-%E4%B8%AD%E5%88%9B%E5%BB%BAMySQL%E5%AD%98%E5%82%A8%E7%9A%84%E7%B1%BB"><span class="nav-text">2. 在 pipelines.py 中创建MySQL存储的类</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E6%B3%A8%E5%86%8CPipeline"><span class="nav-text">3. 注册Pipeline</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-%E7%BC%96%E5%86%99%E8%BF%9E%E6%8E%A5MySQL%E7%9A%84%E9%80%BB%E8%BE%91"><span class="nav-text">4. 编写连接MySQL的逻辑</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-%E7%BC%96%E5%86%99%E5%AD%98%E5%82%A8%E9%80%BB%E8%BE%91"><span class="nav-text">5.编写存储逻辑</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E5%AE%9A%E4%B9%89"><span class="nav-text">方法定义</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#SQL-%E8%AF%AD%E5%8F%A5%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-text">SQL 语句的定义</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%89%A7%E8%A1%8CSQL%E8%AF%AD%E5%8F%A5"><span class="nav-text">执行SQL语句</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4%E4%BA%8B%E5%8A%A1"><span class="nav-text">提交事务</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%A7%E8%A1%8C%E7%A1%AE%E8%AE%A4"><span class="nav-text">执行确认</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scray-Request-%E7%B1%BB"><span class="nav-text">Scray Request 类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0"><span class="nav-text">构造函数参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Request-meta-%E7%9A%84%E7%89%B9%E6%AE%8A%E9%94%AE%E5%80%BC"><span class="nav-text">Request.meta 的特殊键值</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FormRequest-%E7%B1%BB"><span class="nav-text">FormRequest 类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0-1"><span class="nav-text">构造函数参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E9%80%BB%E8%BE%91"><span class="nav-text">方法逻辑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%88%AB%E8%AF%B4%E6%98%8E"><span class="nav-text">特别说明</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HTTP%E5%93%8D%E5%BA%94%E5%A4%84%E7%90%86"><span class="nav-text">HTTP响应处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%9E%E6%80%A7%E5%92%8C%E6%96%B9%E6%B3%95"><span class="nav-text">属性和方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy%E6%97%A5%E5%BF%97%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8"><span class="nav-text">Scrapy日志配置和使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE"><span class="nav-text">日志文件配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Python%E6%97%A5%E5%BF%97%E6%A8%A1%E5%9D%97"><span class="nav-text">Python日志模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E8%AE%BE%E7%BD%AE"><span class="nav-text">项目中的常见设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-text">注意事项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GitHub%E7%99%BB%E5%BD%95%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E7%8E%B0"><span class="nav-text">GitHub登录过程分析与实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%99%BB%E5%BD%95%E5%88%86%E6%9E%90"><span class="nav-text">登录分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%99%BB%E5%BD%95%E5%8F%82%E6%95%B0"><span class="nav-text">登录参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#form-data-%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90"><span class="nav-text">form_data 数据来源</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B-1"><span class="nav-text">请求流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B-2"><span class="nav-text">请求流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E5%AE%9E%E7%8E%B0"><span class="nav-text">Scrapy实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%96%E8%BE%91%E7%88%AC%E8%99%AB%E6%96%87%E4%BB%B6"><span class="nav-text">编辑爬虫文件</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="nav-text">代码解析</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E6%96%87%E4%BB%B6"><span class="nav-text">测试文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Settings-%E6%96%87%E4%BB%B6-1"><span class="nav-text">Settings 文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E6%89%A7%E8%A1%8C"><span class="nav-text">测试执行</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6%EF%BC%88Downloader-Middleware%EF%BC%89"><span class="nav-text">Scrapy下载中间件（Downloader Middleware）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E7%BD%AE%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">内置中间件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%AE%9E%E4%BE%8B"><span class="nav-text">下载中间件实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6API"><span class="nav-text">下载中间件API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-text">返回值的重要性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">自定义中间件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#UA%E4%BB%A3%E7%90%86%E6%B1%A0%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">UA代理池中间件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%9C%A8-settings-py-%E4%B8%AD%E5%AE%9A%E4%B9%89%E7%94%A8%E6%88%B7%E4%BB%A3%E7%90%86%E5%88%97%E8%A1%A8"><span class="nav-text">1. 在 settings.py 中定义用户代理列表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%9C%A8-middlewares-py-%E4%B8%AD%E5%AE%9E%E7%8E%B0%E7%94%A8%E6%88%B7%E4%BB%A3%E7%90%86%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">2. 在 middlewares.py 中实现用户代理中间件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%9C%A8-settings-py-%E4%B8%AD%E5%90%AF%E7%94%A8%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">3. 在 settings.py 中启用中间件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-1"><span class="nav-text">注意事项</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IP%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-text">IP代理池</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-text">实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E5%9C%A8-settings-py-%E4%B8%AD%E5%AE%9A%E4%B9%89IP%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-text">1. 在 settings.py 中定义IP代理池</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E5%9C%A8-middlewares-py-%E4%B8%AD%E5%AE%9E%E7%8E%B0IP%E4%BB%A3%E7%90%86%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">2. 在 middlewares.py 中实现IP代理中间件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E5%9C%A8-settings-py-%E4%B8%AD%E5%90%AF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">3. 在 settings.py 中启用自定义中间件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-2"><span class="nav-text">注意事项</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy-settings-py-%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3"><span class="nav-text">Scrapy settings.py 配置详解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%85%88%E7%BA%A7"><span class="nav-text">优先级</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE"><span class="nav-text">基础配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%BB%B6%E8%BF%9F"><span class="nav-text">并发与延迟</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%99%BA%E8%83%BD%E9%99%90%E9%80%9F-AutoThrottle"><span class="nav-text">智能限速 (AutoThrottle)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%AD%E9%97%B4%E4%BB%B6%E3%80%81Pipelines%E3%80%81%E6%89%A9%E5%B1%95"><span class="nav-text">中间件、Pipelines、扩展</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spider%E7%B1%BB%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87"><span class="nav-text">Spider类下载图片</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E6%96%87%E4%BB%B6-1"><span class="nav-text">测试文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8%E4%BF%9D%E5%AD%98%E5%9B%BE%E7%89%87"><span class="nav-text">手动保存图片</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Spider%E6%96%87%E4%BB%B6"><span class="nav-text">Spider文件</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E8%A7%A3%E9%87%8A"><span class="nav-text">代码解释</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-3"><span class="nav-text">注意事项</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pipeline-%E4%BF%9D%E5%AD%98%E5%9B%BE%E7%89%87"><span class="nav-text">Pipeline 保存图片</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Items-%E6%96%87%E4%BB%B6-1"><span class="nav-text">Items 文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spider-%E6%96%87%E4%BB%B6-1"><span class="nav-text">Spider 文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pipelines-%E6%96%87%E4%BB%B6-1"><span class="nav-text">Pipelines 文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-4"><span class="nav-text">注意事项</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ImagesPipeline%E7%B1%BB%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87"><span class="nav-text">ImagesPipeline类下载图片</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AA%92%E4%BD%93%E7%AE%A1%E9%81%93%E7%9A%84%E7%89%B9%E6%80%A7"><span class="nav-text">媒体管道的特性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E7%89%B9%E6%80%A7"><span class="nav-text">基本特性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E7%89%B9%E6%9C%89%E5%8A%9F%E8%83%BD"><span class="nav-text">图像特有功能</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AA%92%E4%BD%93%E7%AE%A1%E9%81%93%E7%9A%84%E8%AE%BE%E7%BD%AE"><span class="nav-text">媒体管道的设置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%AF%E7%94%A8%E5%AA%92%E4%BD%93%E7%AE%A1%E9%81%93"><span class="nav-text">启用媒体管道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E5%AD%98%E5%82%A8%E4%BD%8D%E7%BD%AE%E5%92%8C%E5%AD%97%E6%AE%B5"><span class="nav-text">设置存储位置和字段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spider%E6%96%87%E4%BB%B6-1"><span class="nav-text">Spider文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Items%E6%96%87%E4%BB%B6"><span class="nav-text">Items文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pipelines%E6%96%87%E4%BB%B6"><span class="nav-text">Pipelines文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Settings%E6%96%87%E4%BB%B6"><span class="nav-text">Settings文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ImagesPipeline%E7%B1%BB%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99-%E6%94%B9%E5%90%8D%E4%B8%8E%E7%BF%BB%E9%A1%B5"><span class="nav-text">ImagesPipeline类方法重写 (改名与翻页)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spider%E6%96%87%E4%BB%B6-2"><span class="nav-text">Spider文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pipelines%E6%96%87%E4%BB%B6-1"><span class="nav-text">Pipelines文件</span></a></li></ol></li></ol>

    </div>
</div>
        </div>
    
</div>



                

            </div>

            

        </div>

        <div class="main-content-footer">
            <footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2018</span>
              -
            
            2024&nbsp;&nbsp;<i class="fa-solid fa-cog fa-spin" style="--fa-animation-duration: 5s"></i>&nbsp;&nbsp;<a href="/">Yiuhang Chan</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        共 41 篇文章
                    </span>
                    
                        <span>
                            共 398k 字
                        </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">访问人数</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">总访问量</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a> 驱动</span>
            <span class="text-sm lg:block">主题&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.6.1</a></span>
        </div>
        
        
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="right-bottom-tools rss flex justify-center items-center">
                <a class="flex justify-center items-center"
                   href="/atom.xml"
                   target="_blank"
                >
                <i class="fa-regular fa-rss"></i>
                </a>
            </li>
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>


    
<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>







<script src="/js/tools/imageViewer.js" type="module"></script>

<script src="/js/utils.js" type="module"></script>

<script src="/js/main.js" type="module"></script>

<script src="/js/layouts/navbarShrink.js" type="module"></script>

<script src="/js/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/layouts/categoryList.js" type="module"></script>



    
<script src="/js/tools/localSearch.js" type="module"></script>




    
<script src="/js/tools/codeBlock.js" type="module"></script>




    
<script src="/js/layouts/lazyload.js" type="module"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js" type="module"></script>




    
<script src="/js/libs/mermaid.min.js"></script>

    
<script src="/js/plugins/mermaid.js"></script>







<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/tools/tocToggle.js" type="module"></script>

<script src="/js/layouts/toc.js" type="module"></script>

<script src="/js/plugins/tabs.js" type="module"></script>

    
</div>


</body>
</html>
